{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee0c8fc0",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fe413e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tools.imports import *\n",
    "\n",
    "from tools.pca import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c988a2f7",
   "metadata": {},
   "source": [
    "###  Configuration  (Loading PCA and Frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "466fe401",
   "metadata": {},
   "outputs": [],
   "source": [
    "RGB_FOLDER = Path(r\"D:\\Datasets\\Datasets\\EPIC_Kitchen\\RGB\\P01_01\\Original\")\n",
    "FLOW_FOLDER = Path(r\"D:\\Datasets\\Datasets\\EPIC_Kitchen\\OpticalFlow\\P01_01\\P01_01\")\n",
    "LABEL_CSV = Path(r\"EPIC-Kitchens\\Labels\\P01_01.csv\")\n",
    "\n",
    "OUTPUT_FUSED_CSV = Path(r\"EPIC-Kitchens\\Features\\P01_01_fused_features_PCA.csv\")\n",
    "\n",
    "PCA_PATH = \"pca_2048_to_512.pkl\"\n",
    "\n",
    "SAMPLE_RATE = 1\n",
    "FEAT_DIM = 512\n",
    "W_RGB = 0.6\n",
    "W_FLOW = 0.4\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "OUTPUT_FUSED_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "_frame_number_re = re.compile(r\"(\\d+)(?=\\.[^.]+$)\")\n",
    "\n",
    "def parse_frame_index(fname: str):\n",
    "    m = _frame_number_re.search(fname)\n",
    "    if m:\n",
    "        return int(m.group(1))\n",
    "    digs = re.findall(r\"\\d+\", fname)\n",
    "    return int(digs[-1]) if digs else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9fd587e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PAWANESH\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba89f6c1ca3f43ae8de1398fa7c5a9be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extract & Fuse:   0%|          | 0/99029 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVED] EPIC-Kitchens\\Features\\P01_01_fused_features_PCA.csv\n"
     ]
    }
   ],
   "source": [
    "_resnet = resnet50(weights=True)\n",
    "_resnet = nn.Sequential(*list(_resnet.children())[:-1])\n",
    "_resnet = _resnet.to(DEVICE).eval()   # output: (B,2048,1,1)\n",
    "\n",
    "_transform = T.Compose([\n",
    "    T.Resize((224,224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485,0.456,0.406],\n",
    "                [0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# ------------------ LOAD PCA --------------------------------\n",
    "\n",
    "pca = joblib.load(PCA_PATH)\n",
    "assert pca.components_.shape == (FEAT_DIM, 2048), \"PCA dimension mismatch\"\n",
    "\n",
    "# ---------------- FEATURE EXTRACTION ------------------------\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_feature_from_pil(pil_img: Image.Image):\n",
    "    x = _transform(pil_img).unsqueeze(0).to(DEVICE)  # (1,3,224,224)\n",
    "    feat = _resnet(x).view(-1).cpu().numpy()         # (2048,)\n",
    "    feat = pca.transform(feat[None, :])[0]          # (512,)\n",
    "    return feat.astype(np.float32)\n",
    "\n",
    "# ---------------- MAIN EXTRACTION FUNCTION ------------------\n",
    "\n",
    "def extract_and_save_fused(csv_labels_path: Path,\n",
    "                           rgb_folder: Path,\n",
    "                           flow_folder: Path or None,\n",
    "                           out_fused_csv: Path,\n",
    "                           sample_rate: int = 1,\n",
    "                           w_rgb: float = 0.6,\n",
    "                           w_flow: float = 0.4):\n",
    "\n",
    "    labels_df = pd.read_csv(csv_labels_path)\n",
    "\n",
    "    rgb_files = sorted([\n",
    "        p for p in rgb_folder.iterdir()\n",
    "        if p.suffix.lower() in [\".jpg\", \".png\", \".jpeg\"]\n",
    "    ])\n",
    "\n",
    "    sampled = rgb_files[::sample_rate]\n",
    "    if len(sampled) == 0:\n",
    "        raise RuntimeError(f\"No frames found in {rgb_folder}\")\n",
    "\n",
    "    fused_rows = []\n",
    "\n",
    "    for fp in tqdm(sampled, desc=\"Extract & Fuse\"):\n",
    "        fname = fp.name\n",
    "        frame_idx = parse_frame_index(fname)\n",
    "\n",
    "        # -------- RGB --------\n",
    "        try:\n",
    "            pil = Image.open(fp).convert(\"RGB\")\n",
    "            rgb_feat = extract_feature_from_pil(pil)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] RGB skip {fname}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # -------- FLOW --------\n",
    "        if flow_folder is not None:\n",
    "            ffp = flow_folder / fname\n",
    "            if not ffp.exists():\n",
    "                ffp = fp\n",
    "            try:\n",
    "                pilf = Image.open(ffp).convert(\"RGB\")\n",
    "                flow_feat = extract_feature_from_pil(pilf)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] FLOW skip {fname}: {e}\")\n",
    "                flow_feat = np.zeros(FEAT_DIM, dtype=np.float32)\n",
    "        else:\n",
    "            flow_feat = np.zeros(FEAT_DIM, dtype=np.float32)\n",
    "\n",
    "        # -------- FUSION --------\n",
    "        fused_vec = w_rgb * rgb_feat + w_flow * flow_feat\n",
    "\n",
    "        # -------- LABEL --------\n",
    "        lr = labels_df[\n",
    "            (labels_df[\"StartFrame\"] <= frame_idx) &\n",
    "            (labels_df[\"EndFrame\"] >= frame_idx)\n",
    "        ]\n",
    "\n",
    "        if not lr.empty:\n",
    "            action_label = int(lr.iloc[0].get(\"ActionLabel\", -1))\n",
    "            action_name = str(lr.iloc[0].get(\"ActionName\", \"Unknown\"))\n",
    "        else:\n",
    "            action_label, action_name = -1, \"Unknown\"\n",
    "\n",
    "        row = {\n",
    "            \"frame_idx\": int(frame_idx),\n",
    "            \"frame_name\": fname,\n",
    "            \"ActionLabel\": action_label,\n",
    "            \"ActionName\": action_name\n",
    "        }\n",
    "\n",
    "        for i, v in enumerate(fused_vec):\n",
    "            row[f\"feat_{i}\"] = float(v)\n",
    "\n",
    "        fused_rows.append(row)\n",
    "\n",
    "    if len(fused_rows) == 0:\n",
    "        raise RuntimeError(\"No fused rows extracted\")\n",
    "\n",
    "    df_fused = pd.DataFrame(fused_rows)\n",
    "    df_fused.to_csv(out_fused_csv, index=False)\n",
    "\n",
    "    print(f\"[SAVED] {out_fused_csv}\")\n",
    "    return df_fused\n",
    "\n",
    "# ---------------- RUN ---------------------------------------\n",
    "\n",
    "df_fused = extract_and_save_fused(\n",
    "    csv_labels_path=LABEL_CSV,\n",
    "    rgb_folder=RGB_FOLDER,\n",
    "    flow_folder=FLOW_FOLDER if FLOW_FOLDER.exists() else None,\n",
    "    out_fused_csv=OUTPUT_FUSED_CSV,\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    w_rgb=W_RGB,\n",
    "    w_flow=W_FLOW\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b7c031d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame_idx</th>\n",
       "      <th>frame_name</th>\n",
       "      <th>ActionLabel</th>\n",
       "      <th>ActionName</th>\n",
       "      <th>feat_0</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_502</th>\n",
       "      <th>feat_503</th>\n",
       "      <th>feat_504</th>\n",
       "      <th>feat_505</th>\n",
       "      <th>feat_506</th>\n",
       "      <th>feat_507</th>\n",
       "      <th>feat_508</th>\n",
       "      <th>feat_509</th>\n",
       "      <th>feat_510</th>\n",
       "      <th>feat_511</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>frame_00000.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>-0.255704</td>\n",
       "      <td>0.014638</td>\n",
       "      <td>-0.127248</td>\n",
       "      <td>0.135866</td>\n",
       "      <td>-0.104432</td>\n",
       "      <td>0.529116</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081831</td>\n",
       "      <td>-0.088528</td>\n",
       "      <td>-0.030375</td>\n",
       "      <td>0.093051</td>\n",
       "      <td>0.060815</td>\n",
       "      <td>0.002188</td>\n",
       "      <td>-0.084385</td>\n",
       "      <td>0.218704</td>\n",
       "      <td>0.416650</td>\n",
       "      <td>-0.089111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>frame_00001.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>-0.230471</td>\n",
       "      <td>-0.016865</td>\n",
       "      <td>-0.117778</td>\n",
       "      <td>0.106307</td>\n",
       "      <td>-0.078303</td>\n",
       "      <td>0.522713</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.096457</td>\n",
       "      <td>-0.081578</td>\n",
       "      <td>-0.038350</td>\n",
       "      <td>0.075853</td>\n",
       "      <td>0.064037</td>\n",
       "      <td>-0.006416</td>\n",
       "      <td>-0.072980</td>\n",
       "      <td>0.234338</td>\n",
       "      <td>0.377129</td>\n",
       "      <td>-0.094787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>frame_00002.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>-0.273896</td>\n",
       "      <td>-0.004563</td>\n",
       "      <td>-0.079866</td>\n",
       "      <td>0.153029</td>\n",
       "      <td>-0.142856</td>\n",
       "      <td>0.486159</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.039064</td>\n",
       "      <td>-0.073958</td>\n",
       "      <td>-0.103700</td>\n",
       "      <td>0.016556</td>\n",
       "      <td>0.052797</td>\n",
       "      <td>0.036062</td>\n",
       "      <td>-0.094503</td>\n",
       "      <td>0.249313</td>\n",
       "      <td>0.400744</td>\n",
       "      <td>-0.055098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>frame_00003.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>-0.186261</td>\n",
       "      <td>-0.046241</td>\n",
       "      <td>-0.045938</td>\n",
       "      <td>0.125934</td>\n",
       "      <td>-0.146717</td>\n",
       "      <td>0.536510</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.048182</td>\n",
       "      <td>-0.073675</td>\n",
       "      <td>-0.129074</td>\n",
       "      <td>-0.007902</td>\n",
       "      <td>0.046740</td>\n",
       "      <td>0.017751</td>\n",
       "      <td>-0.068501</td>\n",
       "      <td>0.309177</td>\n",
       "      <td>0.469210</td>\n",
       "      <td>-0.033593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>frame_00004.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>-0.155096</td>\n",
       "      <td>-0.016367</td>\n",
       "      <td>-0.074818</td>\n",
       "      <td>0.140323</td>\n",
       "      <td>-0.188490</td>\n",
       "      <td>0.487527</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.094458</td>\n",
       "      <td>-0.096937</td>\n",
       "      <td>-0.110825</td>\n",
       "      <td>-0.026800</td>\n",
       "      <td>0.013834</td>\n",
       "      <td>-0.002211</td>\n",
       "      <td>-0.115081</td>\n",
       "      <td>0.297364</td>\n",
       "      <td>0.386572</td>\n",
       "      <td>-0.028801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76320</th>\n",
       "      <td>76320</td>\n",
       "      <td>frame_76320.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0.247844</td>\n",
       "      <td>-0.085575</td>\n",
       "      <td>0.053120</td>\n",
       "      <td>0.611242</td>\n",
       "      <td>-0.299913</td>\n",
       "      <td>0.715905</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032531</td>\n",
       "      <td>0.135446</td>\n",
       "      <td>0.012278</td>\n",
       "      <td>-0.044854</td>\n",
       "      <td>0.086764</td>\n",
       "      <td>0.046305</td>\n",
       "      <td>0.082122</td>\n",
       "      <td>0.253486</td>\n",
       "      <td>0.450076</td>\n",
       "      <td>0.017609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76321</th>\n",
       "      <td>76321</td>\n",
       "      <td>frame_76321.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0.045791</td>\n",
       "      <td>-0.003731</td>\n",
       "      <td>-0.000484</td>\n",
       "      <td>0.464047</td>\n",
       "      <td>-0.210662</td>\n",
       "      <td>0.553370</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001068</td>\n",
       "      <td>0.213597</td>\n",
       "      <td>-0.024942</td>\n",
       "      <td>-0.065709</td>\n",
       "      <td>0.017595</td>\n",
       "      <td>-0.005818</td>\n",
       "      <td>-0.079159</td>\n",
       "      <td>0.176065</td>\n",
       "      <td>0.269000</td>\n",
       "      <td>0.035629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76322</th>\n",
       "      <td>76322</td>\n",
       "      <td>frame_76322.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0.090800</td>\n",
       "      <td>-0.070876</td>\n",
       "      <td>-0.058114</td>\n",
       "      <td>0.529759</td>\n",
       "      <td>-0.270593</td>\n",
       "      <td>0.681007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013677</td>\n",
       "      <td>0.212171</td>\n",
       "      <td>0.000859</td>\n",
       "      <td>-0.044839</td>\n",
       "      <td>0.090881</td>\n",
       "      <td>-0.009402</td>\n",
       "      <td>0.097548</td>\n",
       "      <td>0.200796</td>\n",
       "      <td>0.435586</td>\n",
       "      <td>0.218567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76323</th>\n",
       "      <td>76323</td>\n",
       "      <td>frame_76323.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0.094027</td>\n",
       "      <td>0.004024</td>\n",
       "      <td>0.074413</td>\n",
       "      <td>0.527673</td>\n",
       "      <td>-0.240405</td>\n",
       "      <td>0.576615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037703</td>\n",
       "      <td>0.203596</td>\n",
       "      <td>-0.049638</td>\n",
       "      <td>-0.022510</td>\n",
       "      <td>-0.021834</td>\n",
       "      <td>-0.098123</td>\n",
       "      <td>-0.090395</td>\n",
       "      <td>0.206394</td>\n",
       "      <td>0.312601</td>\n",
       "      <td>0.091531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76324</th>\n",
       "      <td>76324</td>\n",
       "      <td>frame_76324.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0.015007</td>\n",
       "      <td>0.091800</td>\n",
       "      <td>0.045839</td>\n",
       "      <td>0.463823</td>\n",
       "      <td>-0.224934</td>\n",
       "      <td>0.579037</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029282</td>\n",
       "      <td>0.183742</td>\n",
       "      <td>-0.037138</td>\n",
       "      <td>0.001828</td>\n",
       "      <td>-0.040245</td>\n",
       "      <td>-0.050955</td>\n",
       "      <td>-0.035446</td>\n",
       "      <td>0.145794</td>\n",
       "      <td>0.247459</td>\n",
       "      <td>-0.048350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76325 rows Ã— 516 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       frame_idx       frame_name  ActionLabel ActionName    feat_0    feat_1  \\\n",
       "0              0  frame_00000.jpg           -1    Unknown -0.255704  0.014638   \n",
       "1              1  frame_00001.jpg           -1    Unknown -0.230471 -0.016865   \n",
       "2              2  frame_00002.jpg           -1    Unknown -0.273896 -0.004563   \n",
       "3              3  frame_00003.jpg           -1    Unknown -0.186261 -0.046241   \n",
       "4              4  frame_00004.jpg           -1    Unknown -0.155096 -0.016367   \n",
       "...          ...              ...          ...        ...       ...       ...   \n",
       "76320      76320  frame_76320.jpg           -1    Unknown  0.247844 -0.085575   \n",
       "76321      76321  frame_76321.jpg           -1    Unknown  0.045791 -0.003731   \n",
       "76322      76322  frame_76322.jpg           -1    Unknown  0.090800 -0.070876   \n",
       "76323      76323  frame_76323.jpg           -1    Unknown  0.094027  0.004024   \n",
       "76324      76324  frame_76324.jpg           -1    Unknown  0.015007  0.091800   \n",
       "\n",
       "         feat_2    feat_3    feat_4    feat_5  ...  feat_502  feat_503  \\\n",
       "0     -0.127248  0.135866 -0.104432  0.529116  ... -0.081831 -0.088528   \n",
       "1     -0.117778  0.106307 -0.078303  0.522713  ... -0.096457 -0.081578   \n",
       "2     -0.079866  0.153029 -0.142856  0.486159  ... -0.039064 -0.073958   \n",
       "3     -0.045938  0.125934 -0.146717  0.536510  ... -0.048182 -0.073675   \n",
       "4     -0.074818  0.140323 -0.188490  0.487527  ... -0.094458 -0.096937   \n",
       "...         ...       ...       ...       ...  ...       ...       ...   \n",
       "76320  0.053120  0.611242 -0.299913  0.715905  ... -0.032531  0.135446   \n",
       "76321 -0.000484  0.464047 -0.210662  0.553370  ... -0.001068  0.213597   \n",
       "76322 -0.058114  0.529759 -0.270593  0.681007  ...  0.013677  0.212171   \n",
       "76323  0.074413  0.527673 -0.240405  0.576615  ...  0.037703  0.203596   \n",
       "76324  0.045839  0.463823 -0.224934  0.579037  ...  0.029282  0.183742   \n",
       "\n",
       "       feat_504  feat_505  feat_506  feat_507  feat_508  feat_509  feat_510  \\\n",
       "0     -0.030375  0.093051  0.060815  0.002188 -0.084385  0.218704  0.416650   \n",
       "1     -0.038350  0.075853  0.064037 -0.006416 -0.072980  0.234338  0.377129   \n",
       "2     -0.103700  0.016556  0.052797  0.036062 -0.094503  0.249313  0.400744   \n",
       "3     -0.129074 -0.007902  0.046740  0.017751 -0.068501  0.309177  0.469210   \n",
       "4     -0.110825 -0.026800  0.013834 -0.002211 -0.115081  0.297364  0.386572   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "76320  0.012278 -0.044854  0.086764  0.046305  0.082122  0.253486  0.450076   \n",
       "76321 -0.024942 -0.065709  0.017595 -0.005818 -0.079159  0.176065  0.269000   \n",
       "76322  0.000859 -0.044839  0.090881 -0.009402  0.097548  0.200796  0.435586   \n",
       "76323 -0.049638 -0.022510 -0.021834 -0.098123 -0.090395  0.206394  0.312601   \n",
       "76324 -0.037138  0.001828 -0.040245 -0.050955 -0.035446  0.145794  0.247459   \n",
       "\n",
       "       feat_511  \n",
       "0     -0.089111  \n",
       "1     -0.094787  \n",
       "2     -0.055098  \n",
       "3     -0.033593  \n",
       "4     -0.028801  \n",
       "...         ...  \n",
       "76320  0.017609  \n",
       "76321  0.035629  \n",
       "76322  0.218567  \n",
       "76323  0.091531  \n",
       "76324 -0.048350  \n",
       "\n",
       "[76325 rows x 516 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(r\"EPIC-Kitchens\\Features\\FusedFeatures\\P01_05_fused_features.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e16eb3",
   "metadata": {},
   "source": [
    "### Load the CSV file and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ec98ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def load_fused_csv_by_path(fused_csv_path: str):\n",
    "    fp = Path(fused_csv_path)\n",
    "    if not fp.exists():\n",
    "        raise FileNotFoundError(f\"Fused features CSV not found: {fp}\")\n",
    "    df = pd.read_csv(fp)\n",
    "    if \"frame_idx\" not in df.columns:\n",
    "        raise KeyError(\"Fused CSV must contain 'frame_idx' column\")\n",
    "    df[\"frame_idx\"] = df[\"frame_idx\"].astype(int)\n",
    "    df = df.sort_values(\"frame_idx\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def load_label_csv_by_path(label_csv_path: str):\n",
    "    fp = Path(label_csv_path)\n",
    "    if not fp.exists():\n",
    "        raise FileNotFoundError(f\"Label CSV not found: {fp}\")\n",
    "    df = pd.read_csv(fp)\n",
    "    return df\n",
    "\n",
    "# ------------------------------------ Paths ---------------------------\n",
    "fused_df = load_fused_csv_by_path(r\"EPIC-Kitchens\\Features\\FusedFeatures\\P01_05_fused_features.csv\")\n",
    "labels_df = load_label_csv_by_path(r\"EPIC-Kitchens\\Labels\\P01_05.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bd8ede",
   "metadata": {},
   "source": [
    "### Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13ef15d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_INDEX = -1\n",
    "\n",
    "class SingleVideoAnticipationDataset(Dataset):\n",
    "    def __init__(self, fused_df_or_path, labels_df_or_path,\n",
    "                 t_obs: int, k_fut: int, feat_dim: int,\n",
    "                 fps: float, horizons_s: list[float]):\n",
    "        \n",
    "        # load paths\n",
    "        if isinstance(fused_df_or_path, (str, Path)):\n",
    "            fused_df = pd.read_csv(fused_df_or_path)\n",
    "        else:\n",
    "            fused_df = fused_df_or_path.copy()\n",
    "        if isinstance(labels_df_or_path, (str, Path)):\n",
    "            labels_df = pd.read_csv(labels_df_or_path)\n",
    "        else:\n",
    "            labels_df = labels_df_or_path.copy()\n",
    "\n",
    "        if \"frame_idx\" not in fused_df.columns:\n",
    "            raise KeyError(\"fused_df must contain 'frame_idx'\")\n",
    "\n",
    "        fused_df[\"frame_idx\"] = fused_df[\"frame_idx\"].astype(int)\n",
    "        self.fused_df = fused_df.set_index(\"frame_idx\", drop=False).sort_index()\n",
    "        self.labels_df = labels_df.reset_index(drop=True)\n",
    "\n",
    "        if not all(c in self.labels_df.columns for c in [\"StartFrame\", \"EndFrame\"]):\n",
    "            raise KeyError(\"labels_df must contain StartFrame and EndFrame\")\n",
    "\n",
    "        self.t_obs = int(t_obs)\n",
    "        self.k_fut = int(k_fut)\n",
    "        self.feat_dim = int(feat_dim)\n",
    "        self.feat_cols = [f\"feat_{i}\" for i in range(self.feat_dim)]\n",
    "\n",
    "        # NEW: time info\n",
    "        self.fps = float(fps)\n",
    "        assert len(horizons_s) == self.k_fut, \"len(horizons_s) must equal k_fut\"\n",
    "        self.horizons_s = list(horizons_s)\n",
    "\n",
    "        # samples: one per label row (use EndFrame as obs_end)\n",
    "        self.samples = []\n",
    "        for ridx, row in self.labels_df.iterrows():\n",
    "            try:\n",
    "                obs_end = int(row[\"EndFrame\"])\n",
    "            except:\n",
    "                continue\n",
    "            self.samples.append({\"label_row_idx\": int(ridx), \"obs_end\": obs_end})\n",
    "\n",
    "        if len(self.samples) == 0:\n",
    "            raise RuntimeError(\"No valid label rows found\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def _time_based_future_labels(self, obs_end: int):\n",
    "        labels_df = self.labels_df\n",
    "\n",
    "        def pick(cols):\n",
    "            for c in cols:\n",
    "                if c in labels_df.columns:\n",
    "                    return c\n",
    "            return None\n",
    "\n",
    "        vcol = pick([\"Verb_class\",\"verb\",\"Verb\",\"verb_class\"])\n",
    "        ncol = pick([\"Noun_class\",\"noun\",\"Noun\",\"noun_class\"])\n",
    "        acol = pick([\"Action_class\",\"action\",\"Action\",\"ActionLabel\"])\n",
    "\n",
    "        verb_targets   = []\n",
    "        noun_targets   = []\n",
    "        action_targets = []\n",
    "\n",
    "        for h_sec in self.horizons_s:\n",
    "            future_frame = obs_end + int(round(h_sec * self.fps))\n",
    "            seg = labels_df[(labels_df[\"StartFrame\"] <= future_frame) &\n",
    "                            (labels_df[\"EndFrame\"]   >= future_frame)]\n",
    "            if seg.empty:\n",
    "                # nothing happening at that exact time\n",
    "                verb_targets.append(IGNORE_INDEX)\n",
    "                noun_targets.append(IGNORE_INDEX)\n",
    "                action_targets.append(IGNORE_INDEX)\n",
    "            else:\n",
    "                row = seg.iloc[0]\n",
    "                if vcol is not None and not pd.isna(row[vcol]):\n",
    "                    verb_targets.append(int(row[vcol]))\n",
    "                else:\n",
    "                    verb_targets.append(IGNORE_INDEX)\n",
    "\n",
    "                if ncol is not None and not pd.isna(row[ncol]):\n",
    "                    noun_targets.append(int(row[ncol]))\n",
    "                else:\n",
    "                    noun_targets.append(IGNORE_INDEX)\n",
    "\n",
    "                if acol is not None and not pd.isna(row[acol]):\n",
    "                    action_targets.append(int(row[acol]))\n",
    "                else:\n",
    "                    action_targets.append(IGNORE_INDEX)\n",
    "\n",
    "        return {\n",
    "            \"verb\":   torch.LongTensor(verb_targets),\n",
    "            \"noun\":   torch.LongTensor(noun_targets),\n",
    "            \"action\": torch.LongTensor(action_targets)\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rec = self.samples[idx]\n",
    "        obs_end = rec[\"obs_end\"]\n",
    "        obs_start = obs_end - (self.t_obs - 1)\n",
    "        if obs_start < 0:\n",
    "            obs_start = 0\n",
    "            obs_end = obs_start + (self.t_obs - 1)\n",
    "\n",
    "        fused_idx_min = int(self.fused_df.index.min())\n",
    "        fused_idx_max = int(self.fused_df.index.max())\n",
    "        obs_end = min(obs_end, fused_idx_max)\n",
    "        obs_start = max(obs_end - (self.t_obs - 1), fused_idx_min)\n",
    "\n",
    "        desired = list(range(obs_start, obs_end + 1))\n",
    "        sel = self.fused_df.reindex(desired).fillna(method=\"ffill\").fillna(method=\"bfill\").fillna(0.0)\n",
    "\n",
    "        if sel.shape[0] < self.t_obs:\n",
    "            if sel.shape[0] == 0:\n",
    "                zero_row = {c:0.0 for c in self.feat_cols}\n",
    "                sel = pd.DataFrame([zero_row] * self.t_obs)\n",
    "            else:\n",
    "                first = sel.iloc[[0]]\n",
    "                pads = pd.concat([first] * (self.t_obs - sel.shape[0]), ignore_index=True)\n",
    "                sel = pd.concat([pads, sel.reset_index(drop=True)], ignore_index=True)\n",
    "\n",
    "        for c in self.feat_cols:\n",
    "            if c not in sel.columns:\n",
    "                sel[c] = 0.0\n",
    "\n",
    "        F_window = torch.from_numpy(sel[self.feat_cols].values).float()   # (T_obs, FEAT_DIM)\n",
    "\n",
    "        y_multi = self._time_based_future_labels(obs_end)\n",
    "\n",
    "        meta = {\"obs_start\": int(obs_start),\n",
    "                \"obs_end\":   int(obs_end),\n",
    "                \"label_row_idx\": int(rec[\"label_row_idx\"])}\n",
    "        return F_window, y_multi, meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af6052c",
   "metadata": {},
   "source": [
    "### Graph Construction\n",
    "_Graph Construction using kNN strategy and then apply GAT_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd3cb459",
   "metadata": {},
   "outputs": [],
   "source": [
    "K=5  \n",
    "DROP=0.1\n",
    "\n",
    "def build_topk_edge_index(features: torch.Tensor, k=K):\n",
    "    Tn = int(features.size(0))\n",
    "    x = F.normalize(features, dim=1)\n",
    "    sim = torch.matmul(x, x.t())   # (T,T) T is the number of frame-> features\n",
    "    sim.fill_diagonal_(-1.0)\n",
    "    vals, idxs = torch.topk(sim, k, dim=1)\n",
    "    src = torch.arange(Tn).unsqueeze(1).expand(-1, k).reshape(-1)\n",
    "    dst = idxs.reshape(-1)\n",
    "    edge = torch.stack([src, dst], dim=0)\n",
    "    edge_rev = torch.stack([dst, src], dim=0)\n",
    "    return torch.cat([edge, edge_rev], dim=1).long()\n",
    "\n",
    "class BatchedGAT(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim=None, num_layers=3, heads=8, dropout=DROP):\n",
    "        super().__init__()\n",
    "        hid = hid_dim or in_dim\n",
    "        self.convs = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_ch = in_dim if i==0 else hid\n",
    "            self.convs.append(GATConv(in_ch, hid//heads, heads=heads, concat=True, dropout=dropout))\n",
    "        self.proj = nn.Linear(hid, in_dim)\n",
    "        self.norm = nn.LayerNorm(in_dim)\n",
    "        self.act = nn.GELU()\n",
    "    def forward(self, pyg_batch: PyGBatch, T_per_sample: int):\n",
    "        x = pyg_batch.x; edge_index = pyg_batch.edge_index\n",
    "        h = x\n",
    "        for conv in self.convs:\n",
    "            h = conv(h, edge_index); h = self.act(h)\n",
    "        h = self.proj(h)\n",
    "        node_feats, mask = to_dense_batch(h, batch=pyg_batch.batch)\n",
    "        B, max_nodes, D = node_feats.shape\n",
    "        if max_nodes < T_per_sample:\n",
    "            pad = torch.zeros(B, T_per_sample - max_nodes, D, device=node_feats.device)\n",
    "            node_feats = torch.cat([node_feats, pad], dim=1)\n",
    "        elif max_nodes > T_per_sample:\n",
    "            node_feats = node_feats[:, :T_per_sample, :]\n",
    "        return self.norm(node_feats) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231b5679",
   "metadata": {},
   "source": [
    "### Fusion (Graph +Transformer) + Transfomer Decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c1a54ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GETR(nn.Module):\n",
    "    def __init__(self, d_model, nhead=8, num_layers=3, dim_feedforward=2048, dropout=DROP, max_len=1000):\n",
    "        super().__init__()\n",
    "        enc = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, activation='gelu', batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(enc, num_layers=num_layers)\n",
    "        self.pos_emb = nn.Parameter(torch.randn(1, max_len, d_model))\n",
    "    def forward(self, x):\n",
    "        B,T,D = x.shape\n",
    "        pos = self.pos_emb[:, :T, :].to(x.device)\n",
    "        return self.encoder(x + pos)\n",
    "\n",
    "class AnticipationModel(nn.Module):\n",
    "    def __init__(self, feat_dim, num_classes: dict, k_fut=5, gat_layers=3, gat_heads=8, dec_layers=3, dec_heads=8, dropout=DROP):\n",
    "        super().__init__()\n",
    "        self.feat_dim = feat_dim; self.k_fut = k_fut\n",
    "        self.gat = BatchedGAT(in_dim=feat_dim, hid_dim=feat_dim, num_layers=gat_layers, heads=gat_heads, dropout=dropout)\n",
    "        self.encoder = GETR(d_model=feat_dim, nhead=dec_heads, num_layers=3)\n",
    "        dec_layer = nn.TransformerDecoderLayer(d_model=feat_dim, nhead=dec_heads, dim_feedforward=feat_dim*4, dropout=dropout, activation='gelu', batch_first=True)\n",
    "        self.decoder = nn.TransformerDecoder(dec_layer, num_layers=dec_layers)\n",
    "        self.queries = nn.Parameter(torch.randn(1, k_fut, feat_dim))\n",
    "        assert isinstance(num_classes, dict)\n",
    "        self.verb_head = nn.Linear(feat_dim, num_classes[\"verb\"])\n",
    "        self.noun_head = nn.Linear(feat_dim, num_classes[\"noun\"])\n",
    "        self.action_head = nn.Linear(feat_dim, num_classes[\"action\"])\n",
    "\n",
    "    def forward(self, F_batch):\n",
    "        # F_batch: (B, T, D)\n",
    "        B,T,D = F_batch.shape; device = F_batch.device\n",
    "        data_list=[]\n",
    "        for b in range(B):\n",
    "            x = F_batch[b]\n",
    "            edge_index = build_topk_edge_index(x.detach().cpu(), k=K).to(device)\n",
    "            data_list.append(PyGData(x=x, edge_index=edge_index))\n",
    "        pyg_batch = PyGBatch.from_data_list(data_list).to(device)\n",
    "        G = self.gat(pyg_batch, T_per_sample=T)   # (B,T,D)\n",
    "        H = self.encoder(F_batch)                 # (B,T,D)\n",
    "        U = H + G\n",
    "        q = self.queries.expand(B, -1, -1).to(device)\n",
    "        dec_out = self.decoder(tgt=q, memory=U)   # (B, K_fut, D)\n",
    "        return {\"verb\": self.verb_head(dec_out),\n",
    "                \"noun\": self.noun_head(dec_out),\n",
    "                \"action\": self.action_head(dec_out)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8877d16",
   "metadata": {},
   "source": [
    "### Cross-Entropy (Masked means Loss for Verb, Noun, Action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87748c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def masked_cross_entropy(logits, labels, ignore_index=IGNORE_INDEX):\n",
    "    B, K, C = logits.shape\n",
    "    logits_flat = logits.view(B * K, C)      # (B*K, C)\n",
    "    labels_flat = labels.view(B * K)         # (B*K,)\n",
    "\n",
    "    loss_flat = F.cross_entropy(\n",
    "        logits_flat,\n",
    "        labels_flat,\n",
    "        reduction='none',\n",
    "        ignore_index=ignore_index\n",
    "    )  # (B*K,)\n",
    "\n",
    "    mask = (labels_flat != ignore_index).float()\n",
    "    valid = mask.sum()\n",
    "\n",
    "    if valid == 0:\n",
    "        return (logits_flat * 0.0).sum()\n",
    "\n",
    "    return (loss_flat * mask).sum() / valid\n",
    "\n",
    "\n",
    "def topk_accuracy_per_task(logits, labels, topk=(1,5), ignore_index=IGNORE_INDEX):\n",
    "    B,K,C = logits.shape\n",
    "    res = {}\n",
    "    overall = {k:0 for k in topk}\n",
    "    total_cnt = 0\n",
    "    preds_topk = logits.topk(max(topk), dim=-1)[1]  # (B,K,maxk)\n",
    "    for h in range(K):\n",
    "        lab = labels[:,h]; mask = (lab != ignore_index); cnt = int(mask.sum().item())\n",
    "        for k in topk:\n",
    "            if cnt == 0:\n",
    "                res.setdefault(f\"per_h{h+1}_top{k}\", None)\n",
    "                continue\n",
    "            predk = preds_topk[:,h,:k]  # (B,k)\n",
    "            lab_exp = lab.unsqueeze(1).expand(-1, k)\n",
    "            hits = (predk == lab_exp)\n",
    "            hit = int(hits[mask].any(dim=1).float().sum().item())\n",
    "            res[f\"per_h{h+1}_top{k}\"] = hit / cnt\n",
    "            overall[k] += hit\n",
    "        total_cnt += cnt\n",
    "    for k in topk:\n",
    "        res[f\"overall_top{k}\"] = overall[k] / total_cnt if total_cnt>0 else None\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77754828",
   "metadata": {},
   "source": [
    "### Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de3e3a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "FUSED_CSV_PATH = r\"EPIC-Kitchens\\Features\\FusedFeatures\\P01_05_fused_features.csv\"\n",
    "LABEL_CSV_PATH = r\"EPIC-Kitchens\\Labels\\P01_05.csv\"\n",
    "BEST_MODEL_PATH = Path(r\"Model\\P01_05_fused_model.pth\")\n",
    "\n",
    "\n",
    "# Hyperparams\n",
    "T_OBS = 90\n",
    "FEAT_DIM = 512\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 20\n",
    "LR = 1e-4\n",
    "WD = 1e-4\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "# === Time-based anticipation config ===\n",
    "FPS = 30.0 \n",
    "HORIZONS_S = [0.25, 0.5, 0.75, 1.0, 1.25, 1.50, 1.75, 2.0] \n",
    "K_FUT = len(HORIZONS_S) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57484c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected classes: {'verb': 81, 'noun': 332, 'action': 123}\n",
      "\n",
      "========== MODEL PARAMETERS ==========\n",
      "Total parameters     : 23.9 M\n",
      "Trainable parameters : 23.92 M\n",
      "=====================================\n",
      "\n",
      "========== MODEL FLOPs ==========\n",
      "MACs  : 0.08 G\n",
      "FLOPs : 0.15 G\n",
      "================================\n",
      "\n",
      "========== INFERENCE LATENCY ==========\n",
      "Average latency per sample: 21.58 ms\n",
      "======================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def detect_num_classes_from_labels_df(labels_df):\n",
    "    verbs = set()\n",
    "    nouns = set()\n",
    "    actions = set()\n",
    "    for cand in [\"Verb_class\", \"verb\", \"Verb\", \"verb_class\"]:\n",
    "        if cand in labels_df.columns:\n",
    "            verbs.update(labels_df[cand].dropna().astype(int).tolist())\n",
    "            break\n",
    "    for cand in [\"Noun_class\", \"noun\", \"Noun\", \"noun_class\"]:\n",
    "        if cand in labels_df.columns:\n",
    "            nouns.update(labels_df[cand].dropna().astype(int).tolist())\n",
    "            break\n",
    "    for cand in [\"Action_class\", \"action\", \"Action\", \"ActionLabel\"]:\n",
    "        if cand in labels_df.columns:\n",
    "            actions.update(labels_df[cand].dropna().astype(int).tolist())\n",
    "            break\n",
    "    nv = (max(verbs) + 1) if len(verbs) > 0 else 1\n",
    "    nn_ = (max(nouns) + 1) if len(nouns) > 0 else 1\n",
    "    na = (max(actions) + 1) if len(actions) > 0 else 1\n",
    "    return {\"verb\": int(nv), \"noun\": int(nn_), \"action\": int(na)}\n",
    "\n",
    "\n",
    "def topk_counts(logits, labels, k):\n",
    "    # logits: (B, K_fut, C); labels: (B, K_fut)\n",
    "    with torch.no_grad():\n",
    "        B, K, C = logits.shape\n",
    "        topk_preds = logits.topk(k, dim=-1)[1]  # (B, K, k)\n",
    "        hits = 0\n",
    "        total = 0\n",
    "        for h in range(K):\n",
    "            lab = labels[:, h]  # (B,)\n",
    "            mask = (lab != IGNORE_INDEX)\n",
    "            if int(mask.sum().item()) == 0:\n",
    "                continue\n",
    "            predk = topk_preds[:, h, :]  # (B, k)\n",
    "            lab_exp = lab.unsqueeze(1).expand(-1, k)\n",
    "            masked_pred = predk[mask]   # (M, k)\n",
    "            masked_lab = lab_exp[mask]  # (M, k)\n",
    "            hit_vec = (masked_pred == masked_lab).any(dim=1).float()\n",
    "            hits += int(hit_vec.sum().item())\n",
    "            total += int(mask.sum().item())\n",
    "        return hits, total\n",
    "\n",
    "\n",
    "# Load fused and labels\n",
    "fused_df = pd.read_csv(FUSED_CSV_PATH)\n",
    "labels_df = pd.read_csv(LABEL_CSV_PATH)\n",
    "\n",
    "# Dataset\n",
    "dataset = SingleVideoAnticipationDataset(\n",
    "    fused_df,\n",
    "    labels_df,\n",
    "    t_obs=T_OBS,\n",
    "    k_fut=K_FUT,        # must equal len(HORIZONS_S)\n",
    "    feat_dim=FEAT_DIM,\n",
    "    fps=FPS,\n",
    "    horizons_s=HORIZONS_S\n",
    ")\n",
    "\n",
    "# split indices for train/val (60/40)\n",
    "indices = list(range(len(dataset)))\n",
    "random.seed(42)\n",
    "random.shuffle(indices)\n",
    "split_at = int(0.6 * len(indices))\n",
    "train_idx = indices[:split_at]\n",
    "val_idx = indices[split_at:]\n",
    "\n",
    "train_ds = Subset(dataset, train_idx)\n",
    "val_ds = Subset(dataset, val_idx)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=(DEVICE == \"cuda\")\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=(DEVICE == \"cuda\")\n",
    ")\n",
    "\n",
    "num_classes = detect_num_classes_from_labels_df(labels_df)\n",
    "print(\"Detected classes:\", num_classes)\n",
    "\n",
    "model = AnticipationModel(\n",
    "    feat_dim=FEAT_DIM,\n",
    "    num_classes=num_classes,\n",
    "    k_fut=K_FUT\n",
    ").to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "# ================= MODEL PARAMETER COUNT =================\n",
    "def count_parameters(model):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "total_params, trainable_params = count_parameters(model)\n",
    "\n",
    "print(\"\\n========== MODEL PARAMETERS ==========\")\n",
    "print(f\"Total parameters     : {total_params/1e6:.1f} M\")\n",
    "print(f\"Trainable parameters : {trainable_params/1e6:.2f} M\")\n",
    "print(\"=====================================\\n\")\n",
    "\n",
    "\n",
    "# ================= MODEL FLOPs =================\n",
    "from thop import profile\n",
    "\n",
    "model.eval()\n",
    "\n",
    "dummy_input = torch.randn(\n",
    "    1, T_OBS, FEAT_DIM\n",
    ").to(DEVICE)\n",
    "\n",
    "macs, params = profile(\n",
    "    model,\n",
    "    inputs=(dummy_input,),\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"========== MODEL FLOPs ==========\")\n",
    "print(f\"MACs  : {macs/1e9:.2f} G\")\n",
    "print(f\"FLOPs : {(2*macs)/1e9:.2f} G\")\n",
    "print(\"================================\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# ================= INFERENCE LATENCY =================\n",
    "def measure_latency(model, device, runs=100):\n",
    "    model.eval()\n",
    "    dummy = torch.randn(1, T_OBS, FEAT_DIM).to(device)\n",
    "\n",
    "    # warm-up\n",
    "    for _ in range(10):\n",
    "        _ = model(dummy)\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    start = time.time()\n",
    "    for _ in range(runs):\n",
    "        _ = model(dummy)\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    avg_latency = (time.time() - start) / runs\n",
    "    return avg_latency * 1000  # ms\n",
    "\n",
    "\n",
    "latency_ms = measure_latency(model, DEVICE)\n",
    "\n",
    "print(\"========== INFERENCE LATENCY ==========\")\n",
    "print(f\"Average latency per sample: {latency_ms:.2f} ms\")\n",
    "print(\"======================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d88f76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam(model.parameters(), lr=LR, weight_decay=WD)\n",
    "sched = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=3\n",
    ")\n",
    "\n",
    "best_val_loss = float(\"inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "112f5f9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.0001\n",
       "    maximize: False\n",
       "    weight_decay: 0.0001\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03e2ab8",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7212978",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 Train:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 Val:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 | Time 5.7s\n",
      "  Train Loss: 9.1144 | Val Loss: 9.2078\n",
      "  VERB   Train Top1: 0.14446227929373998, Top5: 0.5601926163723917; Val Top1: 0.05511811023622047, Top5: 0.6404199475065617\n",
      "  NOUN   Train Top1: 0.08186195826645265, Top5: 0.2504012841091493; Val Top1: 0.015748031496062992, Top5: 0.2992125984251969\n",
      "  ACTION Train Top1: 0.033707865168539325, Top5: 0.16051364365971107; Val Top1: 0.0, Top5: 0.015748031496062992\n",
      "  VERB   Val Precision: 0.0037, Recall: 0.0667, F1: 0.0070\n",
      "  NOUN   Val Precision: 0.0005, Recall: 0.0323, F1: 0.0010\n",
      "  ACTION Val Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "  ---- Mean Top-5 Recall (validation) ----\n",
      "     VERB    Mean Top-5 Recall: 0.6323922134275302\n",
      "     NOUN    Mean Top-5 Recall: 0.2991466414430677\n",
      "     ACTION  Mean Top-5 Recall: 0.015830903523067503\n",
      "  VERB   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.058823529411764705  Top5: 0.5882352941176471\n",
      "    @ 0.50s  Top1: 0.05405405405405406  Top5: 0.5675675675675675\n",
      "    @ 0.75s  Top1: 0.045454545454545456  Top5: 0.5909090909090909\n",
      "    @ 1.00s  Top1: 0.0625  Top5: 0.6041666666666666\n",
      "    @ 1.25s  Top1: 0.0625  Top5: 0.625\n",
      "    @ 1.50s  Top1: 0.05454545454545454  Top5: 0.7090909090909091\n",
      "    @ 1.75s  Top1: 0.05263157894736842  Top5: 0.7017543859649122\n",
      "    @ 2.00s  Top1: 0.05172413793103448  Top5: 0.6724137931034483\n",
      "    overall_top1: 0.05511811023622047, overall_top5: 0.6404199475065617\n",
      "  NOUN   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.029411764705882353  Top5: 0.3235294117647059\n",
      "    @ 0.50s  Top1: 0.02702702702702703  Top5: 0.2972972972972973\n",
      "    @ 0.75s  Top1: 0.0  Top5: 0.29545454545454547\n",
      "    @ 1.00s  Top1: 0.0  Top5: 0.2708333333333333\n",
      "    @ 1.25s  Top1: 0.0  Top5: 0.2708333333333333\n",
      "    @ 1.50s  Top1: 0.01818181818181818  Top5: 0.3090909090909091\n",
      "    @ 1.75s  Top1: 0.017543859649122806  Top5: 0.3157894736842105\n",
      "    @ 2.00s  Top1: 0.034482758620689655  Top5: 0.3103448275862069\n",
      "    overall_top1: 0.015748031496062992, overall_top5: 0.2992125984251969\n",
      "  ACTION per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.0  Top5: 0.029411764705882353\n",
      "    @ 0.50s  Top1: 0.0  Top5: 0.02702702702702703\n",
      "    @ 0.75s  Top1: 0.0  Top5: 0.0\n",
      "    @ 1.00s  Top1: 0.0  Top5: 0.0\n",
      "    @ 1.25s  Top1: 0.0  Top5: 0.0\n",
      "    @ 1.50s  Top1: 0.0  Top5: 0.01818181818181818\n",
      "    @ 1.75s  Top1: 0.0  Top5: 0.017543859649122806\n",
      "    @ 2.00s  Top1: 0.0  Top5: 0.034482758620689655\n",
      "    overall_top1: 0.0, overall_top5: 0.015748031496062992\n",
      "[SAVED BEST] -> Model\\P01_05_fused_model.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 Train:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 Val:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 | Time 5.6s\n",
      "  Train Loss: 7.6256 | Val Loss: 8.7153\n",
      "  VERB   Train Top1: 0.14125200642054575, Top5: 0.7078651685393258; Val Top1: 0.2125984251968504, Top5: 0.6272965879265092\n",
      "  NOUN   Train Top1: 0.1492776886035313, Top5: 0.42215088282504015; Val Top1: 0.015748031496062992, Top5: 0.19160104986876642\n",
      "  ACTION Train Top1: 0.056179775280898875, Top5: 0.24398073836276082; Val Top1: 0.0, Top5: 0.07611548556430446\n",
      "  VERB   Val Precision: 0.0299, Recall: 0.0553, F1: 0.0370\n",
      "  NOUN   Val Precision: 0.0005, Recall: 0.0323, F1: 0.0010\n",
      "  ACTION Val Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "  ---- Mean Top-5 Recall (validation) ----\n",
      "     VERB    Mean Top-5 Recall: 0.6194906940299676\n",
      "     NOUN    Mean Top-5 Recall: 0.19366243469611677\n",
      "     ACTION  Mean Top-5 Recall: 0.07775373082218257\n",
      "  VERB   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.23529411764705882  Top5: 0.5588235294117647\n",
      "    @ 0.50s  Top1: 0.16216216216216217  Top5: 0.5675675675675675\n",
      "    @ 0.75s  Top1: 0.1590909090909091  Top5: 0.5909090909090909\n",
      "    @ 1.00s  Top1: 0.20833333333333334  Top5: 0.6041666666666666\n",
      "    @ 1.25s  Top1: 0.20833333333333334  Top5: 0.6041666666666666\n",
      "    @ 1.50s  Top1: 0.21818181818181817  Top5: 0.6909090909090909\n",
      "    @ 1.75s  Top1: 0.2807017543859649  Top5: 0.6842105263157895\n",
      "    @ 2.00s  Top1: 0.20689655172413793  Top5: 0.6551724137931034\n",
      "    overall_top1: 0.2125984251968504, overall_top5: 0.6272965879265092\n",
      "  NOUN   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.029411764705882353  Top5: 0.23529411764705882\n",
      "    @ 0.50s  Top1: 0.02702702702702703  Top5: 0.21621621621621623\n",
      "    @ 0.75s  Top1: 0.0  Top5: 0.18181818181818182\n",
      "    @ 1.00s  Top1: 0.0  Top5: 0.16666666666666666\n",
      "    @ 1.25s  Top1: 0.0  Top5: 0.16666666666666666\n",
      "    @ 1.50s  Top1: 0.01818181818181818  Top5: 0.2\n",
      "    @ 1.75s  Top1: 0.017543859649122806  Top5: 0.19298245614035087\n",
      "    @ 2.00s  Top1: 0.034482758620689655  Top5: 0.1896551724137931\n",
      "    overall_top1: 0.015748031496062992, overall_top5: 0.19160104986876642\n",
      "  ACTION per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.0  Top5: 0.11764705882352941\n",
      "    @ 0.50s  Top1: 0.0  Top5: 0.10810810810810811\n",
      "    @ 0.75s  Top1: 0.0  Top5: 0.045454545454545456\n",
      "    @ 1.00s  Top1: 0.0  Top5: 0.041666666666666664\n",
      "    @ 1.25s  Top1: 0.0  Top5: 0.0625\n",
      "    @ 1.50s  Top1: 0.0  Top5: 0.07272727272727272\n",
      "    @ 1.75s  Top1: 0.0  Top5: 0.08771929824561403\n",
      "    @ 2.00s  Top1: 0.0  Top5: 0.08620689655172414\n",
      "    overall_top1: 0.0, overall_top5: 0.07611548556430446\n",
      "[SAVED BEST] -> Model\\P01_05_fused_model.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3 Train:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3 Val:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 | Time 5.6s\n",
      "  Train Loss: 7.0397 | Val Loss: 8.8476\n",
      "  VERB   Train Top1: 0.18138041733547353, Top5: 0.7158908507223114; Val Top1: 0.14435695538057744, Top5: 0.6272965879265092\n",
      "  NOUN   Train Top1: 0.13001605136436598, Top5: 0.4446227929373997; Val Top1: 0.05774278215223097, Top5: 0.18110236220472442\n",
      "  ACTION Train Top1: 0.07062600321027288, Top5: 0.2953451043338684; Val Top1: 0.015748031496062992, Top5: 0.023622047244094488\n",
      "  VERB   Val Precision: 0.0235, Recall: 0.0693, F1: 0.0303\n",
      "  NOUN   Val Precision: 0.0269, Recall: 0.0566, F1: 0.0199\n",
      "  ACTION Val Precision: 0.0003, Recall: 0.0196, F1: 0.0006\n",
      "  ---- Mean Top-5 Recall (validation) ----\n",
      "     VERB    Mean Top-5 Recall: 0.6194906940299676\n",
      "     NOUN    Mean Top-5 Recall: 0.18386920406860757\n",
      "     ACTION  Mean Top-5 Recall: 0.022451785665728228\n",
      "  VERB   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.17647058823529413  Top5: 0.5588235294117647\n",
      "    @ 0.50s  Top1: 0.08108108108108109  Top5: 0.5675675675675675\n",
      "    @ 0.75s  Top1: 0.13636363636363635  Top5: 0.5909090909090909\n",
      "    @ 1.00s  Top1: 0.08333333333333333  Top5: 0.6041666666666666\n",
      "    @ 1.25s  Top1: 0.14583333333333334  Top5: 0.6041666666666666\n",
      "    @ 1.50s  Top1: 0.14545454545454545  Top5: 0.6909090909090909\n",
      "    @ 1.75s  Top1: 0.19298245614035087  Top5: 0.6842105263157895\n",
      "    @ 2.00s  Top1: 0.1724137931034483  Top5: 0.6551724137931034\n",
      "    overall_top1: 0.14435695538057744, overall_top5: 0.6272965879265092\n",
      "  NOUN   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.08823529411764706  Top5: 0.23529411764705882\n",
      "    @ 0.50s  Top1: 0.08108108108108109  Top5: 0.21621621621621623\n",
      "    @ 0.75s  Top1: 0.045454545454545456  Top5: 0.1590909090909091\n",
      "    @ 1.00s  Top1: 0.041666666666666664  Top5: 0.14583333333333334\n",
      "    @ 1.25s  Top1: 0.041666666666666664  Top5: 0.16666666666666666\n",
      "    @ 1.50s  Top1: 0.05454545454545454  Top5: 0.2\n",
      "    @ 1.75s  Top1: 0.05263157894736842  Top5: 0.17543859649122806\n",
      "    @ 2.00s  Top1: 0.06896551724137931  Top5: 0.1724137931034483\n",
      "    overall_top1: 0.05774278215223097, overall_top5: 0.18110236220472442\n",
      "  ACTION per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.029411764705882353  Top5: 0.029411764705882353\n",
      "    @ 0.50s  Top1: 0.02702702702702703  Top5: 0.02702702702702703\n",
      "    @ 0.75s  Top1: 0.0  Top5: 0.0\n",
      "    @ 1.00s  Top1: 0.0  Top5: 0.0\n",
      "    @ 1.25s  Top1: 0.0  Top5: 0.0\n",
      "    @ 1.50s  Top1: 0.01818181818181818  Top5: 0.03636363636363636\n",
      "    @ 1.75s  Top1: 0.017543859649122806  Top5: 0.03508771929824561\n",
      "    @ 2.00s  Top1: 0.034482758620689655  Top5: 0.05172413793103448\n",
      "    overall_top1: 0.015748031496062992, overall_top5: 0.023622047244094488\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4 Train:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4 Val:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 | Time 5.5s\n",
      "  Train Loss: 6.6104 | Val Loss: 8.7230\n",
      "  VERB   Train Top1: 0.2102728731942215, Top5: 0.7255216693418941; Val Top1: 0.18110236220472442, Top5: 0.6981627296587927\n",
      "  NOUN   Train Top1: 0.20545746388443017, Top5: 0.4478330658105939; Val Top1: 0.05511811023622047, Top5: 0.28346456692913385\n",
      "  ACTION Train Top1: 0.13162118780096307, Top5: 0.3563402889245586; Val Top1: 0.0, Top5: 0.06561679790026247\n",
      "  VERB   Val Precision: 0.0483, Recall: 0.0760, F1: 0.0399\n",
      "  NOUN   Val Precision: 0.0105, Recall: 0.0566, F1: 0.0133\n",
      "  ACTION Val Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "  ---- Mean Top-5 Recall (validation) ----\n",
      "     VERB    Mean Top-5 Recall: 0.6912864967731787\n",
      "     NOUN    Mean Top-5 Recall: 0.2869464685486024\n",
      "     ACTION  Mean Top-5 Recall: 0.06742303090720407\n",
      "  VERB   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.17647058823529413  Top5: 0.6470588235294118\n",
      "    @ 0.50s  Top1: 0.08108108108108109  Top5: 0.6486486486486487\n",
      "    @ 0.75s  Top1: 0.18181818181818182  Top5: 0.6590909090909091\n",
      "    @ 1.00s  Top1: 0.1875  Top5: 0.6666666666666666\n",
      "    @ 1.25s  Top1: 0.1875  Top5: 0.6666666666666666\n",
      "    @ 1.50s  Top1: 0.18181818181818182  Top5: 0.7636363636363637\n",
      "    @ 1.75s  Top1: 0.21052631578947367  Top5: 0.7543859649122807\n",
      "    @ 2.00s  Top1: 0.20689655172413793  Top5: 0.7241379310344828\n",
      "    overall_top1: 0.18110236220472442, overall_top5: 0.6981627296587927\n",
      "  NOUN   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.08823529411764706  Top5: 0.35294117647058826\n",
      "    @ 0.50s  Top1: 0.08108108108108109  Top5: 0.32432432432432434\n",
      "    @ 0.75s  Top1: 0.045454545454545456  Top5: 0.25\n",
      "    @ 1.00s  Top1: 0.041666666666666664  Top5: 0.2916666666666667\n",
      "    @ 1.25s  Top1: 0.041666666666666664  Top5: 0.22916666666666666\n",
      "    @ 1.50s  Top1: 0.05454545454545454  Top5: 0.2909090909090909\n",
      "    @ 1.75s  Top1: 0.05263157894736842  Top5: 0.2807017543859649\n",
      "    @ 2.00s  Top1: 0.05172413793103448  Top5: 0.27586206896551724\n",
      "    overall_top1: 0.05511811023622047, overall_top5: 0.28346456692913385\n",
      "  ACTION per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.0  Top5: 0.11764705882352941\n",
      "    @ 0.50s  Top1: 0.0  Top5: 0.08108108108108109\n",
      "    @ 0.75s  Top1: 0.0  Top5: 0.045454545454545456\n",
      "    @ 1.00s  Top1: 0.0  Top5: 0.041666666666666664\n",
      "    @ 1.25s  Top1: 0.0  Top5: 0.041666666666666664\n",
      "    @ 1.50s  Top1: 0.0  Top5: 0.07272727272727272\n",
      "    @ 1.75s  Top1: 0.0  Top5: 0.07017543859649122\n",
      "    @ 2.00s  Top1: 0.0  Top5: 0.06896551724137931\n",
      "    overall_top1: 0.0, overall_top5: 0.06561679790026247\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5 Train:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5 Val:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 | Time 5.3s\n",
      "  Train Loss: 5.8192 | Val Loss: 8.5952\n",
      "  VERB   Train Top1: 0.2102728731942215, Top5: 0.7624398073836276; Val Top1: 0.25196850393700787, Top5: 0.6797900262467191\n",
      "  NOUN   Train Top1: 0.29213483146067415, Top5: 0.5585874799357945; Val Top1: 0.16272965879265092, Top5: 0.29133858267716534\n",
      "  ACTION Train Top1: 0.18459069020866772, Top5: 0.43659711075441415; Val Top1: 0.07874015748031496, Top5: 0.13910761154855644\n",
      "  VERB   Val Precision: 0.1160, Recall: 0.1458, F1: 0.1101\n",
      "  NOUN   Val Precision: 0.0261, Recall: 0.0735, F1: 0.0335\n",
      "  ACTION Val Precision: 0.0173, Recall: 0.0406, F1: 0.0236\n",
      "  ---- Mean Top-5 Recall (validation) ----\n",
      "     VERB    Mean Top-5 Recall: 0.6721656899063286\n",
      "     NOUN    Mean Top-5 Recall: 0.29231187476540177\n",
      "     ACTION  Mean Top-5 Recall: 0.1400080727910476\n",
      "  VERB   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.2647058823529412  Top5: 0.6176470588235294\n",
      "    @ 0.50s  Top1: 0.21621621621621623  Top5: 0.6216216216216216\n",
      "    @ 0.75s  Top1: 0.2727272727272727  Top5: 0.6363636363636364\n",
      "    @ 1.00s  Top1: 0.2708333333333333  Top5: 0.6458333333333334\n",
      "    @ 1.25s  Top1: 0.25  Top5: 0.6666666666666666\n",
      "    @ 1.50s  Top1: 0.2545454545454545  Top5: 0.7454545454545455\n",
      "    @ 1.75s  Top1: 0.24561403508771928  Top5: 0.7368421052631579\n",
      "    @ 2.00s  Top1: 0.2413793103448276  Top5: 0.7068965517241379\n",
      "    overall_top1: 0.25196850393700787, overall_top5: 0.6797900262467191\n",
      "  NOUN   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.20588235294117646  Top5: 0.3235294117647059\n",
      "    @ 0.50s  Top1: 0.1891891891891892  Top5: 0.2972972972972973\n",
      "    @ 0.75s  Top1: 0.1590909090909091  Top5: 0.29545454545454547\n",
      "    @ 1.00s  Top1: 0.125  Top5: 0.25\n",
      "    @ 1.25s  Top1: 0.16666666666666666  Top5: 0.2708333333333333\n",
      "    @ 1.50s  Top1: 0.14545454545454545  Top5: 0.32727272727272727\n",
      "    @ 1.75s  Top1: 0.17543859649122806  Top5: 0.2982456140350877\n",
      "    @ 2.00s  Top1: 0.15517241379310345  Top5: 0.27586206896551724\n",
      "    overall_top1: 0.16272965879265092, overall_top5: 0.29133858267716534\n",
      "  ACTION per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.08823529411764706  Top5: 0.17647058823529413\n",
      "    @ 0.50s  Top1: 0.10810810810810811  Top5: 0.16216216216216217\n",
      "    @ 0.75s  Top1: 0.09090909090909091  Top5: 0.11363636363636363\n",
      "    @ 1.00s  Top1: 0.041666666666666664  Top5: 0.10416666666666667\n",
      "    @ 1.25s  Top1: 0.08333333333333333  Top5: 0.10416666666666667\n",
      "    @ 1.50s  Top1: 0.07272727272727272  Top5: 0.16363636363636364\n",
      "    @ 1.75s  Top1: 0.08771929824561403  Top5: 0.15789473684210525\n",
      "    @ 2.00s  Top1: 0.06896551724137931  Top5: 0.13793103448275862\n",
      "    overall_top1: 0.07874015748031496, overall_top5: 0.13910761154855644\n",
      "[SAVED BEST] -> Model\\P01_05_fused_model.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6 Train:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6 Val:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 | Time 5.6s\n",
      "  Train Loss: 5.6809 | Val Loss: 8.4189\n",
      "  VERB   Train Top1: 0.26163723916532905, Top5: 0.8073836276083467; Val Top1: 0.2992125984251969, Top5: 0.7847769028871391\n",
      "  NOUN   Train Top1: 0.32263242375601925, Top5: 0.6356340288924559; Val Top1: 0.15748031496062992, Top5: 0.3123359580052493\n",
      "  ACTION Train Top1: 0.20064205457463885, Top5: 0.5553772070626003; Val Top1: 0.05774278215223097, Top5: 0.2755905511811024\n",
      "  VERB   Val Precision: 0.1154, Recall: 0.2025, F1: 0.1441\n",
      "  NOUN   Val Precision: 0.0588, Recall: 0.1026, F1: 0.0610\n",
      "  ACTION Val Precision: 0.0170, Recall: 0.0290, F1: 0.0199\n",
      "  ---- Mean Top-5 Recall (validation) ----\n",
      "     VERB    Mean Top-5 Recall: 0.7813192966433732\n",
      "     NOUN    Mean Top-5 Recall: 0.31366759886263196\n",
      "     ACTION  Mean Top-5 Recall: 0.2804676318604871\n",
      "  VERB   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.38235294117647056  Top5: 0.7647058823529411\n",
      "    @ 0.50s  Top1: 0.2972972972972973  Top5: 0.7567567567567568\n",
      "    @ 0.75s  Top1: 0.3181818181818182  Top5: 0.75\n",
      "    @ 1.00s  Top1: 0.3125  Top5: 0.7708333333333334\n",
      "    @ 1.25s  Top1: 0.2708333333333333  Top5: 0.7708333333333334\n",
      "    @ 1.50s  Top1: 0.21818181818181817  Top5: 0.8545454545454545\n",
      "    @ 1.75s  Top1: 0.3333333333333333  Top5: 0.8070175438596491\n",
      "    @ 2.00s  Top1: 0.29310344827586204  Top5: 0.7758620689655172\n",
      "    overall_top1: 0.2992125984251969, overall_top5: 0.7847769028871391\n",
      "  NOUN   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.20588235294117646  Top5: 0.35294117647058826\n",
      "    @ 0.50s  Top1: 0.1891891891891892  Top5: 0.32432432432432434\n",
      "    @ 0.75s  Top1: 0.1590909090909091  Top5: 0.3181818181818182\n",
      "    @ 1.00s  Top1: 0.14583333333333334  Top5: 0.2708333333333333\n",
      "    @ 1.25s  Top1: 0.14583333333333334  Top5: 0.2708333333333333\n",
      "    @ 1.50s  Top1: 0.14545454545454545  Top5: 0.36363636363636365\n",
      "    @ 1.75s  Top1: 0.14035087719298245  Top5: 0.2982456140350877\n",
      "    @ 2.00s  Top1: 0.15517241379310345  Top5: 0.3103448275862069\n",
      "    overall_top1: 0.15748031496062992, overall_top5: 0.3123359580052493\n",
      "  ACTION per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.11764705882352941  Top5: 0.35294117647058826\n",
      "    @ 0.50s  Top1: 0.10810810810810811  Top5: 0.32432432432432434\n",
      "    @ 0.75s  Top1: 0.045454545454545456  Top5: 0.29545454545454547\n",
      "    @ 1.00s  Top1: 0.041666666666666664  Top5: 0.22916666666666666\n",
      "    @ 1.25s  Top1: 0.041666666666666664  Top5: 0.22916666666666666\n",
      "    @ 1.50s  Top1: 0.05454545454545454  Top5: 0.2909090909090909\n",
      "    @ 1.75s  Top1: 0.03508771929824561  Top5: 0.2631578947368421\n",
      "    @ 2.00s  Top1: 0.05172413793103448  Top5: 0.25862068965517243\n",
      "    overall_top1: 0.05774278215223097, overall_top5: 0.2755905511811024\n",
      "[SAVED BEST] -> Model\\P01_05_fused_model.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7 Train:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7 Val:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 | Time 6.0s\n",
      "  Train Loss: 4.7991 | Val Loss: 8.1295\n",
      "  VERB   Train Top1: 0.3258426966292135, Top5: 0.8715890850722311; Val Top1: 0.2992125984251969, Top5: 0.8162729658792651\n",
      "  NOUN   Train Top1: 0.3884430176565008, Top5: 0.6966292134831461; Val Top1: 0.17060367454068243, Top5: 0.3648293963254593\n",
      "  ACTION Train Top1: 0.3402889245585875, Top5: 0.7207062600321027; Val Top1: 0.08923884514435695, Top5: 0.2650918635170604\n",
      "  VERB   Val Precision: 0.1600, Recall: 0.2124, F1: 0.1502\n",
      "  NOUN   Val Precision: 0.0827, Recall: 0.1161, F1: 0.0671\n",
      "  ACTION Val Precision: 0.0463, Recall: 0.0599, F1: 0.0388\n",
      "  ---- Mean Top-5 Recall (validation) ----\n",
      "     VERB    Mean Top-5 Recall: 0.8113540126723042\n",
      "     NOUN    Mean Top-5 Recall: 0.36987789544921634\n",
      "     ACTION  Mean Top-5 Recall: 0.27076909820267486\n",
      "  VERB   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.23529411764705882  Top5: 0.7647058823529411\n",
      "    @ 0.50s  Top1: 0.16216216216216217  Top5: 0.7837837837837838\n",
      "    @ 0.75s  Top1: 0.25  Top5: 0.7954545454545454\n",
      "    @ 1.00s  Top1: 0.2708333333333333  Top5: 0.7916666666666666\n",
      "    @ 1.25s  Top1: 0.3333333333333333  Top5: 0.8125\n",
      "    @ 1.50s  Top1: 0.34545454545454546  Top5: 0.8727272727272727\n",
      "    @ 1.75s  Top1: 0.38596491228070173  Top5: 0.8596491228070176\n",
      "    @ 2.00s  Top1: 0.3275862068965517  Top5: 0.8103448275862069\n",
      "    overall_top1: 0.2992125984251969, overall_top5: 0.8162729658792651\n",
      "  NOUN   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.20588235294117646  Top5: 0.4411764705882353\n",
      "    @ 0.50s  Top1: 0.21621621621621623  Top5: 0.43243243243243246\n",
      "    @ 0.75s  Top1: 0.18181818181818182  Top5: 0.36363636363636365\n",
      "    @ 1.00s  Top1: 0.14583333333333334  Top5: 0.3333333333333333\n",
      "    @ 1.25s  Top1: 0.16666666666666666  Top5: 0.3125\n",
      "    @ 1.50s  Top1: 0.16363636363636364  Top5: 0.34545454545454546\n",
      "    @ 1.75s  Top1: 0.15789473684210525  Top5: 0.3684210526315789\n",
      "    @ 2.00s  Top1: 0.15517241379310345  Top5: 0.3620689655172414\n",
      "    overall_top1: 0.17060367454068243, overall_top5: 0.3648293963254593\n",
      "  ACTION per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.14705882352941177  Top5: 0.35294117647058826\n",
      "    @ 0.50s  Top1: 0.10810810810810811  Top5: 0.32432432432432434\n",
      "    @ 0.75s  Top1: 0.09090909090909091  Top5: 0.25\n",
      "    @ 1.00s  Top1: 0.0625  Top5: 0.25\n",
      "    @ 1.25s  Top1: 0.08333333333333333  Top5: 0.22916666666666666\n",
      "    @ 1.50s  Top1: 0.09090909090909091  Top5: 0.2727272727272727\n",
      "    @ 1.75s  Top1: 0.07017543859649122  Top5: 0.24561403508771928\n",
      "    @ 2.00s  Top1: 0.08620689655172414  Top5: 0.2413793103448276\n",
      "    overall_top1: 0.08923884514435695, overall_top5: 0.2650918635170604\n",
      "[SAVED BEST] -> Model\\P01_05_fused_model.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8 Train:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8 Val:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 | Time 5.8s\n",
      "  Train Loss: 4.0443 | Val Loss: 8.1580\n",
      "  VERB   Train Top1: 0.4333868378812199, Top5: 0.9020866773675762; Val Top1: 0.24146981627296588, Top5: 0.8031496062992126\n",
      "  NOUN   Train Top1: 0.4959871589085072, Top5: 0.8523274478330658; Val Top1: 0.17585301837270342, Top5: 0.4094488188976378\n",
      "  ACTION Train Top1: 0.43659711075441415, Top5: 0.8523274478330658; Val Top1: 0.13123359580052493, Top5: 0.25984251968503935\n",
      "  VERB   Val Precision: 0.0779, Recall: 0.1357, F1: 0.0930\n",
      "  NOUN   Val Precision: 0.1106, Recall: 0.1118, F1: 0.0879\n",
      "  ACTION Val Precision: 0.0763, Recall: 0.0978, F1: 0.0773\n",
      "  ---- Mean Top-5 Recall (validation) ----\n",
      "     VERB    Mean Top-5 Recall: 0.7980270387651353\n",
      "     NOUN    Mean Top-5 Recall: 0.4107772704506918\n",
      "     ACTION  Mean Top-5 Recall: 0.26326525498318765\n",
      "  VERB   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.23529411764705882  Top5: 0.7647058823529411\n",
      "    @ 0.50s  Top1: 0.24324324324324326  Top5: 0.7567567567567568\n",
      "    @ 0.75s  Top1: 0.22727272727272727  Top5: 0.7727272727272727\n",
      "    @ 1.00s  Top1: 0.22916666666666666  Top5: 0.7916666666666666\n",
      "    @ 1.25s  Top1: 0.2708333333333333  Top5: 0.7916666666666666\n",
      "    @ 1.50s  Top1: 0.2545454545454545  Top5: 0.8545454545454545\n",
      "    @ 1.75s  Top1: 0.22807017543859648  Top5: 0.8245614035087719\n",
      "    @ 2.00s  Top1: 0.2413793103448276  Top5: 0.8275862068965517\n",
      "    overall_top1: 0.24146981627296588, overall_top5: 0.8031496062992126\n",
      "  NOUN   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.20588235294117646  Top5: 0.47058823529411764\n",
      "    @ 0.50s  Top1: 0.21621621621621623  Top5: 0.40540540540540543\n",
      "    @ 0.75s  Top1: 0.20454545454545456  Top5: 0.38636363636363635\n",
      "    @ 1.00s  Top1: 0.14583333333333334  Top5: 0.3958333333333333\n",
      "    @ 1.25s  Top1: 0.16666666666666666  Top5: 0.375\n",
      "    @ 1.50s  Top1: 0.16363636363636364  Top5: 0.41818181818181815\n",
      "    @ 1.75s  Top1: 0.15789473684210525  Top5: 0.42105263157894735\n",
      "    @ 2.00s  Top1: 0.1724137931034483  Top5: 0.41379310344827586\n",
      "    overall_top1: 0.17585301837270342, overall_top5: 0.4094488188976378\n",
      "  ACTION per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.14705882352941177  Top5: 0.3235294117647059\n",
      "    @ 0.50s  Top1: 0.1891891891891892  Top5: 0.2972972972972973\n",
      "    @ 0.75s  Top1: 0.11363636363636363  Top5: 0.25\n",
      "    @ 1.00s  Top1: 0.08333333333333333  Top5: 0.22916666666666666\n",
      "    @ 1.25s  Top1: 0.125  Top5: 0.22916666666666666\n",
      "    @ 1.50s  Top1: 0.12727272727272726  Top5: 0.2727272727272727\n",
      "    @ 1.75s  Top1: 0.14035087719298245  Top5: 0.24561403508771928\n",
      "    @ 2.00s  Top1: 0.13793103448275862  Top5: 0.25862068965517243\n",
      "    overall_top1: 0.13123359580052493, overall_top5: 0.25984251968503935\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9 Train:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9 Val:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 | Time 5.7s\n",
      "  Train Loss: 3.2306 | Val Loss: 8.1727\n",
      "  VERB   Train Top1: 0.5521669341894061, Top5: 0.9486356340288925; Val Top1: 0.2020997375328084, Top5: 0.7926509186351706\n",
      "  NOUN   Train Top1: 0.6067415730337079, Top5: 0.8860353130016051; Val Top1: 0.1968503937007874, Top5: 0.4409448818897638\n",
      "  ACTION Train Top1: 0.6500802568218299, Top5: 0.9438202247191011; Val Top1: 0.11023622047244094, Top5: 0.2992125984251969\n",
      "  VERB   Val Precision: 0.0845, Recall: 0.1331, F1: 0.0906\n",
      "  NOUN   Val Precision: 0.1290, Recall: 0.1525, F1: 0.1124\n",
      "  ACTION Val Precision: 0.0613, Recall: 0.0758, F1: 0.0657\n",
      "  ---- Mean Top-5 Recall (validation) ----\n",
      "     VERB    Mean Top-5 Recall: 0.7841672763324239\n",
      "     NOUN    Mean Top-5 Recall: 0.4443558747800034\n",
      "     ACTION  Mean Top-5 Recall: 0.30210339741072634\n",
      "  VERB   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.14705882352941177  Top5: 0.7352941176470589\n",
      "    @ 0.50s  Top1: 0.13513513513513514  Top5: 0.7027027027027027\n",
      "    @ 0.75s  Top1: 0.13636363636363635  Top5: 0.7727272727272727\n",
      "    @ 1.00s  Top1: 0.16666666666666666  Top5: 0.7291666666666666\n",
      "    @ 1.25s  Top1: 0.2708333333333333  Top5: 0.7916666666666666\n",
      "    @ 1.50s  Top1: 0.23636363636363636  Top5: 0.8545454545454545\n",
      "    @ 1.75s  Top1: 0.22807017543859648  Top5: 0.8596491228070176\n",
      "    @ 2.00s  Top1: 0.2413793103448276  Top5: 0.8275862068965517\n",
      "    overall_top1: 0.2020997375328084, overall_top5: 0.7926509186351706\n",
      "  NOUN   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.2647058823529412  Top5: 0.5294117647058824\n",
      "    @ 0.50s  Top1: 0.24324324324324326  Top5: 0.4594594594594595\n",
      "    @ 0.75s  Top1: 0.20454545454545456  Top5: 0.4318181818181818\n",
      "    @ 1.00s  Top1: 0.1875  Top5: 0.4166666666666667\n",
      "    @ 1.25s  Top1: 0.16666666666666666  Top5: 0.3958333333333333\n",
      "    @ 1.50s  Top1: 0.18181818181818182  Top5: 0.4\n",
      "    @ 1.75s  Top1: 0.17543859649122806  Top5: 0.45614035087719296\n",
      "    @ 2.00s  Top1: 0.1896551724137931  Top5: 0.46551724137931033\n",
      "    overall_top1: 0.1968503937007874, overall_top5: 0.4409448818897638\n",
      "  ACTION per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.17647058823529413  Top5: 0.35294117647058826\n",
      "    @ 0.50s  Top1: 0.16216216216216217  Top5: 0.32432432432432434\n",
      "    @ 0.75s  Top1: 0.11363636363636363  Top5: 0.2727272727272727\n",
      "    @ 1.00s  Top1: 0.10416666666666667  Top5: 0.2916666666666667\n",
      "    @ 1.25s  Top1: 0.0625  Top5: 0.2916666666666667\n",
      "    @ 1.50s  Top1: 0.09090909090909091  Top5: 0.3090909090909091\n",
      "    @ 1.75s  Top1: 0.10526315789473684  Top5: 0.3157894736842105\n",
      "    @ 2.00s  Top1: 0.10344827586206896  Top5: 0.25862068965517243\n",
      "    overall_top1: 0.11023622047244094, overall_top5: 0.2992125984251969\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10 Train:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10 Val:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 | Time 5.6s\n",
      "  Train Loss: 2.7615 | Val Loss: 7.9064\n",
      "  VERB   Train Top1: 0.6051364365971108, Top5: 0.9823434991974318; Val Top1: 0.28608923884514437, Top5: 0.7480314960629921\n",
      "  NOUN   Train Top1: 0.6597110754414125, Top5: 0.9598715890850722; Val Top1: 0.26246719160104987, Top5: 0.5118110236220472\n",
      "  ACTION Train Top1: 0.6789727126805778, Top5: 0.9759229534510433; Val Top1: 0.15485564304461943, Top5: 0.29396325459317585\n",
      "  VERB   Val Precision: 0.0908, Recall: 0.1868, F1: 0.1152\n",
      "  NOUN   Val Precision: 0.1804, Recall: 0.1561, F1: 0.1398\n",
      "  ACTION Val Precision: 0.0883, Recall: 0.1161, F1: 0.0921\n",
      "  ---- Mean Top-5 Recall (validation) ----\n",
      "     VERB    Mean Top-5 Recall: 0.7367416892444917\n",
      "     NOUN    Mean Top-5 Recall: 0.5194117276554289\n",
      "     ACTION  Mean Top-5 Recall: 0.2973876258108803\n",
      "  VERB   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.23529411764705882  Top5: 0.6764705882352942\n",
      "    @ 0.50s  Top1: 0.21621621621621623  Top5: 0.6486486486486487\n",
      "    @ 0.75s  Top1: 0.2727272727272727  Top5: 0.6818181818181818\n",
      "    @ 1.00s  Top1: 0.2708333333333333  Top5: 0.7291666666666666\n",
      "    @ 1.25s  Top1: 0.2708333333333333  Top5: 0.6875\n",
      "    @ 1.50s  Top1: 0.3090909090909091  Top5: 0.8181818181818182\n",
      "    @ 1.75s  Top1: 0.3333333333333333  Top5: 0.8245614035087719\n",
      "    @ 2.00s  Top1: 0.3275862068965517  Top5: 0.8275862068965517\n",
      "    overall_top1: 0.28608923884514437, overall_top5: 0.7480314960629921\n",
      "  NOUN   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.38235294117647056  Top5: 0.6176470588235294\n",
      "    @ 0.50s  Top1: 0.2972972972972973  Top5: 0.5675675675675675\n",
      "    @ 0.75s  Top1: 0.2727272727272727  Top5: 0.5227272727272727\n",
      "    @ 1.00s  Top1: 0.25  Top5: 0.5208333333333334\n",
      "    @ 1.25s  Top1: 0.25  Top5: 0.4791666666666667\n",
      "    @ 1.50s  Top1: 0.2545454545454545  Top5: 0.4909090909090909\n",
      "    @ 1.75s  Top1: 0.22807017543859648  Top5: 0.47368421052631576\n",
      "    @ 2.00s  Top1: 0.22413793103448276  Top5: 0.4827586206896552\n",
      "    overall_top1: 0.26246719160104987, overall_top5: 0.5118110236220472\n",
      "  ACTION per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.20588235294117646  Top5: 0.35294117647058826\n",
      "    @ 0.50s  Top1: 0.1891891891891892  Top5: 0.32432432432432434\n",
      "    @ 0.75s  Top1: 0.13636363636363635  Top5: 0.29545454545454547\n",
      "    @ 1.00s  Top1: 0.125  Top5: 0.2708333333333333\n",
      "    @ 1.25s  Top1: 0.125  Top5: 0.2708333333333333\n",
      "    @ 1.50s  Top1: 0.16363636363636364  Top5: 0.2909090909090909\n",
      "    @ 1.75s  Top1: 0.15789473684210525  Top5: 0.2807017543859649\n",
      "    @ 2.00s  Top1: 0.15517241379310345  Top5: 0.29310344827586204\n",
      "    overall_top1: 0.15485564304461943, overall_top5: 0.29396325459317585\n",
      "[SAVED BEST] -> Model\\P01_05_fused_model.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11 Train:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11 Val:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 | Time 5.8s\n",
      "  Train Loss: 2.3846 | Val Loss: 8.2674\n",
      "  VERB   Train Top1: 0.6163723916532905, Top5: 0.985553772070626; Val Top1: 0.16010498687664043, Top5: 0.7139107611548556\n",
      "  NOUN   Train Top1: 0.7351524879614767, Top5: 0.9727126805778491; Val Top1: 0.2388451443569554, Top5: 0.4776902887139108\n",
      "  ACTION Train Top1: 0.812199036918138, Top5: 0.9743178170144462; Val Top1: 0.14698162729658792, Top5: 0.31758530183727035\n",
      "  VERB   Val Precision: 0.1255, Recall: 0.1842, F1: 0.0976\n",
      "  NOUN   Val Precision: 0.1444, Recall: 0.1577, F1: 0.1233\n",
      "  ACTION Val Precision: 0.0778, Recall: 0.1121, F1: 0.0854\n",
      "  ---- Mean Top-5 Recall (validation) ----\n",
      "     VERB    Mean Top-5 Recall: 0.7074110296478384\n",
      "     NOUN    Mean Top-5 Recall: 0.47975678106906755\n",
      "     ACTION  Mean Top-5 Recall: 0.3180307058856755\n",
      "  VERB   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.17647058823529413  Top5: 0.6764705882352942\n",
      "    @ 0.50s  Top1: 0.16216216216216217  Top5: 0.6756756756756757\n",
      "    @ 0.75s  Top1: 0.1590909090909091  Top5: 0.6590909090909091\n",
      "    @ 1.00s  Top1: 0.16666666666666666  Top5: 0.6875\n",
      "    @ 1.25s  Top1: 0.125  Top5: 0.6666666666666666\n",
      "    @ 1.50s  Top1: 0.14545454545454545  Top5: 0.7636363636363637\n",
      "    @ 1.75s  Top1: 0.15789473684210525  Top5: 0.7543859649122807\n",
      "    @ 2.00s  Top1: 0.1896551724137931  Top5: 0.7758620689655172\n",
      "    overall_top1: 0.16010498687664043, overall_top5: 0.7139107611548556\n",
      "  NOUN   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.29411764705882354  Top5: 0.5294117647058824\n",
      "    @ 0.50s  Top1: 0.2972972972972973  Top5: 0.4864864864864865\n",
      "    @ 0.75s  Top1: 0.22727272727272727  Top5: 0.45454545454545453\n",
      "    @ 1.00s  Top1: 0.22916666666666666  Top5: 0.4791666666666667\n",
      "    @ 1.25s  Top1: 0.1875  Top5: 0.4583333333333333\n",
      "    @ 1.50s  Top1: 0.23636363636363636  Top5: 0.4909090909090909\n",
      "    @ 1.75s  Top1: 0.22807017543859648  Top5: 0.47368421052631576\n",
      "    @ 2.00s  Top1: 0.2413793103448276  Top5: 0.46551724137931033\n",
      "    overall_top1: 0.2388451443569554, overall_top5: 0.4776902887139108\n",
      "  ACTION per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.20588235294117646  Top5: 0.35294117647058826\n",
      "    @ 0.50s  Top1: 0.1891891891891892  Top5: 0.32432432432432434\n",
      "    @ 0.75s  Top1: 0.13636363636363635  Top5: 0.29545454545454547\n",
      "    @ 1.00s  Top1: 0.125  Top5: 0.2916666666666667\n",
      "    @ 1.25s  Top1: 0.125  Top5: 0.2916666666666667\n",
      "    @ 1.50s  Top1: 0.14545454545454545  Top5: 0.32727272727272727\n",
      "    @ 1.75s  Top1: 0.14035087719298245  Top5: 0.3333333333333333\n",
      "    @ 2.00s  Top1: 0.13793103448275862  Top5: 0.3275862068965517\n",
      "    overall_top1: 0.14698162729658792, overall_top5: 0.31758530183727035\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12 Train:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12 Val:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 | Time 5.6s\n",
      "  Train Loss: 2.0940 | Val Loss: 7.9282\n",
      "  VERB   Train Top1: 0.7399678972712681, Top5: 0.985553772070626; Val Top1: 0.32020997375328086, Top5: 0.7007874015748031\n",
      "  NOUN   Train Top1: 0.8218298555377207, Top5: 0.9919743178170144; Val Top1: 0.26246719160104987, Top5: 0.5118110236220472\n",
      "  ACTION Train Top1: 0.8475120385232745, Top5: 0.985553772070626; Val Top1: 0.14698162729658792, Top5: 0.28346456692913385\n",
      "  VERB   Val Precision: 0.1441, Recall: 0.2039, F1: 0.1542\n",
      "  NOUN   Val Precision: 0.1849, Recall: 0.1594, F1: 0.1474\n",
      "  ACTION Val Precision: 0.0825, Recall: 0.1121, F1: 0.0873\n",
      "  ---- Mean Top-5 Recall (validation) ----\n",
      "     VERB    Mean Top-5 Recall: 0.6881080258101984\n",
      "     NOUN    Mean Top-5 Recall: 0.5118273555760211\n",
      "     ACTION  Mean Top-5 Recall: 0.2860775598671937\n",
      "  VERB   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.29411764705882354  Top5: 0.5882352941176471\n",
      "    @ 0.50s  Top1: 0.24324324324324326  Top5: 0.5945945945945946\n",
      "    @ 0.75s  Top1: 0.3181818181818182  Top5: 0.6590909090909091\n",
      "    @ 1.00s  Top1: 0.3125  Top5: 0.6875\n",
      "    @ 1.25s  Top1: 0.3333333333333333  Top5: 0.6458333333333334\n",
      "    @ 1.50s  Top1: 0.34545454545454546  Top5: 0.7818181818181819\n",
      "    @ 1.75s  Top1: 0.3333333333333333  Top5: 0.7719298245614035\n",
      "    @ 2.00s  Top1: 0.3448275862068966  Top5: 0.7758620689655172\n",
      "    overall_top1: 0.32020997375328086, overall_top5: 0.7007874015748031\n",
      "  NOUN   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.38235294117647056  Top5: 0.5588235294117647\n",
      "    @ 0.50s  Top1: 0.32432432432432434  Top5: 0.5135135135135135\n",
      "    @ 0.75s  Top1: 0.2727272727272727  Top5: 0.5\n",
      "    @ 1.00s  Top1: 0.25  Top5: 0.4583333333333333\n",
      "    @ 1.25s  Top1: 0.22916666666666666  Top5: 0.4583333333333333\n",
      "    @ 1.50s  Top1: 0.23636363636363636  Top5: 0.5272727272727272\n",
      "    @ 1.75s  Top1: 0.22807017543859648  Top5: 0.543859649122807\n",
      "    @ 2.00s  Top1: 0.2413793103448276  Top5: 0.5344827586206896\n",
      "    overall_top1: 0.26246719160104987, overall_top5: 0.5118110236220472\n",
      "  ACTION per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.20588235294117646  Top5: 0.35294117647058826\n",
      "    @ 0.50s  Top1: 0.1891891891891892  Top5: 0.2972972972972973\n",
      "    @ 0.75s  Top1: 0.13636363636363635  Top5: 0.2727272727272727\n",
      "    @ 1.00s  Top1: 0.125  Top5: 0.25\n",
      "    @ 1.25s  Top1: 0.125  Top5: 0.25\n",
      "    @ 1.50s  Top1: 0.14545454545454545  Top5: 0.3090909090909091\n",
      "    @ 1.75s  Top1: 0.14035087719298245  Top5: 0.2807017543859649\n",
      "    @ 2.00s  Top1: 0.13793103448275862  Top5: 0.27586206896551724\n",
      "    overall_top1: 0.14698162729658792, overall_top5: 0.28346456692913385\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13 Train:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13 Val:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 | Time 5.4s\n",
      "  Train Loss: 1.7619 | Val Loss: 8.0634\n",
      "  VERB   Train Top1: 0.7768860353130016, Top5: 0.9919743178170144; Val Top1: 0.2073490813648294, Top5: 0.7165354330708661\n",
      "  NOUN   Train Top1: 0.8202247191011236, Top5: 0.985553772070626; Val Top1: 0.28083989501312334, Top5: 0.4881889763779528\n",
      "  ACTION Train Top1: 0.8764044943820225, Top5: 0.9903691813804173; Val Top1: 0.15223097112860892, Top5: 0.31758530183727035\n",
      "  VERB   Val Precision: 0.1809, Recall: 0.2080, F1: 0.1600\n",
      "  NOUN   Val Precision: 0.2149, Recall: 0.2021, F1: 0.1858\n",
      "  ACTION Val Precision: 0.1138, Recall: 0.1198, F1: 0.0973\n",
      "  ---- Mean Top-5 Recall (validation) ----\n",
      "     VERB    Mean Top-5 Recall: 0.7078372412872853\n",
      "     NOUN    Mean Top-5 Recall: 0.48668631999224105\n",
      "     ACTION  Mean Top-5 Recall: 0.3187715704133047\n",
      "  VERB   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.17647058823529413  Top5: 0.6470588235294118\n",
      "    @ 0.50s  Top1: 0.21621621621621623  Top5: 0.6486486486486487\n",
      "    @ 0.75s  Top1: 0.25  Top5: 0.6590909090909091\n",
      "    @ 1.00s  Top1: 0.1875  Top5: 0.6875\n",
      "    @ 1.25s  Top1: 0.1875  Top5: 0.7083333333333334\n",
      "    @ 1.50s  Top1: 0.21818181818181817  Top5: 0.7818181818181819\n",
      "    @ 1.75s  Top1: 0.21052631578947367  Top5: 0.7543859649122807\n",
      "    @ 2.00s  Top1: 0.20689655172413793  Top5: 0.7758620689655172\n",
      "    overall_top1: 0.2073490813648294, overall_top5: 0.7165354330708661\n",
      "  NOUN   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.38235294117647056  Top5: 0.5\n",
      "    @ 0.50s  Top1: 0.35135135135135137  Top5: 0.4594594594594595\n",
      "    @ 0.75s  Top1: 0.2727272727272727  Top5: 0.45454545454545453\n",
      "    @ 1.00s  Top1: 0.2708333333333333  Top5: 0.5\n",
      "    @ 1.25s  Top1: 0.25  Top5: 0.4791666666666667\n",
      "    @ 1.50s  Top1: 0.2545454545454545  Top5: 0.509090909090909\n",
      "    @ 1.75s  Top1: 0.2631578947368421  Top5: 0.49122807017543857\n",
      "    @ 2.00s  Top1: 0.25862068965517243  Top5: 0.5\n",
      "    overall_top1: 0.28083989501312334, overall_top5: 0.4881889763779528\n",
      "  ACTION per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.17647058823529413  Top5: 0.38235294117647056\n",
      "    @ 0.50s  Top1: 0.16216216216216217  Top5: 0.32432432432432434\n",
      "    @ 0.75s  Top1: 0.1590909090909091  Top5: 0.29545454545454547\n",
      "    @ 1.00s  Top1: 0.14583333333333334  Top5: 0.2708333333333333\n",
      "    @ 1.25s  Top1: 0.125  Top5: 0.2708333333333333\n",
      "    @ 1.50s  Top1: 0.16363636363636364  Top5: 0.34545454545454546\n",
      "    @ 1.75s  Top1: 0.14035087719298245  Top5: 0.3333333333333333\n",
      "    @ 2.00s  Top1: 0.15517241379310345  Top5: 0.3275862068965517\n",
      "    overall_top1: 0.15223097112860892, overall_top5: 0.31758530183727035\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14 Train:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14 Val:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 | Time 5.7s\n",
      "  Train Loss: 1.2395 | Val Loss: 8.3215\n",
      "  VERB   Train Top1: 0.8587479935794543, Top5: 0.9919743178170144; Val Top1: 0.2440944881889764, Top5: 0.6299212598425197\n",
      "  NOUN   Train Top1: 0.9197431781701445, Top5: 0.9887640449438202; Val Top1: 0.24671916010498687, Top5: 0.4881889763779528\n",
      "  ACTION Train Top1: 0.9373996789727127, Top5: 0.9903691813804173; Val Top1: 0.16010498687664043, Top5: 0.31758530183727035\n",
      "  VERB   Val Precision: 0.1871, Recall: 0.2105, F1: 0.1599\n",
      "  NOUN   Val Precision: 0.1922, Recall: 0.1573, F1: 0.1598\n",
      "  ACTION Val Precision: 0.0988, Recall: 0.1181, F1: 0.1010\n",
      "  ---- Mean Top-5 Recall (validation) ----\n",
      "     VERB    Mean Top-5 Recall: 0.6227805973737501\n",
      "     NOUN    Mean Top-5 Recall: 0.4928231285346023\n",
      "     ACTION  Mean Top-5 Recall: 0.3199018384850295\n",
      "  VERB   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.20588235294117646  Top5: 0.5588235294117647\n",
      "    @ 0.50s  Top1: 0.1891891891891892  Top5: 0.5675675675675675\n",
      "    @ 0.75s  Top1: 0.20454545454545456  Top5: 0.6136363636363636\n",
      "    @ 1.00s  Top1: 0.25  Top5: 0.6041666666666666\n",
      "    @ 1.25s  Top1: 0.22916666666666666  Top5: 0.625\n",
      "    @ 1.50s  Top1: 0.2727272727272727  Top5: 0.6909090909090909\n",
      "    @ 1.75s  Top1: 0.2807017543859649  Top5: 0.6842105263157895\n",
      "    @ 2.00s  Top1: 0.27586206896551724  Top5: 0.6379310344827587\n",
      "    overall_top1: 0.2440944881889764, overall_top5: 0.6299212598425197\n",
      "  NOUN   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.29411764705882354  Top5: 0.5882352941176471\n",
      "    @ 0.50s  Top1: 0.2702702702702703  Top5: 0.5135135135135135\n",
      "    @ 0.75s  Top1: 0.2727272727272727  Top5: 0.5\n",
      "    @ 1.00s  Top1: 0.25  Top5: 0.4375\n",
      "    @ 1.25s  Top1: 0.22916666666666666  Top5: 0.4375\n",
      "    @ 1.50s  Top1: 0.23636363636363636  Top5: 0.509090909090909\n",
      "    @ 1.75s  Top1: 0.22807017543859648  Top5: 0.49122807017543857\n",
      "    @ 2.00s  Top1: 0.22413793103448276  Top5: 0.46551724137931033\n",
      "    overall_top1: 0.24671916010498687, overall_top5: 0.4881889763779528\n",
      "  ACTION per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.17647058823529413  Top5: 0.35294117647058826\n",
      "    @ 0.50s  Top1: 0.16216216216216217  Top5: 0.35135135135135137\n",
      "    @ 0.75s  Top1: 0.1590909090909091  Top5: 0.3181818181818182\n",
      "    @ 1.00s  Top1: 0.14583333333333334  Top5: 0.2916666666666667\n",
      "    @ 1.25s  Top1: 0.14583333333333334  Top5: 0.2916666666666667\n",
      "    @ 1.50s  Top1: 0.16363636363636364  Top5: 0.32727272727272727\n",
      "    @ 1.75s  Top1: 0.17543859649122806  Top5: 0.3157894736842105\n",
      "    @ 2.00s  Top1: 0.15517241379310345  Top5: 0.3103448275862069\n",
      "    overall_top1: 0.16010498687664043, overall_top5: 0.31758530183727035\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15 Train:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15 Val:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 | Time 5.6s\n",
      "  Train Loss: 1.0222 | Val Loss: 8.2169\n",
      "  VERB   Train Top1: 0.9004815409309791, Top5: 0.9951845906902087; Val Top1: 0.2204724409448819, Top5: 0.6824146981627297\n",
      "  NOUN   Train Top1: 0.9486356340288925, Top5: 0.9935794542536116; Val Top1: 0.3123359580052493, Top5: 0.5091863517060368\n",
      "  ACTION Train Top1: 0.9582664526484751, Top5: 0.9951845906902087; Val Top1: 0.2099737532808399, Top5: 0.30446194225721784\n",
      "  VERB   Val Precision: 0.2191, Recall: 0.1957, F1: 0.1655\n",
      "  NOUN   Val Precision: 0.2561, Recall: 0.2031, F1: 0.2172\n",
      "  ACTION Val Precision: 0.1286, Recall: 0.1388, F1: 0.1232\n",
      "  ---- Mean Top-5 Recall (validation) ----\n",
      "     VERB    Mean Top-5 Recall: 0.6745909519284465\n",
      "     NOUN    Mean Top-5 Recall: 0.5140930122506067\n",
      "     ACTION  Mean Top-5 Recall: 0.30768497854177634\n",
      "  VERB   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.17647058823529413  Top5: 0.5882352941176471\n",
      "    @ 0.50s  Top1: 0.1891891891891892  Top5: 0.6486486486486487\n",
      "    @ 0.75s  Top1: 0.20454545454545456  Top5: 0.6590909090909091\n",
      "    @ 1.00s  Top1: 0.20833333333333334  Top5: 0.6458333333333334\n",
      "    @ 1.25s  Top1: 0.1875  Top5: 0.6666666666666666\n",
      "    @ 1.50s  Top1: 0.23636363636363636  Top5: 0.7272727272727273\n",
      "    @ 1.75s  Top1: 0.2631578947368421  Top5: 0.7368421052631579\n",
      "    @ 2.00s  Top1: 0.25862068965517243  Top5: 0.7241379310344828\n",
      "    overall_top1: 0.2204724409448819, overall_top5: 0.6824146981627297\n",
      "  NOUN   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.35294117647058826  Top5: 0.5882352941176471\n",
      "    @ 0.50s  Top1: 0.35135135135135137  Top5: 0.5675675675675675\n",
      "    @ 0.75s  Top1: 0.36363636363636365  Top5: 0.5\n",
      "    @ 1.00s  Top1: 0.3125  Top5: 0.4791666666666667\n",
      "    @ 1.25s  Top1: 0.2916666666666667  Top5: 0.4583333333333333\n",
      "    @ 1.50s  Top1: 0.2909090909090909  Top5: 0.5454545454545454\n",
      "    @ 1.75s  Top1: 0.2807017543859649  Top5: 0.49122807017543857\n",
      "    @ 2.00s  Top1: 0.29310344827586204  Top5: 0.4827586206896552\n",
      "    overall_top1: 0.3123359580052493, overall_top5: 0.5091863517060368\n",
      "  ACTION per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.23529411764705882  Top5: 0.38235294117647056\n",
      "    @ 0.50s  Top1: 0.21621621621621623  Top5: 0.32432432432432434\n",
      "    @ 0.75s  Top1: 0.22727272727272727  Top5: 0.29545454545454547\n",
      "    @ 1.00s  Top1: 0.20833333333333334  Top5: 0.2708333333333333\n",
      "    @ 1.25s  Top1: 0.20833333333333334  Top5: 0.2708333333333333\n",
      "    @ 1.50s  Top1: 0.21818181818181817  Top5: 0.3090909090909091\n",
      "    @ 1.75s  Top1: 0.19298245614035087  Top5: 0.2982456140350877\n",
      "    @ 2.00s  Top1: 0.1896551724137931  Top5: 0.3103448275862069\n",
      "    overall_top1: 0.2099737532808399, overall_top5: 0.30446194225721784\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16 Train:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16 Val:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 | Time 5.4s\n",
      "  Train Loss: 0.7975 | Val Loss: 8.1119\n",
      "  VERB   Train Top1: 0.9390048154093098, Top5: 0.9951845906902087; Val Top1: 0.2440944881889764, Top5: 0.7086614173228346\n",
      "  NOUN   Train Top1: 0.9695024077046549, Top5: 0.9983948635634029; Val Top1: 0.31496062992125984, Top5: 0.5144356955380578\n",
      "  ACTION Train Top1: 0.9727126805778491, Top5: 0.9983948635634029; Val Top1: 0.22572178477690288, Top5: 0.28346456692913385\n",
      "  VERB   Val Precision: 0.1789, Recall: 0.1767, F1: 0.1642\n",
      "  NOUN   Val Precision: 0.2264, Recall: 0.1964, F1: 0.2038\n",
      "  ACTION Val Precision: 0.1311, Recall: 0.1463, F1: 0.1329\n",
      "  ---- Mean Top-5 Recall (validation) ----\n",
      "     VERB    Mean Top-5 Recall: 0.69967598408518\n",
      "     NOUN    Mean Top-5 Recall: 0.5161834672767335\n",
      "     ACTION  Mean Top-5 Recall: 0.2871832109728448\n",
      "  VERB   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.23529411764705882  Top5: 0.6176470588235294\n",
      "    @ 0.50s  Top1: 0.21621621621621623  Top5: 0.6486486486486487\n",
      "    @ 0.75s  Top1: 0.22727272727272727  Top5: 0.6818181818181818\n",
      "    @ 1.00s  Top1: 0.20833333333333334  Top5: 0.6666666666666666\n",
      "    @ 1.25s  Top1: 0.22916666666666666  Top5: 0.6875\n",
      "    @ 1.50s  Top1: 0.2545454545454545  Top5: 0.7818181818181819\n",
      "    @ 1.75s  Top1: 0.24561403508771928  Top5: 0.7719298245614035\n",
      "    @ 2.00s  Top1: 0.3103448275862069  Top5: 0.7413793103448276\n",
      "    overall_top1: 0.2440944881889764, overall_top5: 0.7086614173228346\n",
      "  NOUN   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.35294117647058826  Top5: 0.5588235294117647\n",
      "    @ 0.50s  Top1: 0.32432432432432434  Top5: 0.5405405405405406\n",
      "    @ 0.75s  Top1: 0.3409090909090909  Top5: 0.5\n",
      "    @ 1.00s  Top1: 0.3125  Top5: 0.4791666666666667\n",
      "    @ 1.25s  Top1: 0.2916666666666667  Top5: 0.4791666666666667\n",
      "    @ 1.50s  Top1: 0.32727272727272727  Top5: 0.5454545454545454\n",
      "    @ 1.75s  Top1: 0.2982456140350877  Top5: 0.5263157894736842\n",
      "    @ 2.00s  Top1: 0.29310344827586204  Top5: 0.5\n",
      "    overall_top1: 0.31496062992125984, overall_top5: 0.5144356955380578\n",
      "  ACTION per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.23529411764705882  Top5: 0.35294117647058826\n",
      "    @ 0.50s  Top1: 0.21621621621621623  Top5: 0.32432432432432434\n",
      "    @ 0.75s  Top1: 0.22727272727272727  Top5: 0.2727272727272727\n",
      "    @ 1.00s  Top1: 0.20833333333333334  Top5: 0.25\n",
      "    @ 1.25s  Top1: 0.20833333333333334  Top5: 0.25\n",
      "    @ 1.50s  Top1: 0.23636363636363636  Top5: 0.2909090909090909\n",
      "    @ 1.75s  Top1: 0.22807017543859648  Top5: 0.2807017543859649\n",
      "    @ 2.00s  Top1: 0.2413793103448276  Top5: 0.27586206896551724\n",
      "    overall_top1: 0.22572178477690288, overall_top5: 0.28346456692913385\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17 Train:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17 Val:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 | Time 5.6s\n",
      "  Train Loss: 0.6419 | Val Loss: 8.1922\n",
      "  VERB   Train Top1: 0.9390048154093098, Top5: 0.9951845906902087; Val Top1: 0.27296587926509186, Top5: 0.7716535433070866\n",
      "  NOUN   Train Top1: 0.9727126805778491, Top5: 0.9967897271268058; Val Top1: 0.31758530183727035, Top5: 0.48556430446194226\n",
      "  ACTION Train Top1: 0.9791332263242376, Top5: 0.9983948635634029; Val Top1: 0.1968503937007874, Top5: 0.2887139107611549\n",
      "  VERB   Val Precision: 0.2133, Recall: 0.2061, F1: 0.1726\n",
      "  NOUN   Val Precision: 0.2251, Recall: 0.1977, F1: 0.1972\n",
      "  ACTION Val Precision: 0.1205, Recall: 0.1299, F1: 0.1180\n",
      "  ---- Mean Top-5 Recall (validation) ----\n",
      "     VERB    Mean Top-5 Recall: 0.7648509046857304\n",
      "     NOUN    Mean Top-5 Recall: 0.4869150061287352\n",
      "     ACTION  Mean Top-5 Recall: 0.29190920286922206\n",
      "  VERB   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.2647058823529412  Top5: 0.7058823529411765\n",
      "    @ 0.50s  Top1: 0.24324324324324326  Top5: 0.7297297297297297\n",
      "    @ 0.75s  Top1: 0.25  Top5: 0.75\n",
      "    @ 1.00s  Top1: 0.25  Top5: 0.7291666666666666\n",
      "    @ 1.25s  Top1: 0.25  Top5: 0.75\n",
      "    @ 1.50s  Top1: 0.2909090909090909  Top5: 0.8363636363636363\n",
      "    @ 1.75s  Top1: 0.2982456140350877  Top5: 0.8245614035087719\n",
      "    @ 2.00s  Top1: 0.3103448275862069  Top5: 0.7931034482758621\n",
      "    overall_top1: 0.27296587926509186, overall_top5: 0.7716535433070866\n",
      "  NOUN   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.38235294117647056  Top5: 0.5588235294117647\n",
      "    @ 0.50s  Top1: 0.35135135135135137  Top5: 0.5135135135135135\n",
      "    @ 0.75s  Top1: 0.3409090909090909  Top5: 0.45454545454545453\n",
      "    @ 1.00s  Top1: 0.3125  Top5: 0.4166666666666667\n",
      "    @ 1.25s  Top1: 0.2916666666666667  Top5: 0.4166666666666667\n",
      "    @ 1.50s  Top1: 0.3090909090909091  Top5: 0.509090909090909\n",
      "    @ 1.75s  Top1: 0.2982456140350877  Top5: 0.5087719298245614\n",
      "    @ 2.00s  Top1: 0.29310344827586204  Top5: 0.5172413793103449\n",
      "    overall_top1: 0.31758530183727035, overall_top5: 0.48556430446194226\n",
      "  ACTION per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.17647058823529413  Top5: 0.38235294117647056\n",
      "    @ 0.50s  Top1: 0.16216216216216217  Top5: 0.2972972972972973\n",
      "    @ 0.75s  Top1: 0.18181818181818182  Top5: 0.2727272727272727\n",
      "    @ 1.00s  Top1: 0.14583333333333334  Top5: 0.25\n",
      "    @ 1.25s  Top1: 0.1875  Top5: 0.25\n",
      "    @ 1.50s  Top1: 0.23636363636363636  Top5: 0.3090909090909091\n",
      "    @ 1.75s  Top1: 0.22807017543859648  Top5: 0.2807017543859649\n",
      "    @ 2.00s  Top1: 0.22413793103448276  Top5: 0.29310344827586204\n",
      "    overall_top1: 0.1968503937007874, overall_top5: 0.2887139107611549\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18 Train:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18 Val:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 | Time 5.6s\n",
      "  Train Loss: 0.5380 | Val Loss: 8.1342\n",
      "  VERB   Train Top1: 0.9743178170144462, Top5: 0.9983948635634029; Val Top1: 0.24671916010498687, Top5: 0.7585301837270341\n",
      "  NOUN   Train Top1: 0.9791332263242376, Top5: 0.9983948635634029; Val Top1: 0.31758530183727035, Top5: 0.5118110236220472\n",
      "  ACTION Train Top1: 0.9807383627608347, Top5: 1.0; Val Top1: 0.2152230971128609, Top5: 0.31496062992125984\n",
      "  VERB   Val Precision: 0.1847, Recall: 0.1623, F1: 0.1497\n",
      "  NOUN   Val Precision: 0.2611, Recall: 0.2063, F1: 0.2136\n",
      "  ACTION Val Precision: 0.1329, Recall: 0.1429, F1: 0.1310\n",
      "  ---- Mean Top-5 Recall (validation) ----\n",
      "     VERB    Mean Top-5 Recall: 0.7549645345190877\n",
      "     NOUN    Mean Top-5 Recall: 0.5160130749465315\n",
      "     ACTION  Mean Top-5 Recall: 0.31988681105610883\n",
      "  VERB   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.23529411764705882  Top5: 0.7352941176470589\n",
      "    @ 0.50s  Top1: 0.24324324324324326  Top5: 0.7567567567567568\n",
      "    @ 0.75s  Top1: 0.20454545454545456  Top5: 0.75\n",
      "    @ 1.00s  Top1: 0.22916666666666666  Top5: 0.7083333333333334\n",
      "    @ 1.25s  Top1: 0.20833333333333334  Top5: 0.7083333333333334\n",
      "    @ 1.50s  Top1: 0.2545454545454545  Top5: 0.7636363636363637\n",
      "    @ 1.75s  Top1: 0.2982456140350877  Top5: 0.8070175438596491\n",
      "    @ 2.00s  Top1: 0.27586206896551724  Top5: 0.8103448275862069\n",
      "    overall_top1: 0.24671916010498687, overall_top5: 0.7585301837270341\n",
      "  NOUN   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.38235294117647056  Top5: 0.5882352941176471\n",
      "    @ 0.50s  Top1: 0.35135135135135137  Top5: 0.5675675675675675\n",
      "    @ 0.75s  Top1: 0.3181818181818182  Top5: 0.5\n",
      "    @ 1.00s  Top1: 0.3125  Top5: 0.4791666666666667\n",
      "    @ 1.25s  Top1: 0.2916666666666667  Top5: 0.4583333333333333\n",
      "    @ 1.50s  Top1: 0.3090909090909091  Top5: 0.509090909090909\n",
      "    @ 1.75s  Top1: 0.3157894736842105  Top5: 0.49122807017543857\n",
      "    @ 2.00s  Top1: 0.29310344827586204  Top5: 0.5344827586206896\n",
      "    overall_top1: 0.31758530183727035, overall_top5: 0.5118110236220472\n",
      "  ACTION per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.20588235294117646  Top5: 0.38235294117647056\n",
      "    @ 0.50s  Top1: 0.16216216216216217  Top5: 0.3783783783783784\n",
      "    @ 0.75s  Top1: 0.22727272727272727  Top5: 0.3181818181818182\n",
      "    @ 1.00s  Top1: 0.1875  Top5: 0.2916666666666667\n",
      "    @ 1.25s  Top1: 0.20833333333333334  Top5: 0.2708333333333333\n",
      "    @ 1.50s  Top1: 0.23636363636363636  Top5: 0.3090909090909091\n",
      "    @ 1.75s  Top1: 0.22807017543859648  Top5: 0.2982456140350877\n",
      "    @ 2.00s  Top1: 0.2413793103448276  Top5: 0.3103448275862069\n",
      "    overall_top1: 0.2152230971128609, overall_top5: 0.31496062992125984\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19 Train:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19 Val:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 | Time 5.6s\n",
      "  Train Loss: 0.4797 | Val Loss: 8.2350\n",
      "  VERB   Train Top1: 0.9678972712680578, Top5: 1.0; Val Top1: 0.28083989501312334, Top5: 0.6902887139107612\n",
      "  NOUN   Train Top1: 0.9807383627608347, Top5: 1.0; Val Top1: 0.30708661417322836, Top5: 0.5459317585301837\n",
      "  ACTION Train Top1: 0.9839486356340289, Top5: 1.0; Val Top1: 0.2125984251968504, Top5: 0.32020997375328086\n",
      "  VERB   Val Precision: 0.2212, Recall: 0.2273, F1: 0.1873\n",
      "  NOUN   Val Precision: 0.2670, Recall: 0.2011, F1: 0.2163\n",
      "  ACTION Val Precision: 0.1269, Recall: 0.1407, F1: 0.1275\n",
      "  ---- Mean Top-5 Recall (validation) ----\n",
      "     VERB    Mean Top-5 Recall: 0.6762647579387865\n",
      "     NOUN    Mean Top-5 Recall: 0.548948417145612\n",
      "     ACTION  Mean Top-5 Recall: 0.3235783090732648\n",
      "  VERB   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.23529411764705882  Top5: 0.5588235294117647\n",
      "    @ 0.50s  Top1: 0.21621621621621623  Top5: 0.5945945945945946\n",
      "    @ 0.75s  Top1: 0.25  Top5: 0.6363636363636364\n",
      "    @ 1.00s  Top1: 0.25  Top5: 0.6458333333333334\n",
      "    @ 1.25s  Top1: 0.2708333333333333  Top5: 0.6458333333333334\n",
      "    @ 1.50s  Top1: 0.3090909090909091  Top5: 0.7636363636363637\n",
      "    @ 1.75s  Top1: 0.3333333333333333  Top5: 0.7719298245614035\n",
      "    @ 2.00s  Top1: 0.3275862068965517  Top5: 0.7931034482758621\n",
      "    overall_top1: 0.28083989501312334, overall_top5: 0.6902887139107612\n",
      "  NOUN   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.3235294117647059  Top5: 0.6176470588235294\n",
      "    @ 0.50s  Top1: 0.32432432432432434  Top5: 0.5675675675675675\n",
      "    @ 0.75s  Top1: 0.3181818181818182  Top5: 0.5227272727272727\n",
      "    @ 1.00s  Top1: 0.3125  Top5: 0.5416666666666666\n",
      "    @ 1.25s  Top1: 0.2916666666666667  Top5: 0.5\n",
      "    @ 1.50s  Top1: 0.2909090909090909  Top5: 0.5636363636363636\n",
      "    @ 1.75s  Top1: 0.3157894736842105  Top5: 0.543859649122807\n",
      "    @ 2.00s  Top1: 0.29310344827586204  Top5: 0.5344827586206896\n",
      "    overall_top1: 0.30708661417322836, overall_top5: 0.5459317585301837\n",
      "  ACTION per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.20588235294117646  Top5: 0.38235294117647056\n",
      "    @ 0.50s  Top1: 0.1891891891891892  Top5: 0.35135135135135137\n",
      "    @ 0.75s  Top1: 0.20454545454545456  Top5: 0.3181818181818182\n",
      "    @ 1.00s  Top1: 0.1875  Top5: 0.2916666666666667\n",
      "    @ 1.25s  Top1: 0.1875  Top5: 0.2916666666666667\n",
      "    @ 1.50s  Top1: 0.23636363636363636  Top5: 0.32727272727272727\n",
      "    @ 1.75s  Top1: 0.22807017543859648  Top5: 0.3157894736842105\n",
      "    @ 2.00s  Top1: 0.2413793103448276  Top5: 0.3103448275862069\n",
      "    overall_top1: 0.2125984251968504, overall_top5: 0.32020997375328086\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20 Train:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20 Val:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 | Time 5.3s\n",
      "  Train Loss: 0.4060 | Val Loss: 8.2631\n",
      "  VERB   Train Top1: 0.9823434991974318, Top5: 1.0; Val Top1: 0.23622047244094488, Top5: 0.6745406824146981\n",
      "  NOUN   Train Top1: 0.9823434991974318, Top5: 1.0; Val Top1: 0.2992125984251969, Top5: 0.5065616797900262\n",
      "  ACTION Train Top1: 0.9839486356340289, Top5: 1.0; Val Top1: 0.2204724409448819, Top5: 0.33858267716535434\n",
      "  VERB   Val Precision: 0.2607, Recall: 0.1736, F1: 0.1703\n",
      "  NOUN   Val Precision: 0.2456, Recall: 0.1876, F1: 0.2052\n",
      "  ACTION Val Precision: 0.1473, Recall: 0.1475, F1: 0.1360\n",
      "  ---- Mean Top-5 Recall (validation) ----\n",
      "     VERB    Mean Top-5 Recall: 0.6594500255287461\n",
      "     NOUN    Mean Top-5 Recall: 0.5140606670135202\n",
      "     ACTION  Mean Top-5 Recall: 0.3415092571596121\n",
      "  VERB   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.20588235294117646  Top5: 0.5\n",
      "    @ 0.50s  Top1: 0.16216216216216217  Top5: 0.5945945945945946\n",
      "    @ 0.75s  Top1: 0.18181818181818182  Top5: 0.6136363636363636\n",
      "    @ 1.00s  Top1: 0.20833333333333334  Top5: 0.625\n",
      "    @ 1.25s  Top1: 0.22916666666666666  Top5: 0.6666666666666666\n",
      "    @ 1.50s  Top1: 0.2727272727272727  Top5: 0.7454545454545455\n",
      "    @ 1.75s  Top1: 0.2807017543859649  Top5: 0.7543859649122807\n",
      "    @ 2.00s  Top1: 0.29310344827586204  Top5: 0.7758620689655172\n",
      "    overall_top1: 0.23622047244094488, overall_top5: 0.6745406824146981\n",
      "  NOUN   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.35294117647058826  Top5: 0.5882352941176471\n",
      "    @ 0.50s  Top1: 0.32432432432432434  Top5: 0.5945945945945946\n",
      "    @ 0.75s  Top1: 0.29545454545454547  Top5: 0.5227272727272727\n",
      "    @ 1.00s  Top1: 0.2916666666666667  Top5: 0.4791666666666667\n",
      "    @ 1.25s  Top1: 0.2708333333333333  Top5: 0.4791666666666667\n",
      "    @ 1.50s  Top1: 0.3090909090909091  Top5: 0.509090909090909\n",
      "    @ 1.75s  Top1: 0.2807017543859649  Top5: 0.49122807017543857\n",
      "    @ 2.00s  Top1: 0.29310344827586204  Top5: 0.4482758620689655\n",
      "    overall_top1: 0.2992125984251969, overall_top5: 0.5065616797900262\n",
      "  ACTION per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.23529411764705882  Top5: 0.38235294117647056\n",
      "    @ 0.50s  Top1: 0.1891891891891892  Top5: 0.3783783783783784\n",
      "    @ 0.75s  Top1: 0.22727272727272727  Top5: 0.3409090909090909\n",
      "    @ 1.00s  Top1: 0.20833333333333334  Top5: 0.3125\n",
      "    @ 1.25s  Top1: 0.20833333333333334  Top5: 0.3125\n",
      "    @ 1.50s  Top1: 0.23636363636363636  Top5: 0.32727272727272727\n",
      "    @ 1.75s  Top1: 0.22807017543859648  Top5: 0.3333333333333333\n",
      "    @ 2.00s  Top1: 0.22413793103448276  Top5: 0.3448275862068966\n",
      "    overall_top1: 0.2204724409448819, overall_top5: 0.33858267716535434\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    t0 = time.time()\n",
    "\n",
    "    # ------------- TRAIN -------------\n",
    "    model.train()\n",
    "    train_loss_sum = 0.0\n",
    "    train_samples = 0\n",
    "    train_counts = {\n",
    "        \"verb_top1\": [0, 0], \"verb_top5\": [0, 0],\n",
    "        \"noun_top1\": [0, 0], \"noun_top5\": [0, 0],\n",
    "        \"action_top1\": [0, 0], \"action_top5\": [0, 0]\n",
    "    }\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch} Train\", leave=False)\n",
    "    for F_batch, y_multi, meta in pbar:\n",
    "        F_batch = F_batch.to(DEVICE)               # (B, T, D)\n",
    "        y_v = y_multi[\"verb\"].to(DEVICE)           # (B, K_fut)\n",
    "        y_n = y_multi[\"noun\"].to(DEVICE)\n",
    "        y_a = y_multi[\"action\"].to(DEVICE)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        logits = model(F_batch)   # dict: \"verb\"/\"noun\"/\"action\" -> (B, K_fut, C)\n",
    "\n",
    "        loss_v = masked_cross_entropy(logits[\"verb\"], y_v)\n",
    "        loss_n = masked_cross_entropy(logits[\"noun\"], y_n)\n",
    "        loss_a = masked_cross_entropy(logits[\"action\"], y_a)\n",
    "        loss = loss_a + 0.5 * loss_v + 0.5 * loss_n\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        b = F_batch.size(0)\n",
    "        train_loss_sum += float(loss.item()) * b\n",
    "        train_samples += b\n",
    "\n",
    "        for (task, lab, lg) in [\n",
    "            (\"verb\", y_v, logits[\"verb\"]),\n",
    "            (\"noun\", y_n, logits[\"noun\"]),\n",
    "            (\"action\", y_a, logits[\"action\"])\n",
    "        ]:\n",
    "            h1, t1 = topk_counts(lg.detach().cpu(), lab.detach().cpu(), k=1)\n",
    "            h5, t5 = topk_counts(lg.detach().cpu(), lab.detach().cpu(), k=5)\n",
    "            train_counts[f\"{task}_top1\"][0] += h1\n",
    "            train_counts[f\"{task}_top1\"][1] += t1\n",
    "            train_counts[f\"{task}_top5\"][0] += h5\n",
    "            train_counts[f\"{task}_top5\"][1] += t5\n",
    "\n",
    "    train_loss = train_loss_sum / max(1, train_samples)\n",
    "    train_metrics = {}\n",
    "    for task in [\"verb\", \"noun\", \"action\"]:\n",
    "        h1, t1 = train_counts[f\"{task}_top1\"]\n",
    "        h5, t5 = train_counts[f\"{task}_top5\"]\n",
    "        train_metrics[f\"{task}_top1\"] = (h1 / t1) if t1 > 0 else None\n",
    "        train_metrics[f\"{task}_top5\"] = (h5 / t5) if t5 > 0 else None\n",
    "\n",
    "    # ------------- VALIDATION -------------\n",
    "    model.eval()\n",
    "    val_loss_sum = 0.0\n",
    "    val_samples = 0\n",
    "    val_counts = {\n",
    "        \"verb_top1\": [0, 0], \"verb_top5\": [0, 0],\n",
    "        \"noun_top1\": [0, 0], \"noun_top5\": [0, 0],\n",
    "        \"action_top1\": [0, 0], \"action_top5\": [0, 0]\n",
    "    }\n",
    "\n",
    "    # store logits/labels for per-horizon + P/R/F1 metrics\n",
    "    val_logits_store = {\"verb\": [], \"noun\": [], \"action\": []}\n",
    "    val_labels_store = {\"verb\": [], \"noun\": [], \"action\": []}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_loader, desc=f\"Epoch {epoch} Val\", leave=False)\n",
    "        for F_batch, y_multi, meta in pbar:\n",
    "            F_batch = F_batch.to(DEVICE)\n",
    "            y_v = y_multi[\"verb\"].to(DEVICE)\n",
    "            y_n = y_multi[\"noun\"].to(DEVICE)\n",
    "            y_a = y_multi[\"action\"].to(DEVICE)\n",
    "\n",
    "            logits = model(F_batch)\n",
    "            loss_v = masked_cross_entropy(logits[\"verb\"], y_v)\n",
    "            loss_n = masked_cross_entropy(logits[\"noun\"], y_n)\n",
    "            loss_a = masked_cross_entropy(logits[\"action\"], y_a)\n",
    "            loss = loss_a + 0.5 * loss_v + 0.5 * loss_n\n",
    "\n",
    "            b = F_batch.size(0)\n",
    "            val_loss_sum += float(loss.item()) * b\n",
    "            val_samples += b\n",
    "\n",
    "            for (task, lab, lg) in [\n",
    "                (\"verb\", y_v, logits[\"verb\"]),\n",
    "                (\"noun\", y_n, logits[\"noun\"]),\n",
    "                (\"action\", y_a, logits[\"action\"])\n",
    "            ]:\n",
    "                h1, t1 = topk_counts(lg.detach().cpu(), lab.detach().cpu(), k=1)\n",
    "                h5, t5 = topk_counts(lg.detach().cpu(), lab.detach().cpu(), k=5)\n",
    "                val_counts[f\"{task}_top1\"][0] += h1\n",
    "                val_counts[f\"{task}_top1\"][1] += t1\n",
    "                val_counts[f\"{task}_top5\"][0] += h5\n",
    "                val_counts[f\"{task}_top5\"][1] += t5\n",
    "\n",
    "            # store for per-horizon + P/R/F1 metrics\n",
    "            val_logits_store[\"verb\"].append(logits[\"verb\"].detach().cpu())\n",
    "            val_logits_store[\"noun\"].append(logits[\"noun\"].detach().cpu())\n",
    "            val_logits_store[\"action\"].append(logits[\"action\"].detach().cpu())\n",
    "            val_labels_store[\"verb\"].append(y_v.detach().cpu())\n",
    "            val_labels_store[\"noun\"].append(y_n.detach().cpu())\n",
    "            val_labels_store[\"action\"].append(y_a.detach().cpu())\n",
    "\n",
    "    val_loss = val_loss_sum / max(1, val_samples)\n",
    "\n",
    "    # overall val metrics (top-1/top-5 over all horizons)\n",
    "    val_metrics = {}\n",
    "    for task in [\"verb\", \"noun\", \"action\"]:\n",
    "        h1, t1 = val_counts[f\"{task}_top1\"]\n",
    "        h5, t5 = val_counts[f\"{task}_top5\"]\n",
    "        val_metrics[f\"{task}_top1\"] = (h1 / t1) if t1 > 0 else None\n",
    "        val_metrics[f\"{task}_top5\"] = (h5 / t5) if t5 > 0 else None\n",
    "\n",
    "    # per-horizon metrics (time-based)\n",
    "    per_horizon_metrics = {\"verb\": {}, \"noun\": {}, \"action\": {}}\n",
    "    for task in [\"verb\", \"noun\", \"action\"]:\n",
    "        if len(val_logits_store[task]) == 0:\n",
    "            continue\n",
    "        logits_all = torch.cat(val_logits_store[task], dim=0)  # (N, K_fut, C)\n",
    "        labels_all = torch.cat(val_labels_store[task], dim=0)  # (N, K_fut)\n",
    "        m = topk_accuracy_per_task(\n",
    "            logits_all,\n",
    "            labels_all,\n",
    "            topk=(1, 5),\n",
    "            ignore_index=IGNORE_INDEX\n",
    "        )\n",
    "        per_horizon_metrics[task] = m\n",
    "\n",
    "    # macro precision / recall / F1 over all horizons (validation)\n",
    "    prf_metrics = {\"verb\": {}, \"noun\": {}, \"action\": {}}\n",
    "    for task in [\"verb\", \"noun\", \"action\"]:\n",
    "        if len(val_logits_store[task]) == 0:\n",
    "            continue\n",
    "\n",
    "        logits_all = torch.cat(val_logits_store[task], dim=0)\n",
    "        labels_all = torch.cat(val_labels_store[task], dim=0)\n",
    "\n",
    "        preds_all = logits_all.argmax(dim=-1)  # (N, K_fut)\n",
    "        mask = (labels_all != IGNORE_INDEX)\n",
    "        if mask.sum().item() == 0:\n",
    "            continue\n",
    "\n",
    "        y_true = labels_all[mask].numpy()\n",
    "        y_pred = preds_all[mask].numpy()\n",
    "\n",
    "        p, r, f1, _ = precision_recall_fscore_support(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            average=\"macro\",\n",
    "            zero_division=0\n",
    "        )\n",
    "        prf_metrics[task][\"precision\"] = p\n",
    "        prf_metrics[task][\"recall\"] = r\n",
    "        prf_metrics[task][\"f1\"] = f1\n",
    "\n",
    "    # mean Top-5 recall across horizons for each task\n",
    "    mean_top5_recall = {}\n",
    "    for task in [\"verb\", \"noun\", \"action\"]:\n",
    "        mh = per_horizon_metrics[task]\n",
    "        if not mh:\n",
    "            mean_top5_recall[task] = None\n",
    "            continue\n",
    "\n",
    "        vals = []\n",
    "        for h_idx in range(K_FUT):\n",
    "            key = f\"per_h{h_idx+1}_top5\"\n",
    "            if key in mh and mh[key] is not None:\n",
    "                vals.append(mh[key])\n",
    "        mean_top5_recall[task] = float(np.mean(vals)) if len(vals) > 0 else None\n",
    "\n",
    "    # scheduler + logging\n",
    "    sched.step(val_loss)\n",
    "    elapsed = time.time() - t0\n",
    "\n",
    "    print(f\"Epoch {epoch}/{NUM_EPOCHS} | Time {elapsed:.1f}s\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    for task in [\"verb\", \"noun\", \"action\"]:\n",
    "        print(\n",
    "            f\"  {task.upper():6s} Train Top1: {train_metrics[f'{task}_top1']}, \"\n",
    "            f\"Top5: {train_metrics[f'{task}_top5']}; \"\n",
    "            f\"Val Top1: {val_metrics[f'{task}_top1']}, \"\n",
    "            f\"Top5: {val_metrics[f'{task}_top5']}\"\n",
    "        )\n",
    "\n",
    "    # print macro precision / recall / F1 (validation)\n",
    "    for task in [\"verb\", \"noun\", \"action\"]:\n",
    "        if prf_metrics[task]:\n",
    "            p = prf_metrics[task][\"precision\"]\n",
    "            r = prf_metrics[task][\"recall\"]\n",
    "            f1 = prf_metrics[task][\"f1\"]\n",
    "            print(\n",
    "                f\"  {task.upper():6s} Val Precision: {p:.4f}, \"\n",
    "                f\"Recall: {r:.4f}, F1: {f1:.4f}\"\n",
    "            )\n",
    "\n",
    "    # print mean Top-5 recall\n",
    "    print(\"  ---- Mean Top-5 Recall (validation) ----\")\n",
    "    for task in [\"verb\", \"noun\", \"action\"]:\n",
    "        print(\n",
    "            f\"     {task.upper():6s}  Mean Top-5 Recall: {mean_top5_recall[task]}\"\n",
    "        )\n",
    "\n",
    "    # print per-horizon by seconds\n",
    "    for task in [\"verb\", \"noun\", \"action\"]:\n",
    "        mh = per_horizon_metrics[task]\n",
    "        if not mh:\n",
    "            continue\n",
    "        print(f\"  {task.upper():6s} per-horizon (time-based):\")\n",
    "        for h_idx, t_sec in enumerate(HORIZONS_S):\n",
    "            key1 = f\"per_h{h_idx+1}_top1\"\n",
    "            key5 = f\"per_h{h_idx+1}_top5\"\n",
    "            v1 = mh.get(key1, None)\n",
    "            v5 = mh.get(key5, None)\n",
    "            print(f\"    @ {t_sec:4.2f}s  Top1: {v1}  Top5: {v5}\")\n",
    "        print(\n",
    "            f\"    overall_top1: {mh.get('overall_top1', None)}, \"\n",
    "            f\"overall_top5: {mh.get('overall_top5', None)}\"\n",
    "        )\n",
    "\n",
    "    # optional: save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(\n",
    "            {\n",
    "                'epoch': epoch,\n",
    "                'model_state': model.state_dict(),\n",
    "                'opt_state': opt.state_dict(),\n",
    "                'val_loss': val_loss\n",
    "            },\n",
    "            BEST_MODEL_PATH\n",
    "        )\n",
    "        print(f\"[SAVED BEST] -> {BEST_MODEL_PATH}\")\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
