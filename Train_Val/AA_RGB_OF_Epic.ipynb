{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bef4f971",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc283ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models import resnet50\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data as PyGData, Batch as PyGBatch\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch.utils.data import Dataset\n",
    "import math, random, time\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e03062",
   "metadata": {},
   "source": [
    "## USER CONFIG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8564d66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RGB_FOLDER = Path(r\"D:\\Datasets\\Datasets\\EPIC_Kitchen\\RGB\\P01_01\\Original\")   # folder with RGB frames\n",
    "FLOW_FOLDER = Path(r\"D:\\Datasets\\Datasets\\EPIC_Kitchen\\OpticalFlow\\P01_01\\P01_01\") # folder with flow frames (same filenames); set None to skip flow\n",
    "LABEL_CSV   = Path(r\"D:\\Datasets\\Datasets\\EPIC\\Labels\\P01_01.csv\")  # label file for this video\n",
    "OUTPUT_FUSED_CSV = Path(r\"D:\\Datasets\\Datasets\\EPIC\\Features\\P01_01_fused_features.csv\")  # where fused CSV will be saved\n",
    "\n",
    "SAMPLE_RATE = 1     # take every S-th frame (1 => every frame)\n",
    "FEAT_DIM = 512      # output projection dim\n",
    "W_RGB = 0.6         # fusion weight for RGB\n",
    "W_FLOW = 0.4        # fusion weight for Flow (ignored if FLOW_FOLDER is None)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "OUTPUT_FUSED_CSV.parent.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4239b67b",
   "metadata": {},
   "source": [
    "\n",
    "### helper: parse integer frame index from filename e.g. frame_000123.jpg -> 123\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ede7a45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_frame_number_re = re.compile(r\"(\\d+)(?=\\.[^.]+$)\")\n",
    "def parse_frame_index(fname: str):\n",
    "    m = _frame_number_re.search(fname)\n",
    "    if m:\n",
    "        return int(m.group(1))\n",
    "    digs = re.findall(r\"\\d+\", fname)\n",
    "    return int(digs[-1]) if digs else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30e73b7",
   "metadata": {},
   "source": [
    "## Feature extractor: ResNet50 backbone (pretrained) + linear proj to FEAT_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2a679dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PAWANESH\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "_resnet = resnet50(weights=True)\n",
    "_resnet = nn.Sequential(*list(_resnet.children())[:-1]).to(DEVICE).eval()   # outputs (B,2048,1,1)\n",
    "_proj = nn.Linear(2048, FEAT_DIM).to(DEVICE).eval()\n",
    "_transform = T.Compose([T.Resize((224,224)), T.ToTensor(),\n",
    "                        T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_feature_from_pil(pil_img: Image.Image):\n",
    "    x = _transform(pil_img).unsqueeze(0).to(DEVICE)   # (1,3,224,224)\n",
    "    feat = _resnet(x).view(1, -1)                     # (1,2048)\n",
    "    feat = _proj(feat)                                # (1,FEAT_DIM)\n",
    "    return feat.squeeze(0).cpu().numpy()              # (FEAT_DIM,)\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Extract RGB & Flow features, fuse, save fused CSV\n",
    "# -------------------------\n",
    "def extract_and_save_fused(csv_labels_path: Path,\n",
    "                           rgb_folder: Path,\n",
    "                           flow_folder: Path or None,\n",
    "                           out_fused_csv: Path,\n",
    "                           sample_rate: int = 1,\n",
    "                           w_rgb: float = 0.6,\n",
    "                           w_flow: float = 0.4):\n",
    "    \"\"\"\n",
    "    Extract & fuse features for one video folder. Saves fused CSV with columns:\n",
    "    frame_idx, frame_name, ActionLabel, ActionName, feat_0..feat_{FEAT_DIM-1}\n",
    "    \"\"\"\n",
    "    \n",
    "    # load labels\n",
    "    labels_df = pd.read_csv(csv_labels_path)\n",
    "    # list rgb frames (canonical filenames)\n",
    "    rgb_files = sorted([p for p in rgb_folder.iterdir() if p.suffix.lower() in [\".jpg\",\".png\",\".jpeg\"]])\n",
    "    sampled = rgb_files[::sample_rate]\n",
    "    if len(sampled) == 0:\n",
    "        raise RuntimeError(f\"No frames found in {rgb_folder}\")\n",
    "\n",
    "    # containers\n",
    "    fused_rows = []\n",
    "    feat_cols = [f\"feat_{i}\" for i in range(FEAT_DIM)]\n",
    "\n",
    "    # iterate frames\n",
    "    for fp in tqdm(sampled, desc=\"Extract & fuse\"):\n",
    "        fname = fp.name\n",
    "        frame_idx = parse_frame_index(fname)\n",
    "\n",
    "        # --- RGB feature ---\n",
    "        try:\n",
    "            pil = Image.open(fp).convert(\"RGB\")\n",
    "            rgb_feat = extract_feature_from_pil(pil)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] RGB skip {fname}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # --- Flow feature ---\n",
    "        if flow_folder is not None:\n",
    "            ffp = Path(flow_folder) / fname\n",
    "            if not ffp.exists():\n",
    "                # fallback: use RGB image for alignment (keeps pipeline working)\n",
    "                ffp = fp\n",
    "            try:\n",
    "                pilf = Image.open(ffp).convert(\"RGB\")\n",
    "                flow_feat = extract_feature_from_pil(pilf)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] FLOW skip {fname}: {e}; using zeros\")\n",
    "                flow_feat = np.zeros(FEAT_DIM, dtype=np.float32)\n",
    "        else:\n",
    "            flow_feat = np.zeros(FEAT_DIM, dtype=np.float32)\n",
    "            \n",
    "# -------------------------- FUSION Here ------------------------------------------------\n",
    "\n",
    "        # --- fused vector (weighted sum) ---\n",
    "        fused_vec = w_rgb * rgb_feat.astype(np.float32) + w_flow * flow_feat.astype(np.float32)\n",
    "\n",
    "        # find label row that contains this frame (StartFrame <= idx <= EndFrame)\n",
    "        lr = labels_df[(labels_df[\"StartFrame\"] <= frame_idx) & (labels_df[\"EndFrame\"] >= frame_idx)]\n",
    "        if not lr.empty:\n",
    "            action_label = int(lr.iloc[0].get(\"ActionLabel\", -1))\n",
    "            action_name  = str(lr.iloc[0].get(\"ActionName\", \"Unknown\"))\n",
    "        else:\n",
    "            action_label, action_name = -1, \"Unknown\"\n",
    "\n",
    "        # build row\n",
    "        row = {\"frame_idx\": int(frame_idx), \"frame_name\": fname, \"ActionLabel\": int(action_label), \"ActionName\": action_name}\n",
    "        for i_val, v in enumerate(fused_vec):\n",
    "            row[f\"feat_{i_val}\"] = float(v)\n",
    "        fused_rows.append(row)\n",
    "\n",
    "    # save\n",
    "    if len(fused_rows) == 0:\n",
    "        raise RuntimeError(\"No fused rows extracted; check paths and files.\")\n",
    "    df_fused = pd.DataFrame(fused_rows)\n",
    "    df_fused.to_csv(out_fused_csv, index=False)\n",
    "    print(f\"[SAVED] fused CSV -> {out_fused_csv}\")\n",
    "    return df_fused"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a833a233",
   "metadata": {},
   "source": [
    "## Run extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d309d741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39be5eb5a9774ff3b930aff8dde37290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extract & fuse:   0%|          | 0/99029 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVED] fused CSV -> D:\\Datasets\\Datasets\\EPIC\\Features\\P01_01_fused_features.csv\n"
     ]
    }
   ],
   "source": [
    "df_fused = extract_and_save_fused(\n",
    "    csv_labels_path = LABEL_CSV,\n",
    "    rgb_folder = RGB_FOLDER,\n",
    "    flow_folder = FLOW_FOLDER if (FLOW_FOLDER is not None and FLOW_FOLDER.exists()) else None,\n",
    "    out_fused_csv = OUTPUT_FUSED_CSV,\n",
    "    sample_rate = SAMPLE_RATE,\n",
    "    w_rgb = W_RGB,\n",
    "    w_flow = W_FLOW\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "47000a09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame_idx</th>\n",
       "      <th>frame_name</th>\n",
       "      <th>ActionLabel</th>\n",
       "      <th>ActionName</th>\n",
       "      <th>feat_0</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_502</th>\n",
       "      <th>feat_503</th>\n",
       "      <th>feat_504</th>\n",
       "      <th>feat_505</th>\n",
       "      <th>feat_506</th>\n",
       "      <th>feat_507</th>\n",
       "      <th>feat_508</th>\n",
       "      <th>feat_509</th>\n",
       "      <th>feat_510</th>\n",
       "      <th>feat_511</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>frame_00000.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0.224212</td>\n",
       "      <td>-0.416339</td>\n",
       "      <td>0.093212</td>\n",
       "      <td>-0.107410</td>\n",
       "      <td>0.128738</td>\n",
       "      <td>-0.086018</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.093053</td>\n",
       "      <td>-0.069731</td>\n",
       "      <td>-0.055004</td>\n",
       "      <td>-0.190981</td>\n",
       "      <td>-0.225788</td>\n",
       "      <td>-0.117915</td>\n",
       "      <td>0.111749</td>\n",
       "      <td>-0.181230</td>\n",
       "      <td>-0.133861</td>\n",
       "      <td>-0.064304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>frame_00001.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0.213923</td>\n",
       "      <td>-0.386899</td>\n",
       "      <td>0.059060</td>\n",
       "      <td>-0.069629</td>\n",
       "      <td>0.119920</td>\n",
       "      <td>-0.166948</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.080017</td>\n",
       "      <td>-0.080227</td>\n",
       "      <td>-0.068668</td>\n",
       "      <td>-0.140048</td>\n",
       "      <td>-0.231339</td>\n",
       "      <td>-0.136919</td>\n",
       "      <td>0.105033</td>\n",
       "      <td>-0.226436</td>\n",
       "      <td>-0.147560</td>\n",
       "      <td>-0.031502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>frame_00002.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0.243420</td>\n",
       "      <td>-0.428746</td>\n",
       "      <td>0.088513</td>\n",
       "      <td>-0.109414</td>\n",
       "      <td>0.146825</td>\n",
       "      <td>-0.124339</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.125939</td>\n",
       "      <td>-0.101465</td>\n",
       "      <td>-0.083090</td>\n",
       "      <td>-0.207229</td>\n",
       "      <td>-0.214796</td>\n",
       "      <td>-0.130478</td>\n",
       "      <td>0.117336</td>\n",
       "      <td>-0.226607</td>\n",
       "      <td>-0.148587</td>\n",
       "      <td>-0.036388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>frame_00003.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0.199176</td>\n",
       "      <td>-0.394861</td>\n",
       "      <td>0.059091</td>\n",
       "      <td>-0.070849</td>\n",
       "      <td>0.138222</td>\n",
       "      <td>-0.188922</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.076485</td>\n",
       "      <td>-0.085018</td>\n",
       "      <td>-0.078830</td>\n",
       "      <td>-0.138564</td>\n",
       "      <td>-0.230030</td>\n",
       "      <td>-0.130138</td>\n",
       "      <td>0.084694</td>\n",
       "      <td>-0.234740</td>\n",
       "      <td>-0.113729</td>\n",
       "      <td>-0.036507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>frame_00004.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0.201879</td>\n",
       "      <td>-0.418372</td>\n",
       "      <td>0.067712</td>\n",
       "      <td>-0.092059</td>\n",
       "      <td>0.153830</td>\n",
       "      <td>-0.097980</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.124027</td>\n",
       "      <td>-0.072635</td>\n",
       "      <td>-0.070298</td>\n",
       "      <td>-0.177150</td>\n",
       "      <td>-0.205052</td>\n",
       "      <td>-0.097257</td>\n",
       "      <td>0.119052</td>\n",
       "      <td>-0.231058</td>\n",
       "      <td>-0.133520</td>\n",
       "      <td>-0.042069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7119</th>\n",
       "      <td>7119</td>\n",
       "      <td>frame_07119.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0.319755</td>\n",
       "      <td>-0.180561</td>\n",
       "      <td>-0.020468</td>\n",
       "      <td>-0.662251</td>\n",
       "      <td>0.286641</td>\n",
       "      <td>-0.220271</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.117545</td>\n",
       "      <td>0.197170</td>\n",
       "      <td>-0.497525</td>\n",
       "      <td>0.254925</td>\n",
       "      <td>-0.314211</td>\n",
       "      <td>-0.339845</td>\n",
       "      <td>0.104476</td>\n",
       "      <td>-0.593676</td>\n",
       "      <td>0.167948</td>\n",
       "      <td>0.032733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7120</th>\n",
       "      <td>7120</td>\n",
       "      <td>frame_07120.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0.377411</td>\n",
       "      <td>-0.274535</td>\n",
       "      <td>-0.041729</td>\n",
       "      <td>-0.628119</td>\n",
       "      <td>0.238089</td>\n",
       "      <td>-0.184473</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.133505</td>\n",
       "      <td>0.089116</td>\n",
       "      <td>-0.308133</td>\n",
       "      <td>0.235201</td>\n",
       "      <td>-0.286874</td>\n",
       "      <td>-0.237807</td>\n",
       "      <td>0.027914</td>\n",
       "      <td>-0.566877</td>\n",
       "      <td>0.290931</td>\n",
       "      <td>0.117833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7121</th>\n",
       "      <td>7121</td>\n",
       "      <td>frame_07121.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0.350540</td>\n",
       "      <td>-0.272856</td>\n",
       "      <td>-0.002134</td>\n",
       "      <td>-0.596341</td>\n",
       "      <td>0.220748</td>\n",
       "      <td>-0.251758</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.146171</td>\n",
       "      <td>0.129557</td>\n",
       "      <td>-0.440564</td>\n",
       "      <td>0.188763</td>\n",
       "      <td>-0.353000</td>\n",
       "      <td>-0.293067</td>\n",
       "      <td>0.075543</td>\n",
       "      <td>-0.538834</td>\n",
       "      <td>0.203080</td>\n",
       "      <td>0.097905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7122</th>\n",
       "      <td>7122</td>\n",
       "      <td>frame_07122.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0.312043</td>\n",
       "      <td>-0.282243</td>\n",
       "      <td>-0.060604</td>\n",
       "      <td>-0.584190</td>\n",
       "      <td>0.237332</td>\n",
       "      <td>-0.254098</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.134249</td>\n",
       "      <td>0.060154</td>\n",
       "      <td>-0.250969</td>\n",
       "      <td>0.223738</td>\n",
       "      <td>-0.298771</td>\n",
       "      <td>-0.242504</td>\n",
       "      <td>0.065116</td>\n",
       "      <td>-0.583450</td>\n",
       "      <td>0.259569</td>\n",
       "      <td>0.088858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7123</th>\n",
       "      <td>7123</td>\n",
       "      <td>frame_07123.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0.356346</td>\n",
       "      <td>-0.234059</td>\n",
       "      <td>-0.021687</td>\n",
       "      <td>-0.614767</td>\n",
       "      <td>0.180654</td>\n",
       "      <td>-0.314178</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.159844</td>\n",
       "      <td>0.095848</td>\n",
       "      <td>-0.398239</td>\n",
       "      <td>0.169730</td>\n",
       "      <td>-0.285300</td>\n",
       "      <td>-0.329218</td>\n",
       "      <td>0.041594</td>\n",
       "      <td>-0.609086</td>\n",
       "      <td>0.154240</td>\n",
       "      <td>0.118611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7124 rows Ã— 516 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      frame_idx       frame_name  ActionLabel ActionName    feat_0    feat_1  \\\n",
       "0             0  frame_00000.jpg           -1    Unknown  0.224212 -0.416339   \n",
       "1             1  frame_00001.jpg           -1    Unknown  0.213923 -0.386899   \n",
       "2             2  frame_00002.jpg           -1    Unknown  0.243420 -0.428746   \n",
       "3             3  frame_00003.jpg           -1    Unknown  0.199176 -0.394861   \n",
       "4             4  frame_00004.jpg           -1    Unknown  0.201879 -0.418372   \n",
       "...         ...              ...          ...        ...       ...       ...   \n",
       "7119       7119  frame_07119.jpg           -1    Unknown  0.319755 -0.180561   \n",
       "7120       7120  frame_07120.jpg           -1    Unknown  0.377411 -0.274535   \n",
       "7121       7121  frame_07121.jpg           -1    Unknown  0.350540 -0.272856   \n",
       "7122       7122  frame_07122.jpg           -1    Unknown  0.312043 -0.282243   \n",
       "7123       7123  frame_07123.jpg           -1    Unknown  0.356346 -0.234059   \n",
       "\n",
       "        feat_2    feat_3    feat_4    feat_5  ...  feat_502  feat_503  \\\n",
       "0     0.093212 -0.107410  0.128738 -0.086018  ... -0.093053 -0.069731   \n",
       "1     0.059060 -0.069629  0.119920 -0.166948  ... -0.080017 -0.080227   \n",
       "2     0.088513 -0.109414  0.146825 -0.124339  ... -0.125939 -0.101465   \n",
       "3     0.059091 -0.070849  0.138222 -0.188922  ... -0.076485 -0.085018   \n",
       "4     0.067712 -0.092059  0.153830 -0.097980  ... -0.124027 -0.072635   \n",
       "...        ...       ...       ...       ...  ...       ...       ...   \n",
       "7119 -0.020468 -0.662251  0.286641 -0.220271  ... -0.117545  0.197170   \n",
       "7120 -0.041729 -0.628119  0.238089 -0.184473  ... -0.133505  0.089116   \n",
       "7121 -0.002134 -0.596341  0.220748 -0.251758  ... -0.146171  0.129557   \n",
       "7122 -0.060604 -0.584190  0.237332 -0.254098  ... -0.134249  0.060154   \n",
       "7123 -0.021687 -0.614767  0.180654 -0.314178  ... -0.159844  0.095848   \n",
       "\n",
       "      feat_504  feat_505  feat_506  feat_507  feat_508  feat_509  feat_510  \\\n",
       "0    -0.055004 -0.190981 -0.225788 -0.117915  0.111749 -0.181230 -0.133861   \n",
       "1    -0.068668 -0.140048 -0.231339 -0.136919  0.105033 -0.226436 -0.147560   \n",
       "2    -0.083090 -0.207229 -0.214796 -0.130478  0.117336 -0.226607 -0.148587   \n",
       "3    -0.078830 -0.138564 -0.230030 -0.130138  0.084694 -0.234740 -0.113729   \n",
       "4    -0.070298 -0.177150 -0.205052 -0.097257  0.119052 -0.231058 -0.133520   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "7119 -0.497525  0.254925 -0.314211 -0.339845  0.104476 -0.593676  0.167948   \n",
       "7120 -0.308133  0.235201 -0.286874 -0.237807  0.027914 -0.566877  0.290931   \n",
       "7121 -0.440564  0.188763 -0.353000 -0.293067  0.075543 -0.538834  0.203080   \n",
       "7122 -0.250969  0.223738 -0.298771 -0.242504  0.065116 -0.583450  0.259569   \n",
       "7123 -0.398239  0.169730 -0.285300 -0.329218  0.041594 -0.609086  0.154240   \n",
       "\n",
       "      feat_511  \n",
       "0    -0.064304  \n",
       "1    -0.031502  \n",
       "2    -0.036388  \n",
       "3    -0.036507  \n",
       "4    -0.042069  \n",
       "...        ...  \n",
       "7119  0.032733  \n",
       "7120  0.117833  \n",
       "7121  0.097905  \n",
       "7122  0.088858  \n",
       "7123  0.118611  \n",
       "\n",
       "[7124 rows x 516 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(r\"D:\\Datasets\\Datasets\\EPIC\\Features\\FusedFeatures\\P01_03_fused_features.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d5fd0f",
   "metadata": {},
   "source": [
    "### NOW From Here, We have to import the fused features and Label file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bea39c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def load_fused_csv_by_path(fused_csv_path: str):\n",
    "    fp = Path(fused_csv_path)\n",
    "    if not fp.exists():\n",
    "        raise FileNotFoundError(f\"Fused features CSV not found: {fp}\")\n",
    "    df = pd.read_csv(fp)\n",
    "    if \"frame_idx\" not in df.columns:\n",
    "        raise KeyError(\"Fused CSV must contain 'frame_idx' column\")\n",
    "    df[\"frame_idx\"] = df[\"frame_idx\"].astype(int)\n",
    "    df = df.sort_values(\"frame_idx\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def load_label_csv_by_path(label_csv_path: str):\n",
    "    fp = Path(label_csv_path)\n",
    "    if not fp.exists():\n",
    "        raise FileNotFoundError(f\"Label CSV not found: {fp}\")\n",
    "    df = pd.read_csv(fp)\n",
    "    return df\n",
    "\n",
    "# ------------------------------------ Paths ---------------------------\n",
    "fused_df = load_fused_csv_by_path(r\"D:\\Datasets\\Datasets\\EPIC\\Features\\FusedFeatures\\P01_02_fused_features.csv\")\n",
    "labels_df = load_label_csv_by_path(r\"D:\\Datasets\\Datasets\\EPIC\\Labels\\P01_02.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6b5175",
   "metadata": {},
   "source": [
    "## Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "abd57e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_INDEX = -1\n",
    "\n",
    "class SingleVideoAnticipationDataset(Dataset):\n",
    "    def __init__(self, fused_df_or_path, labels_df_or_path,\n",
    "                 t_obs: int, k_fut: int, feat_dim: int,\n",
    "                 fps: float, horizons_s: list[float]):\n",
    "        \n",
    "        # load paths\n",
    "        if isinstance(fused_df_or_path, (str, Path)):\n",
    "            fused_df = pd.read_csv(fused_df_or_path)\n",
    "        else:\n",
    "            fused_df = fused_df_or_path.copy()\n",
    "        if isinstance(labels_df_or_path, (str, Path)):\n",
    "            labels_df = pd.read_csv(labels_df_or_path)\n",
    "        else:\n",
    "            labels_df = labels_df_or_path.copy()\n",
    "\n",
    "        if \"frame_idx\" not in fused_df.columns:\n",
    "            raise KeyError(\"fused_df must contain 'frame_idx'\")\n",
    "\n",
    "        fused_df[\"frame_idx\"] = fused_df[\"frame_idx\"].astype(int)\n",
    "        self.fused_df = fused_df.set_index(\"frame_idx\", drop=False).sort_index()\n",
    "        self.labels_df = labels_df.reset_index(drop=True)\n",
    "\n",
    "        if not all(c in self.labels_df.columns for c in [\"StartFrame\", \"EndFrame\"]):\n",
    "            raise KeyError(\"labels_df must contain StartFrame and EndFrame\")\n",
    "\n",
    "        self.t_obs = int(t_obs)\n",
    "        self.k_fut = int(k_fut)\n",
    "        self.feat_dim = int(feat_dim)\n",
    "        self.feat_cols = [f\"feat_{i}\" for i in range(self.feat_dim)]\n",
    "\n",
    "        # NEW: time info\n",
    "        self.fps = float(fps)\n",
    "        assert len(horizons_s) == self.k_fut, \"len(horizons_s) must equal k_fut\"\n",
    "        self.horizons_s = list(horizons_s)\n",
    "\n",
    "        # samples: one per label row (use EndFrame as obs_end)\n",
    "        self.samples = []\n",
    "        for ridx, row in self.labels_df.iterrows():\n",
    "            try:\n",
    "                obs_end = int(row[\"EndFrame\"])\n",
    "            except:\n",
    "                continue\n",
    "            self.samples.append({\"label_row_idx\": int(ridx), \"obs_end\": obs_end})\n",
    "\n",
    "        if len(self.samples) == 0:\n",
    "            raise RuntimeError(\"No valid label rows found\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    # === NEW: time-based future labels instead of next segments ===\n",
    "    def _time_based_future_labels(self, obs_end: int):\n",
    "        \"\"\"\n",
    "        For each horizon t in self.horizons_s (seconds),\n",
    "        compute future_frame = obs_end + round(t * fps),\n",
    "        then find which action segment covers that frame.\n",
    "        \"\"\"\n",
    "        labels_df = self.labels_df\n",
    "\n",
    "        def pick(cols):\n",
    "            for c in cols:\n",
    "                if c in labels_df.columns:\n",
    "                    return c\n",
    "            return None\n",
    "\n",
    "        vcol = pick([\"Verb_class\",\"verb\",\"Verb\",\"verb_class\"])\n",
    "        ncol = pick([\"Noun_class\",\"noun\",\"Noun\",\"noun_class\"])\n",
    "        acol = pick([\"Action_class\",\"action\",\"Action\",\"ActionLabel\"])\n",
    "\n",
    "        verb_targets   = []\n",
    "        noun_targets   = []\n",
    "        action_targets = []\n",
    "\n",
    "        for h_sec in self.horizons_s:\n",
    "            future_frame = obs_end + int(round(h_sec * self.fps))\n",
    "            seg = labels_df[(labels_df[\"StartFrame\"] <= future_frame) &\n",
    "                            (labels_df[\"EndFrame\"]   >= future_frame)]\n",
    "            if seg.empty:\n",
    "                # nothing happening at that exact time\n",
    "                verb_targets.append(IGNORE_INDEX)\n",
    "                noun_targets.append(IGNORE_INDEX)\n",
    "                action_targets.append(IGNORE_INDEX)\n",
    "            else:\n",
    "                row = seg.iloc[0]\n",
    "                if vcol is not None and not pd.isna(row[vcol]):\n",
    "                    verb_targets.append(int(row[vcol]))\n",
    "                else:\n",
    "                    verb_targets.append(IGNORE_INDEX)\n",
    "\n",
    "                if ncol is not None and not pd.isna(row[ncol]):\n",
    "                    noun_targets.append(int(row[ncol]))\n",
    "                else:\n",
    "                    noun_targets.append(IGNORE_INDEX)\n",
    "\n",
    "                if acol is not None and not pd.isna(row[acol]):\n",
    "                    action_targets.append(int(row[acol]))\n",
    "                else:\n",
    "                    action_targets.append(IGNORE_INDEX)\n",
    "\n",
    "        return {\n",
    "            \"verb\":   torch.LongTensor(verb_targets),\n",
    "            \"noun\":   torch.LongTensor(noun_targets),\n",
    "            \"action\": torch.LongTensor(action_targets)\n",
    "        }\n",
    "    # ================================================================\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rec = self.samples[idx]\n",
    "        obs_end = rec[\"obs_end\"]\n",
    "        obs_start = obs_end - (self.t_obs - 1)\n",
    "        if obs_start < 0:\n",
    "            obs_start = 0\n",
    "            obs_end = obs_start + (self.t_obs - 1)\n",
    "\n",
    "        fused_idx_min = int(self.fused_df.index.min())\n",
    "        fused_idx_max = int(self.fused_df.index.max())\n",
    "        obs_end = min(obs_end, fused_idx_max)\n",
    "        obs_start = max(obs_end - (self.t_obs - 1), fused_idx_min)\n",
    "\n",
    "        desired = list(range(obs_start, obs_end + 1))\n",
    "        sel = self.fused_df.reindex(desired).fillna(method=\"ffill\").fillna(method=\"bfill\").fillna(0.0)\n",
    "\n",
    "        if sel.shape[0] < self.t_obs:\n",
    "            if sel.shape[0] == 0:\n",
    "                zero_row = {c:0.0 for c in self.feat_cols}\n",
    "                sel = pd.DataFrame([zero_row] * self.t_obs)\n",
    "            else:\n",
    "                first = sel.iloc[[0]]\n",
    "                pads = pd.concat([first] * (self.t_obs - sel.shape[0]), ignore_index=True)\n",
    "                sel = pd.concat([pads, sel.reset_index(drop=True)], ignore_index=True)\n",
    "\n",
    "        for c in self.feat_cols:\n",
    "            if c not in sel.columns:\n",
    "                sel[c] = 0.0\n",
    "\n",
    "        F_window = torch.from_numpy(sel[self.feat_cols].values).float()   # (T_obs, FEAT_DIM)\n",
    "\n",
    "        # OLD:\n",
    "        # y_multi = self._future_labels(self.labels_df, obs_end)\n",
    "        # NEW: time-based labels\n",
    "        y_multi = self._time_based_future_labels(obs_end)\n",
    "\n",
    "        meta = {\"obs_start\": int(obs_start),\n",
    "                \"obs_end\":   int(obs_end),\n",
    "                \"label_row_idx\": int(rec[\"label_row_idx\"])}\n",
    "        return F_window, y_multi, meta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64236cc9",
   "metadata": {},
   "source": [
    "## GRAPH Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "72022a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "K=5   #----------- This K is to make KNN graph -------------\n",
    "DROP=0.1\n",
    "\n",
    "def build_topk_edge_index(features: torch.Tensor, k=K):\n",
    "    \"\"\"\n",
    "    features: (T, D) torch tensor\n",
    "    returns edge_index: (2, E) long tensor (undirected duplicated edges)\n",
    "    \"\"\"\n",
    "    Tn = int(features.size(0))\n",
    "    x = F.normalize(features, dim=1)\n",
    "    sim = torch.matmul(x, x.t())   # (T,T)\n",
    "    sim.fill_diagonal_(-1.0)\n",
    "    vals, idxs = torch.topk(sim, k, dim=1)\n",
    "    src = torch.arange(Tn).unsqueeze(1).expand(-1, k).reshape(-1)\n",
    "    dst = idxs.reshape(-1)\n",
    "    edge = torch.stack([src, dst], dim=0)\n",
    "    edge_rev = torch.stack([dst, src], dim=0)\n",
    "    return torch.cat([edge, edge_rev], dim=1).long()\n",
    "\n",
    "class BatchedGAT(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim=None, num_layers=3, heads=8, dropout=DROP):\n",
    "        super().__init__()\n",
    "        hid = hid_dim or in_dim\n",
    "        self.convs = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_ch = in_dim if i==0 else hid\n",
    "            self.convs.append(GATConv(in_ch, hid//heads, heads=heads, concat=True, dropout=dropout))\n",
    "        self.proj = nn.Linear(hid, in_dim)\n",
    "        self.norm = nn.LayerNorm(in_dim)\n",
    "        self.act = nn.GELU()\n",
    "    def forward(self, pyg_batch: PyGBatch, T_per_sample: int):\n",
    "        x = pyg_batch.x; edge_index = pyg_batch.edge_index\n",
    "        h = x\n",
    "        for conv in self.convs:\n",
    "            h = conv(h, edge_index); h = self.act(h)\n",
    "        h = self.proj(h)\n",
    "        node_feats, mask = to_dense_batch(h, batch=pyg_batch.batch)  # (B, max_nodes, D)\n",
    "        B, max_nodes, D = node_feats.shape\n",
    "        if max_nodes < T_per_sample:\n",
    "            pad = torch.zeros(B, T_per_sample - max_nodes, D, device=node_feats.device)\n",
    "            node_feats = torch.cat([node_feats, pad], dim=1)\n",
    "        elif max_nodes > T_per_sample:\n",
    "            node_feats = node_feats[:, :T_per_sample, :]\n",
    "        return self.norm(node_feats)  # (B, T_per_sample, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4445dbe6",
   "metadata": {},
   "source": [
    "## Encoder, Decoder, Anticipation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b845eee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, nhead=8, num_layers=3, dim_feedforward=2048, dropout=DROP, max_len=1000):\n",
    "        super().__init__()\n",
    "        enc = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, activation='gelu', batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(enc, num_layers=num_layers)\n",
    "        self.pos_emb = nn.Parameter(torch.randn(1, max_len, d_model))\n",
    "    def forward(self, x):\n",
    "        B,T,D = x.shape\n",
    "        pos = self.pos_emb[:, :T, :].to(x.device)\n",
    "        return self.encoder(x + pos)\n",
    "\n",
    "class AnticipationModel(nn.Module):\n",
    "    def __init__(self, feat_dim, num_classes: dict, k_fut=5, gat_layers=3, gat_heads=8, dec_layers=3, dec_heads=8, dropout=DROP):\n",
    "        super().__init__()\n",
    "        self.feat_dim = feat_dim; self.k_fut = k_fut\n",
    "        self.gat = BatchedGAT(in_dim=feat_dim, hid_dim=feat_dim, num_layers=gat_layers, heads=gat_heads, dropout=dropout)\n",
    "        self.encoder = SimpleTransformerEncoder(d_model=feat_dim, nhead=dec_heads, num_layers=3)\n",
    "        dec_layer = nn.TransformerDecoderLayer(d_model=feat_dim, nhead=dec_heads, dim_feedforward=feat_dim*4, dropout=dropout, activation='gelu', batch_first=True)\n",
    "        self.decoder = nn.TransformerDecoder(dec_layer, num_layers=dec_layers)\n",
    "        self.queries = nn.Parameter(torch.randn(1, k_fut, feat_dim))\n",
    "        assert isinstance(num_classes, dict)\n",
    "        self.verb_head = nn.Linear(feat_dim, num_classes[\"verb\"])\n",
    "        self.noun_head = nn.Linear(feat_dim, num_classes[\"noun\"])\n",
    "        self.action_head = nn.Linear(feat_dim, num_classes[\"action\"])\n",
    "\n",
    "    def forward(self, F_batch):\n",
    "        # F_batch: (B, T, D)\n",
    "        B,T,D = F_batch.shape; device = F_batch.device\n",
    "        data_list=[]\n",
    "        for b in range(B):\n",
    "            x = F_batch[b]\n",
    "            edge_index = build_topk_edge_index(x.detach().cpu(), k=K).to(device)\n",
    "            data_list.append(PyGData(x=x, edge_index=edge_index))\n",
    "        pyg_batch = PyGBatch.from_data_list(data_list).to(device)\n",
    "        G = self.gat(pyg_batch, T_per_sample=T)   # (B,T,D)\n",
    "        H = self.encoder(F_batch)                 # (B,T,D)\n",
    "        U = H + G\n",
    "        q = self.queries.expand(B, -1, -1).to(device)\n",
    "        dec_out = self.decoder(tgt=q, memory=U)   # (B, K_fut, D)\n",
    "        return {\"verb\": self.verb_head(dec_out),\n",
    "                \"noun\": self.noun_head(dec_out),\n",
    "                \"action\": self.action_head(dec_out)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd73f69a",
   "metadata": {},
   "source": [
    "## Novel Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b4f9aee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def masked_cross_entropy(logits, labels, ignore_index=IGNORE_INDEX):\n",
    "    \"\"\"\n",
    "    logits: (B,K,C), labels: (B,K)\n",
    "    Ignores positions where label == ignore_index.\n",
    "    Returns a scalar loss that is always connected to the graph\n",
    "    (even if there are no valid labels in a batch).\n",
    "    \"\"\"\n",
    "    B, K, C = logits.shape\n",
    "    logits_flat = logits.view(B * K, C)      # (B*K, C)\n",
    "    labels_flat = labels.view(B * K)         # (B*K,)\n",
    "\n",
    "    loss_flat = F.cross_entropy(\n",
    "        logits_flat,\n",
    "        labels_flat,\n",
    "        reduction='none',\n",
    "        ignore_index=ignore_index\n",
    "    )  # (B*K,)\n",
    "\n",
    "    mask = (labels_flat != ignore_index).float()\n",
    "    valid = mask.sum()\n",
    "\n",
    "    if valid == 0:\n",
    "        # return a zero that still depends on logits so backward() is valid\n",
    "        return (logits_flat * 0.0).sum()\n",
    "\n",
    "    return (loss_flat * mask).sum() / valid\n",
    "\n",
    "\n",
    "def topk_accuracy_per_task(logits, labels, topk=(1,5), ignore_index=IGNORE_INDEX):\n",
    "    \"\"\"\n",
    "    logits: (B,K,C), labels: (B,K)\n",
    "    returns dict with 'per_h' (dict of lists per topk) and 'overall_top{k}' floats\n",
    "    \"\"\"\n",
    "    B,K,C = logits.shape\n",
    "    res = {}\n",
    "    overall = {k:0 for k in topk}\n",
    "    total_cnt = 0\n",
    "    preds_topk = logits.topk(max(topk), dim=-1)[1]  # (B,K,maxk)\n",
    "    for h in range(K):\n",
    "        lab = labels[:,h]; mask = (lab != ignore_index); cnt = int(mask.sum().item())\n",
    "        for k in topk:\n",
    "            if cnt == 0:\n",
    "                res.setdefault(f\"per_h{h+1}_top{k}\", None)\n",
    "                continue\n",
    "            predk = preds_topk[:,h,:k]  # (B,k)\n",
    "            lab_exp = lab.unsqueeze(1).expand(-1, k)\n",
    "            hits = (predk == lab_exp)\n",
    "            hit = int(hits[mask].any(dim=1).float().sum().item())\n",
    "            res[f\"per_h{h+1}_top{k}\"] = hit / cnt\n",
    "            overall[k] += hit\n",
    "        total_cnt += cnt\n",
    "    for k in topk:\n",
    "        res[f\"overall_top{k}\"] = overall[k] / total_cnt if total_cnt>0 else None\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f2c563",
   "metadata": {},
   "source": [
    "\n",
    "## Training and Testing (Validation) HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "501a210d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected num_classes: {'verb': 59, 'noun': 210, 'action': 63}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 Train:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 Val:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 | Time 2.8s\n",
      "  Train Loss: 8.0264 | Val Loss: 7.1594\n",
      "  VERB   Train Top1: 0.3327205882352941, Top5: 0.7371323529411765; Val Top1: 0.5637393767705382, Top5: 0.9008498583569405\n",
      "  NOUN   Train Top1: 0.15441176470588236, Top5: 0.3161764705882353; Val Top1: 0.16147308781869688, Top5: 0.3087818696883853\n",
      "  ACTION Train Top1: 0.07904411764705882, Top5: 0.15808823529411764; Val Top1: 0.12181303116147309, Top5: 0.26628895184135976\n",
      "  VERB   Val Precision: 0.0626, Recall: 0.1111, F1: 0.0801\n",
      "  NOUN   Val Precision: 0.0090, Recall: 0.0556, F1: 0.0154\n",
      "  ACTION Val Precision: 0.0036, Recall: 0.0294, F1: 0.0064\n",
      "  ---- Mean Top-5 Recall (validation) ----\n",
      "     VERB    Mean Top-5 Recall: 0.9023952897726757\n",
      "     NOUN    Mean Top-5 Recall: 0.3101982768174444\n",
      "     ACTION  Mean Top-5 Recall: 0.2670465411517904\n",
      "  VERB   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.5641025641025641  Top5: 0.9487179487179487\n",
      "    @ 0.50s  Top1: 0.5853658536585366  Top5: 0.9512195121951219\n",
      "    @ 0.75s  Top1: 0.5681818181818182  Top5: 0.8863636363636364\n",
      "    @ 1.00s  Top1: 0.5434782608695652  Top5: 0.8913043478260869\n",
      "    @ 1.25s  Top1: 0.5555555555555556  Top5: 0.8888888888888888\n",
      "    @ 1.50s  Top1: 0.5909090909090909  Top5: 0.8863636363636364\n",
      "    @ 1.75s  Top1: 0.5652173913043478  Top5: 0.8913043478260869\n",
      "    @ 2.00s  Top1: 0.5416666666666666  Top5: 0.875\n",
      "    overall_top1: 0.5637393767705382, overall_top5: 0.9008498583569405\n",
      "  NOUN   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.15384615384615385  Top5: 0.358974358974359\n",
      "    @ 0.50s  Top1: 0.14634146341463414  Top5: 0.34146341463414637\n",
      "    @ 0.75s  Top1: 0.1590909090909091  Top5: 0.3181818181818182\n",
      "    @ 1.00s  Top1: 0.15217391304347827  Top5: 0.30434782608695654\n",
      "    @ 1.25s  Top1: 0.15555555555555556  Top5: 0.28888888888888886\n",
      "    @ 1.50s  Top1: 0.18181818181818182  Top5: 0.29545454545454547\n",
      "    @ 1.75s  Top1: 0.17391304347826086  Top5: 0.2826086956521739\n",
      "    @ 2.00s  Top1: 0.16666666666666666  Top5: 0.2916666666666667\n",
      "    overall_top1: 0.16147308781869688, overall_top5: 0.3087818696883853\n",
      "  ACTION per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.1282051282051282  Top5: 0.28205128205128205\n",
      "    @ 0.50s  Top1: 0.12195121951219512  Top5: 0.2926829268292683\n",
      "    @ 0.75s  Top1: 0.11363636363636363  Top5: 0.2727272727272727\n",
      "    @ 1.00s  Top1: 0.10869565217391304  Top5: 0.2608695652173913\n",
      "    @ 1.25s  Top1: 0.1111111111111111  Top5: 0.24444444444444444\n",
      "    @ 1.50s  Top1: 0.13636363636363635  Top5: 0.2727272727272727\n",
      "    @ 1.75s  Top1: 0.13043478260869565  Top5: 0.2608695652173913\n",
      "    @ 2.00s  Top1: 0.125  Top5: 0.25\n",
      "    overall_top1: 0.12181303116147309, overall_top5: 0.26628895184135976\n",
      "[SAVED BEST] -> D:\\Datasets\\Datasets\\EPIC\\Model\\P01_02_fused_model.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 Train:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 Val:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 | Time 2.2s\n",
      "  Train Loss: 6.6309 | Val Loss: 7.0808\n",
      "  VERB   Train Top1: 0.43933823529411764, Top5: 0.9025735294117647; Val Top1: 0.5637393767705382, Top5: 0.8583569405099151\n",
      "  NOUN   Train Top1: 0.18566176470588236, Top5: 0.47058823529411764; Val Top1: 0.16147308781869688, Top5: 0.3314447592067989\n",
      "  ACTION Train Top1: 0.13419117647058823, Top5: 0.24080882352941177; Val Top1: 0.12181303116147309, Top5: 0.23796033994334279\n",
      "  VERB   Val Precision: 0.0626, Recall: 0.1111, F1: 0.0801\n",
      "  NOUN   Val Precision: 0.0090, Recall: 0.0556, F1: 0.0154\n",
      "  ACTION Val Precision: 0.0036, Recall: 0.0294, F1: 0.0064\n",
      "  ---- Mean Top-5 Recall (validation) ----\n",
      "     VERB    Mean Top-5 Recall: 0.8607768663674019\n",
      "     NOUN    Mean Top-5 Recall: 0.3325781110311863\n",
      "     ACTION  Mean Top-5 Recall: 0.238404397645185\n",
      "  VERB   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.5641025641025641  Top5: 0.9230769230769231\n",
      "    @ 0.50s  Top1: 0.5853658536585366  Top5: 0.926829268292683\n",
      "    @ 0.75s  Top1: 0.5681818181818182  Top5: 0.8636363636363636\n",
      "    @ 1.00s  Top1: 0.5434782608695652  Top5: 0.8478260869565217\n",
      "    @ 1.25s  Top1: 0.5555555555555556  Top5: 0.8444444444444444\n",
      "    @ 1.50s  Top1: 0.5909090909090909  Top5: 0.8409090909090909\n",
      "    @ 1.75s  Top1: 0.5652173913043478  Top5: 0.8478260869565217\n",
      "    @ 2.00s  Top1: 0.5416666666666666  Top5: 0.7916666666666666\n",
      "    overall_top1: 0.5637393767705382, overall_top5: 0.8583569405099151\n",
      "  NOUN   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.15384615384615385  Top5: 0.41025641025641024\n",
      "    @ 0.50s  Top1: 0.14634146341463414  Top5: 0.2926829268292683\n",
      "    @ 0.75s  Top1: 0.1590909090909091  Top5: 0.36363636363636365\n",
      "    @ 1.00s  Top1: 0.15217391304347827  Top5: 0.30434782608695654\n",
      "    @ 1.25s  Top1: 0.15555555555555556  Top5: 0.3111111111111111\n",
      "    @ 1.50s  Top1: 0.18181818181818182  Top5: 0.3409090909090909\n",
      "    @ 1.75s  Top1: 0.17391304347826086  Top5: 0.30434782608695654\n",
      "    @ 2.00s  Top1: 0.16666666666666666  Top5: 0.3333333333333333\n",
      "    overall_top1: 0.16147308781869688, overall_top5: 0.3314447592067989\n",
      "  ACTION per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.1282051282051282  Top5: 0.2564102564102564\n",
      "    @ 0.50s  Top1: 0.12195121951219512  Top5: 0.24390243902439024\n",
      "    @ 0.75s  Top1: 0.11363636363636363  Top5: 0.22727272727272727\n",
      "    @ 1.00s  Top1: 0.10869565217391304  Top5: 0.21739130434782608\n",
      "    @ 1.25s  Top1: 0.1111111111111111  Top5: 0.2222222222222222\n",
      "    @ 1.50s  Top1: 0.13636363636363635  Top5: 0.25\n",
      "    @ 1.75s  Top1: 0.13043478260869565  Top5: 0.2608695652173913\n",
      "    @ 2.00s  Top1: 0.125  Top5: 0.22916666666666666\n",
      "    overall_top1: 0.12181303116147309, overall_top5: 0.23796033994334279\n",
      "[SAVED BEST] -> D:\\Datasets\\Datasets\\EPIC\\Model\\P01_02_fused_model.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3 Train:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3 Val:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 | Time 2.3s\n",
      "  Train Loss: 6.3473 | Val Loss: 6.9476\n",
      "  VERB   Train Top1: 0.4411764705882353, Top5: 0.9117647058823529; Val Top1: 0.5637393767705382, Top5: 0.8583569405099151\n",
      "  NOUN   Train Top1: 0.18566176470588236, Top5: 0.45588235294117646; Val Top1: 0.16147308781869688, Top5: 0.29745042492917845\n",
      "  ACTION Train Top1: 0.13419117647058823, Top5: 0.31066176470588236; Val Top1: 0.12181303116147309, Top5: 0.2747875354107649\n",
      "  VERB   Val Precision: 0.0626, Recall: 0.1111, F1: 0.0801\n",
      "  NOUN   Val Precision: 0.0090, Recall: 0.0556, F1: 0.0154\n",
      "  ACTION Val Precision: 0.0036, Recall: 0.0294, F1: 0.0064\n",
      "  ---- Mean Top-5 Recall (validation) ----\n",
      "     VERB    Mean Top-5 Recall: 0.8607768663674019\n",
      "     NOUN    Mean Top-5 Recall: 0.29797032493546277\n",
      "     ACTION  Mean Top-5 Recall: 0.2758072249124741\n",
      "  VERB   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.5641025641025641  Top5: 0.9230769230769231\n",
      "    @ 0.50s  Top1: 0.5853658536585366  Top5: 0.926829268292683\n",
      "    @ 0.75s  Top1: 0.5681818181818182  Top5: 0.8636363636363636\n",
      "    @ 1.00s  Top1: 0.5434782608695652  Top5: 0.8478260869565217\n",
      "    @ 1.25s  Top1: 0.5555555555555556  Top5: 0.8444444444444444\n",
      "    @ 1.50s  Top1: 0.5909090909090909  Top5: 0.8409090909090909\n",
      "    @ 1.75s  Top1: 0.5652173913043478  Top5: 0.8478260869565217\n",
      "    @ 2.00s  Top1: 0.5416666666666666  Top5: 0.7916666666666666\n",
      "    overall_top1: 0.5637393767705382, overall_top5: 0.8583569405099151\n",
      "  NOUN   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.15384615384615385  Top5: 0.3333333333333333\n",
      "    @ 0.50s  Top1: 0.14634146341463414  Top5: 0.2682926829268293\n",
      "    @ 0.75s  Top1: 0.1590909090909091  Top5: 0.29545454545454547\n",
      "    @ 1.00s  Top1: 0.15217391304347827  Top5: 0.2826086956521739\n",
      "    @ 1.25s  Top1: 0.15555555555555556  Top5: 0.28888888888888886\n",
      "    @ 1.50s  Top1: 0.18181818181818182  Top5: 0.3409090909090909\n",
      "    @ 1.75s  Top1: 0.17391304347826086  Top5: 0.2826086956521739\n",
      "    @ 2.00s  Top1: 0.16666666666666666  Top5: 0.2916666666666667\n",
      "    overall_top1: 0.16147308781869688, overall_top5: 0.29745042492917845\n",
      "  ACTION per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.1282051282051282  Top5: 0.3076923076923077\n",
      "    @ 0.50s  Top1: 0.12195121951219512  Top5: 0.2926829268292683\n",
      "    @ 0.75s  Top1: 0.11363636363636363  Top5: 0.25\n",
      "    @ 1.00s  Top1: 0.10869565217391304  Top5: 0.2391304347826087\n",
      "    @ 1.25s  Top1: 0.1111111111111111  Top5: 0.28888888888888886\n",
      "    @ 1.50s  Top1: 0.13636363636363635  Top5: 0.29545454545454547\n",
      "    @ 1.75s  Top1: 0.13043478260869565  Top5: 0.2826086956521739\n",
      "    @ 2.00s  Top1: 0.125  Top5: 0.25\n",
      "    overall_top1: 0.12181303116147309, overall_top5: 0.2747875354107649\n",
      "[SAVED BEST] -> D:\\Datasets\\Datasets\\EPIC\\Model\\P01_02_fused_model.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4 Train:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4 Val:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 | Time 2.2s\n",
      "  Train Loss: 6.0857 | Val Loss: 6.9624\n",
      "  VERB   Train Top1: 0.43933823529411764, Top5: 0.9117647058823529; Val Top1: 0.5637393767705382, Top5: 0.8583569405099151\n",
      "  NOUN   Train Top1: 0.18566176470588236, Top5: 0.4944852941176471; Val Top1: 0.16147308781869688, Top5: 0.3597733711048159\n",
      "  ACTION Train Top1: 0.1488970588235294, Top5: 0.3272058823529412; Val Top1: 0.12464589235127478, Top5: 0.28611898016997167\n",
      "  VERB   Val Precision: 0.0626, Recall: 0.1111, F1: 0.0801\n",
      "  NOUN   Val Precision: 0.0090, Recall: 0.0556, F1: 0.0154\n",
      "  ACTION Val Precision: 0.0130, Recall: 0.0364, F1: 0.0164\n",
      "  ---- Mean Top-5 Recall (validation) ----\n",
      "     VERB    Mean Top-5 Recall: 0.8607768663674019\n",
      "     NOUN    Mean Top-5 Recall: 0.3604738744303299\n",
      "     ACTION  Mean Top-5 Recall: 0.287445822889553\n",
      "  VERB   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.5641025641025641  Top5: 0.9230769230769231\n",
      "    @ 0.50s  Top1: 0.5853658536585366  Top5: 0.926829268292683\n",
      "    @ 0.75s  Top1: 0.5681818181818182  Top5: 0.8636363636363636\n",
      "    @ 1.00s  Top1: 0.5434782608695652  Top5: 0.8478260869565217\n",
      "    @ 1.25s  Top1: 0.5555555555555556  Top5: 0.8444444444444444\n",
      "    @ 1.50s  Top1: 0.5909090909090909  Top5: 0.8409090909090909\n",
      "    @ 1.75s  Top1: 0.5652173913043478  Top5: 0.8478260869565217\n",
      "    @ 2.00s  Top1: 0.5416666666666666  Top5: 0.7916666666666666\n",
      "    overall_top1: 0.5637393767705382, overall_top5: 0.8583569405099151\n",
      "  NOUN   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.15384615384615385  Top5: 0.38461538461538464\n",
      "    @ 0.50s  Top1: 0.14634146341463414  Top5: 0.3902439024390244\n",
      "    @ 0.75s  Top1: 0.1590909090909091  Top5: 0.36363636363636365\n",
      "    @ 1.00s  Top1: 0.15217391304347827  Top5: 0.3695652173913043\n",
      "    @ 1.25s  Top1: 0.15555555555555556  Top5: 0.35555555555555557\n",
      "    @ 1.50s  Top1: 0.18181818181818182  Top5: 0.3181818181818182\n",
      "    @ 1.75s  Top1: 0.17391304347826086  Top5: 0.34782608695652173\n",
      "    @ 2.00s  Top1: 0.16666666666666666  Top5: 0.3541666666666667\n",
      "    overall_top1: 0.16147308781869688, overall_top5: 0.3597733711048159\n",
      "  ACTION per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.1282051282051282  Top5: 0.3333333333333333\n",
      "    @ 0.50s  Top1: 0.12195121951219512  Top5: 0.3170731707317073\n",
      "    @ 0.75s  Top1: 0.11363636363636363  Top5: 0.2727272727272727\n",
      "    @ 1.00s  Top1: 0.10869565217391304  Top5: 0.2608695652173913\n",
      "    @ 1.25s  Top1: 0.13333333333333333  Top5: 0.26666666666666666\n",
      "    @ 1.50s  Top1: 0.13636363636363635  Top5: 0.29545454545454547\n",
      "    @ 1.75s  Top1: 0.13043478260869565  Top5: 0.2826086956521739\n",
      "    @ 2.00s  Top1: 0.125  Top5: 0.2708333333333333\n",
      "    overall_top1: 0.12464589235127478, overall_top5: 0.28611898016997167\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5 Train:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5 Val:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 | Time 2.2s\n",
      "  Train Loss: 5.9684 | Val Loss: 7.0624\n",
      "  VERB   Train Top1: 0.43933823529411764, Top5: 0.9117647058823529; Val Top1: 0.5637393767705382, Top5: 0.8583569405099151\n",
      "  NOUN   Train Top1: 0.18382352941176472, Top5: 0.5091911764705882; Val Top1: 0.16147308781869688, Top5: 0.3087818696883853\n",
      "  ACTION Train Top1: 0.14338235294117646, Top5: 0.34558823529411764; Val Top1: 0.12181303116147309, Top5: 0.19263456090651557\n",
      "  VERB   Val Precision: 0.0626, Recall: 0.1111, F1: 0.0801\n",
      "  NOUN   Val Precision: 0.0106, Recall: 0.0556, F1: 0.0178\n",
      "  ACTION Val Precision: 0.0036, Recall: 0.0294, F1: 0.0064\n",
      "  ---- Mean Top-5 Recall (validation) ----\n",
      "     VERB    Mean Top-5 Recall: 0.8607768663674019\n",
      "     NOUN    Mean Top-5 Recall: 0.3086307042712796\n",
      "     ACTION  Mean Top-5 Recall: 0.19280352854550892\n",
      "  VERB   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.5641025641025641  Top5: 0.9230769230769231\n",
      "    @ 0.50s  Top1: 0.5853658536585366  Top5: 0.926829268292683\n",
      "    @ 0.75s  Top1: 0.5681818181818182  Top5: 0.8636363636363636\n",
      "    @ 1.00s  Top1: 0.5434782608695652  Top5: 0.8478260869565217\n",
      "    @ 1.25s  Top1: 0.5555555555555556  Top5: 0.8444444444444444\n",
      "    @ 1.50s  Top1: 0.5909090909090909  Top5: 0.8409090909090909\n",
      "    @ 1.75s  Top1: 0.5652173913043478  Top5: 0.8478260869565217\n",
      "    @ 2.00s  Top1: 0.5416666666666666  Top5: 0.7916666666666666\n",
      "    overall_top1: 0.5637393767705382, overall_top5: 0.8583569405099151\n",
      "  NOUN   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.15384615384615385  Top5: 0.3076923076923077\n",
      "    @ 0.50s  Top1: 0.14634146341463414  Top5: 0.2926829268292683\n",
      "    @ 0.75s  Top1: 0.1590909090909091  Top5: 0.3181818181818182\n",
      "    @ 1.00s  Top1: 0.15217391304347827  Top5: 0.30434782608695654\n",
      "    @ 1.25s  Top1: 0.15555555555555556  Top5: 0.3111111111111111\n",
      "    @ 1.50s  Top1: 0.18181818181818182  Top5: 0.3181818181818182\n",
      "    @ 1.75s  Top1: 0.17391304347826086  Top5: 0.30434782608695654\n",
      "    @ 2.00s  Top1: 0.16666666666666666  Top5: 0.3125\n",
      "    overall_top1: 0.16147308781869688, overall_top5: 0.3087818696883853\n",
      "  ACTION per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.1282051282051282  Top5: 0.1794871794871795\n",
      "    @ 0.50s  Top1: 0.12195121951219512  Top5: 0.21951219512195122\n",
      "    @ 0.75s  Top1: 0.11363636363636363  Top5: 0.18181818181818182\n",
      "    @ 1.00s  Top1: 0.10869565217391304  Top5: 0.17391304347826086\n",
      "    @ 1.25s  Top1: 0.1111111111111111  Top5: 0.2\n",
      "    @ 1.50s  Top1: 0.13636363636363635  Top5: 0.20454545454545456\n",
      "    @ 1.75s  Top1: 0.13043478260869565  Top5: 0.1956521739130435\n",
      "    @ 2.00s  Top1: 0.125  Top5: 0.1875\n",
      "    overall_top1: 0.12181303116147309, overall_top5: 0.19263456090651557\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6 Train:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6 Val:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 | Time 2.3s\n",
      "  Train Loss: 5.6904 | Val Loss: 6.8768\n",
      "  VERB   Train Top1: 0.4411764705882353, Top5: 0.9136029411764706; Val Top1: 0.5694050991501416, Top5: 0.8583569405099151\n",
      "  NOUN   Train Top1: 0.22794117647058823, Top5: 0.5349264705882353; Val Top1: 0.18413597733711048, Top5: 0.42776203966005666\n",
      "  ACTION Train Top1: 0.17463235294117646, Top5: 0.4025735294117647; Val Top1: 0.1558073654390935, Top5: 0.2577903682719547\n",
      "  VERB   Val Precision: 0.1227, Recall: 0.1522, F1: 0.1307\n",
      "  NOUN   Val Precision: 0.0356, Recall: 0.0704, F1: 0.0350\n",
      "  ACTION Val Precision: 0.0464, Recall: 0.0450, F1: 0.0284\n",
      "  ---- Mean Top-5 Recall (validation) ----\n",
      "     VERB    Mean Top-5 Recall: 0.8607768663674019\n",
      "     NOUN    Mean Top-5 Recall: 0.4264823074224585\n",
      "     ACTION  Mean Top-5 Recall: 0.25850259158185984\n",
      "  VERB   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.5641025641025641  Top5: 0.9230769230769231\n",
      "    @ 0.50s  Top1: 0.5853658536585366  Top5: 0.926829268292683\n",
      "    @ 0.75s  Top1: 0.5909090909090909  Top5: 0.8636363636363636\n",
      "    @ 1.00s  Top1: 0.5434782608695652  Top5: 0.8478260869565217\n",
      "    @ 1.25s  Top1: 0.5555555555555556  Top5: 0.8444444444444444\n",
      "    @ 1.50s  Top1: 0.5909090909090909  Top5: 0.8409090909090909\n",
      "    @ 1.75s  Top1: 0.5869565217391305  Top5: 0.8478260869565217\n",
      "    @ 2.00s  Top1: 0.5416666666666666  Top5: 0.7916666666666666\n",
      "    overall_top1: 0.5694050991501416, overall_top5: 0.8583569405099151\n",
      "  NOUN   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.1794871794871795  Top5: 0.41025641025641024\n",
      "    @ 0.50s  Top1: 0.17073170731707318  Top5: 0.3902439024390244\n",
      "    @ 0.75s  Top1: 0.18181818181818182  Top5: 0.4318181818181818\n",
      "    @ 1.00s  Top1: 0.17391304347826086  Top5: 0.43478260869565216\n",
      "    @ 1.25s  Top1: 0.17777777777777778  Top5: 0.4444444444444444\n",
      "    @ 1.50s  Top1: 0.20454545454545456  Top5: 0.38636363636363635\n",
      "    @ 1.75s  Top1: 0.1956521739130435  Top5: 0.43478260869565216\n",
      "    @ 2.00s  Top1: 0.1875  Top5: 0.4791666666666667\n",
      "    overall_top1: 0.18413597733711048, overall_top5: 0.42776203966005666\n",
      "  ACTION per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.15384615384615385  Top5: 0.28205128205128205\n",
      "    @ 0.50s  Top1: 0.17073170731707318  Top5: 0.2682926829268293\n",
      "    @ 0.75s  Top1: 0.1590909090909091  Top5: 0.25\n",
      "    @ 1.00s  Top1: 0.13043478260869565  Top5: 0.2608695652173913\n",
      "    @ 1.25s  Top1: 0.15555555555555556  Top5: 0.2222222222222222\n",
      "    @ 1.50s  Top1: 0.18181818181818182  Top5: 0.29545454545454547\n",
      "    @ 1.75s  Top1: 0.15217391304347827  Top5: 0.2391304347826087\n",
      "    @ 2.00s  Top1: 0.14583333333333334  Top5: 0.25\n",
      "    overall_top1: 0.1558073654390935, overall_top5: 0.2577903682719547\n",
      "[SAVED BEST] -> D:\\Datasets\\Datasets\\EPIC\\Model\\P01_02_fused_model.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7 Train:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7 Val:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 | Time 2.3s\n",
      "  Train Loss: 5.3004 | Val Loss: 6.9518\n",
      "  VERB   Train Top1: 0.48161764705882354, Top5: 0.9338235294117647; Val Top1: 0.5779036827195467, Top5: 0.8640226628895185\n",
      "  NOUN   Train Top1: 0.2757352941176471, Top5: 0.6397058823529411; Val Top1: 0.18413597733711048, Top5: 0.39943342776203966\n",
      "  ACTION Train Top1: 0.19301470588235295, Top5: 0.4632352941176471; Val Top1: 0.141643059490085, Top5: 0.2521246458923513\n",
      "  VERB   Val Precision: 0.1381, Recall: 0.1539, F1: 0.1364\n",
      "  NOUN   Val Precision: 0.0588, Recall: 0.0704, F1: 0.0388\n",
      "  ACTION Val Precision: 0.0088, Recall: 0.0373, F1: 0.0132\n",
      "  ---- Mean Top-5 Recall (validation) ----\n",
      "     VERB    Mean Top-5 Recall: 0.8652264600899278\n",
      "     NOUN    Mean Top-5 Recall: 0.399319423948268\n",
      "     ACTION  Mean Top-5 Recall: 0.2518714257861262\n",
      "  VERB   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.5897435897435898  Top5: 0.8974358974358975\n",
      "    @ 0.50s  Top1: 0.5853658536585366  Top5: 0.9024390243902439\n",
      "    @ 0.75s  Top1: 0.5909090909090909  Top5: 0.8409090909090909\n",
      "    @ 1.00s  Top1: 0.5652173913043478  Top5: 0.8478260869565217\n",
      "    @ 1.25s  Top1: 0.5777777777777777  Top5: 0.8666666666666667\n",
      "    @ 1.50s  Top1: 0.5909090909090909  Top5: 0.8636363636363636\n",
      "    @ 1.75s  Top1: 0.5869565217391305  Top5: 0.8695652173913043\n",
      "    @ 2.00s  Top1: 0.5416666666666666  Top5: 0.8333333333333334\n",
      "    overall_top1: 0.5779036827195467, overall_top5: 0.8640226628895185\n",
      "  NOUN   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.1794871794871795  Top5: 0.41025641025641024\n",
      "    @ 0.50s  Top1: 0.17073170731707318  Top5: 0.36585365853658536\n",
      "    @ 0.75s  Top1: 0.18181818181818182  Top5: 0.38636363636363635\n",
      "    @ 1.00s  Top1: 0.17391304347826086  Top5: 0.391304347826087\n",
      "    @ 1.25s  Top1: 0.17777777777777778  Top5: 0.4\n",
      "    @ 1.50s  Top1: 0.20454545454545456  Top5: 0.45454545454545453\n",
      "    @ 1.75s  Top1: 0.1956521739130435  Top5: 0.3695652173913043\n",
      "    @ 2.00s  Top1: 0.1875  Top5: 0.4166666666666667\n",
      "    overall_top1: 0.18413597733711048, overall_top5: 0.39943342776203966\n",
      "  ACTION per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.1282051282051282  Top5: 0.23076923076923078\n",
      "    @ 0.50s  Top1: 0.14634146341463414  Top5: 0.24390243902439024\n",
      "    @ 0.75s  Top1: 0.13636363636363635  Top5: 0.22727272727272727\n",
      "    @ 1.00s  Top1: 0.13043478260869565  Top5: 0.2391304347826087\n",
      "    @ 1.25s  Top1: 0.13333333333333333  Top5: 0.26666666666666666\n",
      "    @ 1.50s  Top1: 0.1590909090909091  Top5: 0.29545454545454547\n",
      "    @ 1.75s  Top1: 0.15217391304347827  Top5: 0.2826086956521739\n",
      "    @ 2.00s  Top1: 0.14583333333333334  Top5: 0.22916666666666666\n",
      "    overall_top1: 0.141643059490085, overall_top5: 0.2521246458923513\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8 Train:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8 Val:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 | Time 2.3s\n",
      "  Train Loss: 4.8697 | Val Loss: 6.9778\n",
      "  VERB   Train Top1: 0.5257352941176471, Top5: 0.9209558823529411; Val Top1: 0.5637393767705382, Top5: 0.8838526912181303\n",
      "  NOUN   Train Top1: 0.32169117647058826, Top5: 0.71875; Val Top1: 0.20396600566572237, Top5: 0.40793201133144474\n",
      "  ACTION Train Top1: 0.22058823529411764, Top5: 0.6801470588235294; Val Top1: 0.17280453257790368, Top5: 0.22662889518413598\n",
      "  VERB   Val Precision: 0.1192, Recall: 0.1511, F1: 0.1303\n",
      "  NOUN   Val Precision: 0.0953, Recall: 0.0842, F1: 0.0668\n",
      "  ACTION Val Precision: 0.0745, Recall: 0.0519, F1: 0.0495\n",
      "  ---- Mean Top-5 Recall (validation) ----\n",
      "     VERB    Mean Top-5 Recall: 0.8861334869619599\n",
      "     NOUN    Mean Top-5 Recall: 0.4082036254955131\n",
      "     ACTION  Mean Top-5 Recall: 0.2256270079043144\n",
      "  VERB   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.5641025641025641  Top5: 0.9487179487179487\n",
      "    @ 0.50s  Top1: 0.5853658536585366  Top5: 0.9512195121951219\n",
      "    @ 0.75s  Top1: 0.5681818181818182  Top5: 0.8863636363636364\n",
      "    @ 1.00s  Top1: 0.5434782608695652  Top5: 0.8695652173913043\n",
      "    @ 1.25s  Top1: 0.5555555555555556  Top5: 0.8666666666666667\n",
      "    @ 1.50s  Top1: 0.5909090909090909  Top5: 0.8636363636363636\n",
      "    @ 1.75s  Top1: 0.5652173913043478  Top5: 0.8695652173913043\n",
      "    @ 2.00s  Top1: 0.5416666666666666  Top5: 0.8333333333333334\n",
      "    overall_top1: 0.5637393767705382, overall_top5: 0.8838526912181303\n",
      "  NOUN   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.1794871794871795  Top5: 0.4358974358974359\n",
      "    @ 0.50s  Top1: 0.1951219512195122  Top5: 0.36585365853658536\n",
      "    @ 0.75s  Top1: 0.20454545454545456  Top5: 0.4090909090909091\n",
      "    @ 1.00s  Top1: 0.1956521739130435  Top5: 0.3695652173913043\n",
      "    @ 1.25s  Top1: 0.2  Top5: 0.4444444444444444\n",
      "    @ 1.50s  Top1: 0.22727272727272727  Top5: 0.45454545454545453\n",
      "    @ 1.75s  Top1: 0.1956521739130435  Top5: 0.3695652173913043\n",
      "    @ 2.00s  Top1: 0.22916666666666666  Top5: 0.4166666666666667\n",
      "    overall_top1: 0.20396600566572237, overall_top5: 0.40793201133144474\n",
      "  ACTION per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.1794871794871795  Top5: 0.1794871794871795\n",
      "    @ 0.50s  Top1: 0.17073170731707318  Top5: 0.21951219512195122\n",
      "    @ 0.75s  Top1: 0.1590909090909091  Top5: 0.22727272727272727\n",
      "    @ 1.00s  Top1: 0.15217391304347827  Top5: 0.21739130434782608\n",
      "    @ 1.25s  Top1: 0.15555555555555556  Top5: 0.2222222222222222\n",
      "    @ 1.50s  Top1: 0.20454545454545456  Top5: 0.25\n",
      "    @ 1.75s  Top1: 0.17391304347826086  Top5: 0.2391304347826087\n",
      "    @ 2.00s  Top1: 0.1875  Top5: 0.25\n",
      "    overall_top1: 0.17280453257790368, overall_top5: 0.22662889518413598\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9 Train:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9 Val:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 | Time 2.3s\n",
      "  Train Loss: 4.1044 | Val Loss: 6.7722\n",
      "  VERB   Train Top1: 0.5386029411764706, Top5: 0.9540441176470589; Val Top1: 0.5637393767705382, Top5: 0.8526912181303116\n",
      "  NOUN   Train Top1: 0.4742647058823529, Top5: 0.8382352941176471; Val Top1: 0.2096317280453258, Top5: 0.46742209631728043\n",
      "  ACTION Train Top1: 0.3952205882352941, Top5: 0.7536764705882353; Val Top1: 0.18980169971671387, Top5: 0.2804532577903683\n",
      "  VERB   Val Precision: 0.1147, Recall: 0.1360, F1: 0.1206\n",
      "  NOUN   Val Precision: 0.1159, Recall: 0.0926, F1: 0.0720\n",
      "  ACTION Val Precision: 0.0737, Recall: 0.0799, F1: 0.0509\n",
      "  ---- Mean Top-5 Recall (validation) ----\n",
      "     VERB    Mean Top-5 Recall: 0.8543994398879076\n",
      "     NOUN    Mean Top-5 Recall: 0.467110271128365\n",
      "     ACTION  Mean Top-5 Recall: 0.2792312515088894\n",
      "  VERB   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.5641025641025641  Top5: 0.8974358974358975\n",
      "    @ 0.50s  Top1: 0.5853658536585366  Top5: 0.9024390243902439\n",
      "    @ 0.75s  Top1: 0.5681818181818182  Top5: 0.8409090909090909\n",
      "    @ 1.00s  Top1: 0.5434782608695652  Top5: 0.8478260869565217\n",
      "    @ 1.25s  Top1: 0.5555555555555556  Top5: 0.8444444444444444\n",
      "    @ 1.50s  Top1: 0.5909090909090909  Top5: 0.8409090909090909\n",
      "    @ 1.75s  Top1: 0.5652173913043478  Top5: 0.8695652173913043\n",
      "    @ 2.00s  Top1: 0.5416666666666666  Top5: 0.7916666666666666\n",
      "    overall_top1: 0.5637393767705382, overall_top5: 0.8526912181303116\n",
      "  NOUN   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.1794871794871795  Top5: 0.46153846153846156\n",
      "    @ 0.50s  Top1: 0.1951219512195122  Top5: 0.4634146341463415\n",
      "    @ 0.75s  Top1: 0.20454545454545456  Top5: 0.4090909090909091\n",
      "    @ 1.00s  Top1: 0.1956521739130435  Top5: 0.45652173913043476\n",
      "    @ 1.25s  Top1: 0.2222222222222222  Top5: 0.4888888888888889\n",
      "    @ 1.50s  Top1: 0.22727272727272727  Top5: 0.5\n",
      "    @ 1.75s  Top1: 0.21739130434782608  Top5: 0.4782608695652174\n",
      "    @ 2.00s  Top1: 0.22916666666666666  Top5: 0.4791666666666667\n",
      "    overall_top1: 0.2096317280453258, overall_top5: 0.46742209631728043\n",
      "  ACTION per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.1794871794871795  Top5: 0.23076923076923078\n",
      "    @ 0.50s  Top1: 0.1951219512195122  Top5: 0.2682926829268293\n",
      "    @ 0.75s  Top1: 0.18181818181818182  Top5: 0.25\n",
      "    @ 1.00s  Top1: 0.17391304347826086  Top5: 0.2391304347826087\n",
      "    @ 1.25s  Top1: 0.17777777777777778  Top5: 0.28888888888888886\n",
      "    @ 1.50s  Top1: 0.20454545454545456  Top5: 0.3181818181818182\n",
      "    @ 1.75s  Top1: 0.1956521739130435  Top5: 0.32608695652173914\n",
      "    @ 2.00s  Top1: 0.20833333333333334  Top5: 0.3125\n",
      "    overall_top1: 0.18980169971671387, overall_top5: 0.2804532577903683\n",
      "[SAVED BEST] -> D:\\Datasets\\Datasets\\EPIC\\Model\\P01_02_fused_model.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10 Train:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10 Val:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 | Time 2.3s\n",
      "  Train Loss: 3.5929 | Val Loss: 6.8810\n",
      "  VERB   Train Top1: 0.49816176470588236, Top5: 0.9669117647058824; Val Top1: 0.5609065155807366, Top5: 0.8441926345609065\n",
      "  NOUN   Train Top1: 0.5091911764705882, Top5: 0.9080882352941176; Val Top1: 0.20679886685552407, Top5: 0.45042492917847027\n",
      "  ACTION Train Top1: 0.5165441176470589, Top5: 0.875; Val Top1: 0.22946175637393768, Top5: 0.2776203966005666\n",
      "  VERB   Val Precision: 0.1181, Recall: 0.1505, F1: 0.1292\n",
      "  NOUN   Val Precision: 0.0944, Recall: 0.0864, F1: 0.0685\n",
      "  ACTION Val Precision: 0.1029, Recall: 0.0972, F1: 0.0798\n",
      "  ---- Mean Top-5 Recall (validation) ----\n",
      "     VERB    Mean Top-5 Recall: 0.845078172828703\n",
      "     NOUN    Mean Top-5 Recall: 0.4495950215611536\n",
      "     ACTION  Mean Top-5 Recall: 0.27714908202878774\n",
      "  VERB   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.5384615384615384  Top5: 0.8717948717948718\n",
      "    @ 0.50s  Top1: 0.5853658536585366  Top5: 0.8780487804878049\n",
      "    @ 0.75s  Top1: 0.5681818181818182  Top5: 0.8181818181818182\n",
      "    @ 1.00s  Top1: 0.5434782608695652  Top5: 0.8260869565217391\n",
      "    @ 1.25s  Top1: 0.5555555555555556  Top5: 0.8444444444444444\n",
      "    @ 1.50s  Top1: 0.5909090909090909  Top5: 0.8409090909090909\n",
      "    @ 1.75s  Top1: 0.5652173913043478  Top5: 0.8478260869565217\n",
      "    @ 2.00s  Top1: 0.5416666666666666  Top5: 0.8333333333333334\n",
      "    overall_top1: 0.5609065155807366, overall_top5: 0.8441926345609065\n",
      "  NOUN   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.1794871794871795  Top5: 0.4358974358974359\n",
      "    @ 0.50s  Top1: 0.1951219512195122  Top5: 0.4146341463414634\n",
      "    @ 0.75s  Top1: 0.20454545454545456  Top5: 0.4318181818181818\n",
      "    @ 1.00s  Top1: 0.1956521739130435  Top5: 0.4782608695652174\n",
      "    @ 1.25s  Top1: 0.2  Top5: 0.4666666666666667\n",
      "    @ 1.50s  Top1: 0.22727272727272727  Top5: 0.4772727272727273\n",
      "    @ 1.75s  Top1: 0.21739130434782608  Top5: 0.41304347826086957\n",
      "    @ 2.00s  Top1: 0.22916666666666666  Top5: 0.4791666666666667\n",
      "    overall_top1: 0.20679886685552407, overall_top5: 0.45042492917847027\n",
      "  ACTION per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.23076923076923078  Top5: 0.2564102564102564\n",
      "    @ 0.50s  Top1: 0.21951219512195122  Top5: 0.2926829268292683\n",
      "    @ 0.75s  Top1: 0.22727272727272727  Top5: 0.25\n",
      "    @ 1.00s  Top1: 0.21739130434782608  Top5: 0.2608695652173913\n",
      "    @ 1.25s  Top1: 0.2222222222222222  Top5: 0.26666666666666666\n",
      "    @ 1.50s  Top1: 0.22727272727272727  Top5: 0.29545454545454547\n",
      "    @ 1.75s  Top1: 0.2391304347826087  Top5: 0.2826086956521739\n",
      "    @ 2.00s  Top1: 0.25  Top5: 0.3125\n",
      "    overall_top1: 0.22946175637393768, overall_top5: 0.2776203966005666\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11 Train:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11 Val:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 | Time 2.2s\n",
      "  Train Loss: 3.0497 | Val Loss: 6.8453\n",
      "  VERB   Train Top1: 0.5992647058823529, Top5: 0.9926470588235294; Val Top1: 0.5014164305949008, Top5: 0.8526912181303116\n",
      "  NOUN   Train Top1: 0.5900735294117647, Top5: 0.9430147058823529; Val Top1: 0.2804532577903683, Top5: 0.46458923512747874\n",
      "  ACTION Train Top1: 0.6507352941176471, Top5: 0.9466911764705882; Val Top1: 0.21246458923512748, Top5: 0.2577903682719547\n",
      "  VERB   Val Precision: 0.1553, Recall: 0.1266, F1: 0.1304\n",
      "  NOUN   Val Precision: 0.2279, Recall: 0.1528, F1: 0.1361\n",
      "  ACTION Val Precision: 0.0641, Recall: 0.0839, F1: 0.0568\n",
      "  ---- Mean Top-5 Recall (validation) ----\n",
      "     VERB    Mean Top-5 Recall: 0.8541729906125453\n",
      "     NOUN    Mean Top-5 Recall: 0.46409179202292933\n",
      "     ACTION  Mean Top-5 Recall: 0.25684301669521714\n",
      "  VERB   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.46153846153846156  Top5: 0.8974358974358975\n",
      "    @ 0.50s  Top1: 0.4634146341463415  Top5: 0.9024390243902439\n",
      "    @ 0.75s  Top1: 0.5  Top5: 0.8409090909090909\n",
      "    @ 1.00s  Top1: 0.4782608695652174  Top5: 0.8260869565217391\n",
      "    @ 1.25s  Top1: 0.5111111111111111  Top5: 0.8444444444444444\n",
      "    @ 1.50s  Top1: 0.5454545454545454  Top5: 0.8409090909090909\n",
      "    @ 1.75s  Top1: 0.5434782608695652  Top5: 0.8478260869565217\n",
      "    @ 2.00s  Top1: 0.5  Top5: 0.8333333333333334\n",
      "    overall_top1: 0.5014164305949008, overall_top5: 0.8526912181303116\n",
      "  NOUN   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.2564102564102564  Top5: 0.4358974358974359\n",
      "    @ 0.50s  Top1: 0.2682926829268293  Top5: 0.4634146341463415\n",
      "    @ 0.75s  Top1: 0.2727272727272727  Top5: 0.4772727272727273\n",
      "    @ 1.00s  Top1: 0.2608695652173913  Top5: 0.4782608695652174\n",
      "    @ 1.25s  Top1: 0.28888888888888886  Top5: 0.4666666666666667\n",
      "    @ 1.50s  Top1: 0.3181818181818182  Top5: 0.4772727272727273\n",
      "    @ 1.75s  Top1: 0.2826086956521739  Top5: 0.43478260869565216\n",
      "    @ 2.00s  Top1: 0.2916666666666667  Top5: 0.4791666666666667\n",
      "    overall_top1: 0.2804532577903683, overall_top5: 0.46458923512747874\n",
      "  ACTION per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.20512820512820512  Top5: 0.23076923076923078\n",
      "    @ 0.50s  Top1: 0.21951219512195122  Top5: 0.24390243902439024\n",
      "    @ 0.75s  Top1: 0.20454545454545456  Top5: 0.22727272727272727\n",
      "    @ 1.00s  Top1: 0.1956521739130435  Top5: 0.2391304347826087\n",
      "    @ 1.25s  Top1: 0.2  Top5: 0.26666666666666666\n",
      "    @ 1.50s  Top1: 0.22727272727272727  Top5: 0.2727272727272727\n",
      "    @ 1.75s  Top1: 0.21739130434782608  Top5: 0.2826086956521739\n",
      "    @ 2.00s  Top1: 0.22916666666666666  Top5: 0.2916666666666667\n",
      "    overall_top1: 0.21246458923512748, overall_top5: 0.2577903682719547\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12 Train:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12 Val:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 | Time 2.3s\n",
      "  Train Loss: 2.5492 | Val Loss: 6.7522\n",
      "  VERB   Train Top1: 0.6819852941176471, Top5: 0.9797794117647058; Val Top1: 0.3371104815864023, Top5: 0.7903682719546742\n",
      "  NOUN   Train Top1: 0.6838235294117647, Top5: 0.96875; Val Top1: 0.32294617563739375, Top5: 0.5609065155807366\n",
      "  ACTION Train Top1: 0.7481617647058824, Top5: 0.9650735294117647; Val Top1: 0.21529745042492918, Top5: 0.32011331444759206\n",
      "  VERB   Val Precision: 0.2412, Recall: 0.2033, F1: 0.1954\n",
      "  NOUN   Val Precision: 0.2347, Recall: 0.2268, F1: 0.1738\n",
      "  ACTION Val Precision: 0.1193, Recall: 0.0994, F1: 0.0873\n",
      "  ---- Mean Top-5 Recall (validation) ----\n",
      "     VERB    Mean Top-5 Recall: 0.7922827621594855\n",
      "     NOUN    Mean Top-5 Recall: 0.5610213824042695\n",
      "     ACTION  Mean Top-5 Recall: 0.3203193955729083\n",
      "  VERB   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.3333333333333333  Top5: 0.8461538461538461\n",
      "    @ 0.50s  Top1: 0.34146341463414637  Top5: 0.8536585365853658\n",
      "    @ 0.75s  Top1: 0.3181818181818182  Top5: 0.7727272727272727\n",
      "    @ 1.00s  Top1: 0.3695652173913043  Top5: 0.782608695652174\n",
      "    @ 1.25s  Top1: 0.35555555555555557  Top5: 0.7777777777777778\n",
      "    @ 1.50s  Top1: 0.3409090909090909  Top5: 0.7727272727272727\n",
      "    @ 1.75s  Top1: 0.34782608695652173  Top5: 0.782608695652174\n",
      "    @ 2.00s  Top1: 0.2916666666666667  Top5: 0.75\n",
      "    overall_top1: 0.3371104815864023, overall_top5: 0.7903682719546742\n",
      "  NOUN   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.3076923076923077  Top5: 0.5641025641025641\n",
      "    @ 0.50s  Top1: 0.34146341463414637  Top5: 0.5609756097560976\n",
      "    @ 0.75s  Top1: 0.3181818181818182  Top5: 0.5909090909090909\n",
      "    @ 1.00s  Top1: 0.32608695652173914  Top5: 0.5652173913043478\n",
      "    @ 1.25s  Top1: 0.3111111111111111  Top5: 0.6\n",
      "    @ 1.50s  Top1: 0.3409090909090909  Top5: 0.5227272727272727\n",
      "    @ 1.75s  Top1: 0.32608695652173914  Top5: 0.5217391304347826\n",
      "    @ 2.00s  Top1: 0.3125  Top5: 0.5625\n",
      "    overall_top1: 0.32294617563739375, overall_top5: 0.5609065155807366\n",
      "  ACTION per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.23076923076923078  Top5: 0.3333333333333333\n",
      "    @ 0.50s  Top1: 0.21951219512195122  Top5: 0.3170731707317073\n",
      "    @ 0.75s  Top1: 0.20454545454545456  Top5: 0.3181818181818182\n",
      "    @ 1.00s  Top1: 0.1956521739130435  Top5: 0.34782608695652173\n",
      "    @ 1.25s  Top1: 0.2  Top5: 0.3111111111111111\n",
      "    @ 1.50s  Top1: 0.20454545454545456  Top5: 0.3181818181818182\n",
      "    @ 1.75s  Top1: 0.21739130434782608  Top5: 0.30434782608695654\n",
      "    @ 2.00s  Top1: 0.25  Top5: 0.3125\n",
      "    overall_top1: 0.21529745042492918, overall_top5: 0.32011331444759206\n",
      "[SAVED BEST] -> D:\\Datasets\\Datasets\\EPIC\\Model\\P01_02_fused_model.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13 Train:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13 Val:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 | Time 2.3s\n",
      "  Train Loss: 2.1420 | Val Loss: 7.2606\n",
      "  VERB   Train Top1: 0.6838235294117647, Top5: 0.9963235294117647; Val Top1: 0.3286118980169972, Top5: 0.7705382436260623\n",
      "  NOUN   Train Top1: 0.78125, Top5: 0.9705882352941176; Val Top1: 0.2719546742209632, Top5: 0.4192634560906516\n",
      "  ACTION Train Top1: 0.7720588235294118, Top5: 0.9816176470588235; Val Top1: 0.1813031161473088, Top5: 0.254957507082153\n",
      "  VERB   Val Precision: 0.2283, Recall: 0.1687, F1: 0.1840\n",
      "  NOUN   Val Precision: 0.2355, Recall: 0.2498, F1: 0.1993\n",
      "  ACTION Val Precision: 0.1185, Recall: 0.0915, F1: 0.0875\n",
      "  ---- Mean Top-5 Recall (validation) ----\n",
      "     VERB    Mean Top-5 Recall: 0.7725275650398264\n",
      "     NOUN    Mean Top-5 Recall: 0.42169806212257166\n",
      "     ACTION  Mean Top-5 Recall: 0.2549975896393579\n",
      "  VERB   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.3333333333333333  Top5: 0.8461538461538461\n",
      "    @ 0.50s  Top1: 0.36585365853658536  Top5: 0.8048780487804879\n",
      "    @ 0.75s  Top1: 0.3181818181818182  Top5: 0.7727272727272727\n",
      "    @ 1.00s  Top1: 0.32608695652173914  Top5: 0.7391304347826086\n",
      "    @ 1.25s  Top1: 0.3333333333333333  Top5: 0.7555555555555555\n",
      "    @ 1.50s  Top1: 0.3409090909090909  Top5: 0.75\n",
      "    @ 1.75s  Top1: 0.30434782608695654  Top5: 0.782608695652174\n",
      "    @ 2.00s  Top1: 0.3125  Top5: 0.7291666666666666\n",
      "    overall_top1: 0.3286118980169972, overall_top5: 0.7705382436260623\n",
      "  NOUN   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.28205128205128205  Top5: 0.48717948717948717\n",
      "    @ 0.50s  Top1: 0.2682926829268293  Top5: 0.4878048780487805\n",
      "    @ 0.75s  Top1: 0.2727272727272727  Top5: 0.4772727272727273\n",
      "    @ 1.00s  Top1: 0.2608695652173913  Top5: 0.391304347826087\n",
      "    @ 1.25s  Top1: 0.28888888888888886  Top5: 0.4\n",
      "    @ 1.50s  Top1: 0.2727272727272727  Top5: 0.38636363636363635\n",
      "    @ 1.75s  Top1: 0.2608695652173913  Top5: 0.34782608695652173\n",
      "    @ 2.00s  Top1: 0.2708333333333333  Top5: 0.3958333333333333\n",
      "    overall_top1: 0.2719546742209632, overall_top5: 0.4192634560906516\n",
      "  ACTION per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.1794871794871795  Top5: 0.2564102564102564\n",
      "    @ 0.50s  Top1: 0.17073170731707318  Top5: 0.2682926829268293\n",
      "    @ 0.75s  Top1: 0.1590909090909091  Top5: 0.25\n",
      "    @ 1.00s  Top1: 0.1956521739130435  Top5: 0.2391304347826087\n",
      "    @ 1.25s  Top1: 0.2  Top5: 0.24444444444444444\n",
      "    @ 1.50s  Top1: 0.18181818181818182  Top5: 0.25\n",
      "    @ 1.75s  Top1: 0.17391304347826086  Top5: 0.2608695652173913\n",
      "    @ 2.00s  Top1: 0.1875  Top5: 0.2708333333333333\n",
      "    overall_top1: 0.1813031161473088, overall_top5: 0.254957507082153\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14 Train:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14 Val:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 | Time 2.3s\n",
      "  Train Loss: 1.6651 | Val Loss: 6.9589\n",
      "  VERB   Train Top1: 0.7941176470588235, Top5: 1.0; Val Top1: 0.44192634560906513, Top5: 0.8441926345609065\n",
      "  NOUN   Train Top1: 0.8566176470588235, Top5: 0.9852941176470589; Val Top1: 0.3342776203966006, Top5: 0.5155807365439093\n",
      "  ACTION Train Top1: 0.8823529411764706, Top5: 0.9889705882352942; Val Top1: 0.20679886685552407, Top5: 0.32577903682719545\n",
      "  VERB   Val Precision: 0.2396, Recall: 0.1871, F1: 0.2047\n",
      "  NOUN   Val Precision: 0.2485, Recall: 0.2322, F1: 0.1891\n",
      "  ACTION Val Precision: 0.1184, Recall: 0.0942, F1: 0.0870\n",
      "  ---- Mean Top-5 Recall (validation) ----\n",
      "     VERB    Mean Top-5 Recall: 0.8453149152529456\n",
      "     NOUN    Mean Top-5 Recall: 0.515596602753615\n",
      "     ACTION  Mean Top-5 Recall: 0.32576447133048403\n",
      "  VERB   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.41025641025641024  Top5: 0.8717948717948718\n",
      "    @ 0.50s  Top1: 0.4878048780487805  Top5: 0.8780487804878049\n",
      "    @ 0.75s  Top1: 0.4318181818181818  Top5: 0.8409090909090909\n",
      "    @ 1.00s  Top1: 0.41304347826086957  Top5: 0.8260869565217391\n",
      "    @ 1.25s  Top1: 0.4444444444444444  Top5: 0.8444444444444444\n",
      "    @ 1.50s  Top1: 0.45454545454545453  Top5: 0.8409090909090909\n",
      "    @ 1.75s  Top1: 0.4782608695652174  Top5: 0.8478260869565217\n",
      "    @ 2.00s  Top1: 0.4166666666666667  Top5: 0.8125\n",
      "    overall_top1: 0.44192634560906513, overall_top5: 0.8441926345609065\n",
      "  NOUN   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.3333333333333333  Top5: 0.5384615384615384\n",
      "    @ 0.50s  Top1: 0.34146341463414637  Top5: 0.5121951219512195\n",
      "    @ 0.75s  Top1: 0.3409090909090909  Top5: 0.5227272727272727\n",
      "    @ 1.00s  Top1: 0.32608695652173914  Top5: 0.5217391304347826\n",
      "    @ 1.25s  Top1: 0.3333333333333333  Top5: 0.4888888888888889\n",
      "    @ 1.50s  Top1: 0.3409090909090909  Top5: 0.5\n",
      "    @ 1.75s  Top1: 0.32608695652173914  Top5: 0.4782608695652174\n",
      "    @ 2.00s  Top1: 0.3333333333333333  Top5: 0.5625\n",
      "    overall_top1: 0.3342776203966006, overall_top5: 0.5155807365439093\n",
      "  ACTION per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.20512820512820512  Top5: 0.3333333333333333\n",
      "    @ 0.50s  Top1: 0.1951219512195122  Top5: 0.3170731707317073\n",
      "    @ 0.75s  Top1: 0.18181818181818182  Top5: 0.3409090909090909\n",
      "    @ 1.00s  Top1: 0.1956521739130435  Top5: 0.32608695652173914\n",
      "    @ 1.25s  Top1: 0.2222222222222222  Top5: 0.3111111111111111\n",
      "    @ 1.50s  Top1: 0.22727272727272727  Top5: 0.3181818181818182\n",
      "    @ 1.75s  Top1: 0.1956521739130435  Top5: 0.32608695652173914\n",
      "    @ 2.00s  Top1: 0.22916666666666666  Top5: 0.3333333333333333\n",
      "    overall_top1: 0.20679886685552407, overall_top5: 0.32577903682719545\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15 Train:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15 Val:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 | Time 2.2s\n",
      "  Train Loss: 1.3237 | Val Loss: 7.0182\n",
      "  VERB   Train Top1: 0.8033088235294118, Top5: 0.9981617647058824; Val Top1: 0.48725212464589235, Top5: 0.8243626062322946\n",
      "  NOUN   Train Top1: 0.9264705882352942, Top5: 0.9908088235294118; Val Top1: 0.311614730878187, Top5: 0.5042492917847026\n",
      "  ACTION Train Top1: 0.9227941176470589, Top5: 0.9889705882352942; Val Top1: 0.2237960339943343, Top5: 0.31444759206798867\n",
      "  VERB   Val Precision: 0.2324, Recall: 0.2079, F1: 0.2065\n",
      "  NOUN   Val Precision: 0.2047, Recall: 0.1845, F1: 0.1615\n",
      "  ACTION Val Precision: 0.1055, Recall: 0.0996, F1: 0.0834\n",
      "  ---- Mean Top-5 Recall (validation) ----\n",
      "     VERB    Mean Top-5 Recall: 0.8254965868201553\n",
      "     NOUN    Mean Top-5 Recall: 0.5046496541707968\n",
      "     ACTION  Mean Top-5 Recall: 0.3142836514257511\n",
      "  VERB   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.48717948717948717  Top5: 0.8717948717948718\n",
      "    @ 0.50s  Top1: 0.5121951219512195  Top5: 0.8292682926829268\n",
      "    @ 0.75s  Top1: 0.4772727272727273  Top5: 0.8181818181818182\n",
      "    @ 1.00s  Top1: 0.4782608695652174  Top5: 0.8043478260869565\n",
      "    @ 1.25s  Top1: 0.4888888888888889  Top5: 0.8444444444444444\n",
      "    @ 1.50s  Top1: 0.5227272727272727  Top5: 0.8181818181818182\n",
      "    @ 1.75s  Top1: 0.45652173913043476  Top5: 0.8260869565217391\n",
      "    @ 2.00s  Top1: 0.4791666666666667  Top5: 0.7916666666666666\n",
      "    overall_top1: 0.48725212464589235, overall_top5: 0.8243626062322946\n",
      "  NOUN   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.3076923076923077  Top5: 0.5128205128205128\n",
      "    @ 0.50s  Top1: 0.3170731707317073  Top5: 0.5121951219512195\n",
      "    @ 0.75s  Top1: 0.3181818181818182  Top5: 0.5227272727272727\n",
      "    @ 1.00s  Top1: 0.30434782608695654  Top5: 0.4782608695652174\n",
      "    @ 1.25s  Top1: 0.3111111111111111  Top5: 0.5111111111111111\n",
      "    @ 1.50s  Top1: 0.3181818181818182  Top5: 0.5227272727272727\n",
      "    @ 1.75s  Top1: 0.30434782608695654  Top5: 0.45652173913043476\n",
      "    @ 2.00s  Top1: 0.3125  Top5: 0.5208333333333334\n",
      "    overall_top1: 0.311614730878187, overall_top5: 0.5042492917847026\n",
      "  ACTION per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.20512820512820512  Top5: 0.3076923076923077\n",
      "    @ 0.50s  Top1: 0.21951219512195122  Top5: 0.3170731707317073\n",
      "    @ 0.75s  Top1: 0.20454545454545456  Top5: 0.3181818181818182\n",
      "    @ 1.00s  Top1: 0.21739130434782608  Top5: 0.30434782608695654\n",
      "    @ 1.25s  Top1: 0.2222222222222222  Top5: 0.3111111111111111\n",
      "    @ 1.50s  Top1: 0.25  Top5: 0.3181818181818182\n",
      "    @ 1.75s  Top1: 0.21739130434782608  Top5: 0.30434782608695654\n",
      "    @ 2.00s  Top1: 0.25  Top5: 0.3333333333333333\n",
      "    overall_top1: 0.2237960339943343, overall_top5: 0.31444759206798867\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16 Train:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16 Val:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 | Time 2.3s\n",
      "  Train Loss: 1.1774 | Val Loss: 7.0835\n",
      "  VERB   Train Top1: 0.8345588235294118, Top5: 0.9981617647058824; Val Top1: 0.37960339943342775, Top5: 0.8130311614730878\n",
      "  NOUN   Train Top1: 0.9503676470588235, Top5: 0.9926470588235294; Val Top1: 0.3031161473087819, Top5: 0.546742209631728\n",
      "  ACTION Train Top1: 0.9099264705882353, Top5: 0.9963235294117647; Val Top1: 0.23229461756373937, Top5: 0.3342776203966006\n",
      "  VERB   Val Precision: 0.1911, Recall: 0.2008, F1: 0.1840\n",
      "  NOUN   Val Precision: 0.2247, Recall: 0.1864, F1: 0.1640\n",
      "  ACTION Val Precision: 0.1205, Recall: 0.1054, F1: 0.0946\n",
      "  ---- Mean Top-5 Recall (validation) ----\n",
      "     VERB    Mean Top-5 Recall: 0.8134818955935075\n",
      "     NOUN    Mean Top-5 Recall: 0.5468401680871856\n",
      "     ACTION  Mean Top-5 Recall: 0.3344319386869758\n",
      "  VERB   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.358974358974359  Top5: 0.8461538461538461\n",
      "    @ 0.50s  Top1: 0.3902439024390244  Top5: 0.8292682926829268\n",
      "    @ 0.75s  Top1: 0.3409090909090909  Top5: 0.7727272727272727\n",
      "    @ 1.00s  Top1: 0.391304347826087  Top5: 0.8043478260869565\n",
      "    @ 1.25s  Top1: 0.37777777777777777  Top5: 0.8222222222222222\n",
      "    @ 1.50s  Top1: 0.4090909090909091  Top5: 0.7954545454545454\n",
      "    @ 1.75s  Top1: 0.3695652173913043  Top5: 0.8043478260869565\n",
      "    @ 2.00s  Top1: 0.3958333333333333  Top5: 0.8333333333333334\n",
      "    overall_top1: 0.37960339943342775, overall_top5: 0.8130311614730878\n",
      "  NOUN   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.3333333333333333  Top5: 0.5641025641025641\n",
      "    @ 0.50s  Top1: 0.3170731707317073  Top5: 0.5609756097560976\n",
      "    @ 0.75s  Top1: 0.3181818181818182  Top5: 0.5454545454545454\n",
      "    @ 1.00s  Top1: 0.2826086956521739  Top5: 0.5217391304347826\n",
      "    @ 1.25s  Top1: 0.28888888888888886  Top5: 0.5555555555555556\n",
      "    @ 1.50s  Top1: 0.3181818181818182  Top5: 0.5227272727272727\n",
      "    @ 1.75s  Top1: 0.2608695652173913  Top5: 0.5\n",
      "    @ 2.00s  Top1: 0.3125  Top5: 0.6041666666666666\n",
      "    overall_top1: 0.3031161473087819, overall_top5: 0.546742209631728\n",
      "  ACTION per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.20512820512820512  Top5: 0.3333333333333333\n",
      "    @ 0.50s  Top1: 0.24390243902439024  Top5: 0.34146341463414637\n",
      "    @ 0.75s  Top1: 0.20454545454545456  Top5: 0.3409090909090909\n",
      "    @ 1.00s  Top1: 0.21739130434782608  Top5: 0.32608695652173914\n",
      "    @ 1.25s  Top1: 0.24444444444444444  Top5: 0.3333333333333333\n",
      "    @ 1.50s  Top1: 0.25  Top5: 0.3409090909090909\n",
      "    @ 1.75s  Top1: 0.2391304347826087  Top5: 0.32608695652173914\n",
      "    @ 2.00s  Top1: 0.25  Top5: 0.3333333333333333\n",
      "    overall_top1: 0.23229461756373937, overall_top5: 0.3342776203966006\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17 Train:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17 Val:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 | Time 2.3s\n",
      "  Train Loss: 0.8870 | Val Loss: 7.0010\n",
      "  VERB   Train Top1: 0.8952205882352942, Top5: 1.0; Val Top1: 0.47875354107648727, Top5: 0.8243626062322946\n",
      "  NOUN   Train Top1: 0.9558823529411765, Top5: 0.9944852941176471; Val Top1: 0.33994334277620397, Top5: 0.5382436260623229\n",
      "  ACTION Train Top1: 0.9595588235294118, Top5: 0.9963235294117647; Val Top1: 0.20113314447592068, Top5: 0.3342776203966006\n",
      "  VERB   Val Precision: 0.2467, Recall: 0.2026, F1: 0.2147\n",
      "  NOUN   Val Precision: 0.2918, Recall: 0.2501, F1: 0.2195\n",
      "  ACTION Val Precision: 0.1139, Recall: 0.1153, F1: 0.0874\n",
      "  ---- Mean Top-5 Recall (validation) ----\n",
      "     VERB    Mean Top-5 Recall: 0.8254033704159632\n",
      "     NOUN    Mean Top-5 Recall: 0.5387305391895132\n",
      "     ACTION  Mean Top-5 Recall: 0.3344319386869758\n",
      "  VERB   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.4358974358974359  Top5: 0.8461538461538461\n",
      "    @ 0.50s  Top1: 0.5121951219512195  Top5: 0.8536585365853658\n",
      "    @ 0.75s  Top1: 0.4772727272727273  Top5: 0.8181818181818182\n",
      "    @ 1.00s  Top1: 0.45652173913043476  Top5: 0.782608695652174\n",
      "    @ 1.25s  Top1: 0.4666666666666667  Top5: 0.8222222222222222\n",
      "    @ 1.50s  Top1: 0.5  Top5: 0.8409090909090909\n",
      "    @ 1.75s  Top1: 0.5  Top5: 0.8478260869565217\n",
      "    @ 2.00s  Top1: 0.4791666666666667  Top5: 0.7916666666666666\n",
      "    overall_top1: 0.47875354107648727, overall_top5: 0.8243626062322946\n",
      "  NOUN   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.38461538461538464  Top5: 0.5641025641025641\n",
      "    @ 0.50s  Top1: 0.34146341463414637  Top5: 0.5609756097560976\n",
      "    @ 0.75s  Top1: 0.36363636363636365  Top5: 0.5227272727272727\n",
      "    @ 1.00s  Top1: 0.32608695652173914  Top5: 0.5217391304347826\n",
      "    @ 1.25s  Top1: 0.3333333333333333  Top5: 0.5333333333333333\n",
      "    @ 1.50s  Top1: 0.3409090909090909  Top5: 0.5227272727272727\n",
      "    @ 1.75s  Top1: 0.2826086956521739  Top5: 0.5217391304347826\n",
      "    @ 2.00s  Top1: 0.3541666666666667  Top5: 0.5625\n",
      "    overall_top1: 0.33994334277620397, overall_top5: 0.5382436260623229\n",
      "  ACTION per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.1794871794871795  Top5: 0.3333333333333333\n",
      "    @ 0.50s  Top1: 0.1951219512195122  Top5: 0.34146341463414637\n",
      "    @ 0.75s  Top1: 0.18181818181818182  Top5: 0.3409090909090909\n",
      "    @ 1.00s  Top1: 0.1956521739130435  Top5: 0.32608695652173914\n",
      "    @ 1.25s  Top1: 0.2  Top5: 0.3333333333333333\n",
      "    @ 1.50s  Top1: 0.20454545454545456  Top5: 0.3409090909090909\n",
      "    @ 1.75s  Top1: 0.21739130434782608  Top5: 0.32608695652173914\n",
      "    @ 2.00s  Top1: 0.22916666666666666  Top5: 0.3333333333333333\n",
      "    overall_top1: 0.20113314447592068, overall_top5: 0.3342776203966006\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18 Train:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18 Val:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 | Time 2.3s\n",
      "  Train Loss: 0.7932 | Val Loss: 7.0709\n",
      "  VERB   Train Top1: 0.8860294117647058, Top5: 1.0; Val Top1: 0.4475920679886686, Top5: 0.7762039660056658\n",
      "  NOUN   Train Top1: 0.9595588235294118, Top5: 0.9981617647058824; Val Top1: 0.29745042492917845, Top5: 0.5354107648725213\n",
      "  ACTION Train Top1: 0.9705882352941176, Top5: 0.9963235294117647; Val Top1: 0.22096317280453256, Top5: 0.3342776203966006\n",
      "  VERB   Val Precision: 0.1859, Recall: 0.1960, F1: 0.1864\n",
      "  NOUN   Val Precision: 0.1835, Recall: 0.1932, F1: 0.1587\n",
      "  ACTION Val Precision: 0.1232, Recall: 0.1223, F1: 0.0993\n",
      "  ---- Mean Top-5 Recall (validation) ----\n",
      "     VERB    Mean Top-5 Recall: 0.7774849038966218\n",
      "     NOUN    Mean Top-5 Recall: 0.5345690931255618\n",
      "     ACTION  Mean Top-5 Recall: 0.3344319386869758\n",
      "  VERB   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.4358974358974359  Top5: 0.8205128205128205\n",
      "    @ 0.50s  Top1: 0.4878048780487805  Top5: 0.8048780487804879\n",
      "    @ 0.75s  Top1: 0.4772727272727273  Top5: 0.7727272727272727\n",
      "    @ 1.00s  Top1: 0.41304347826086957  Top5: 0.7608695652173914\n",
      "    @ 1.25s  Top1: 0.4222222222222222  Top5: 0.7555555555555555\n",
      "    @ 1.50s  Top1: 0.45454545454545453  Top5: 0.7727272727272727\n",
      "    @ 1.75s  Top1: 0.45652173913043476  Top5: 0.782608695652174\n",
      "    @ 2.00s  Top1: 0.4375  Top5: 0.75\n",
      "    overall_top1: 0.4475920679886686, overall_top5: 0.7762039660056658\n",
      "  NOUN   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.3333333333333333  Top5: 0.5128205128205128\n",
      "    @ 0.50s  Top1: 0.3170731707317073  Top5: 0.5121951219512195\n",
      "    @ 0.75s  Top1: 0.3181818181818182  Top5: 0.5681818181818182\n",
      "    @ 1.00s  Top1: 0.2826086956521739  Top5: 0.5434782608695652\n",
      "    @ 1.25s  Top1: 0.3111111111111111  Top5: 0.5555555555555556\n",
      "    @ 1.50s  Top1: 0.2727272727272727  Top5: 0.5227272727272727\n",
      "    @ 1.75s  Top1: 0.2608695652173913  Top5: 0.4782608695652174\n",
      "    @ 2.00s  Top1: 0.2916666666666667  Top5: 0.5833333333333334\n",
      "    overall_top1: 0.29745042492917845, overall_top5: 0.5354107648725213\n",
      "  ACTION per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.1794871794871795  Top5: 0.3333333333333333\n",
      "    @ 0.50s  Top1: 0.21951219512195122  Top5: 0.34146341463414637\n",
      "    @ 0.75s  Top1: 0.18181818181818182  Top5: 0.3409090909090909\n",
      "    @ 1.00s  Top1: 0.1956521739130435  Top5: 0.32608695652173914\n",
      "    @ 1.25s  Top1: 0.24444444444444444  Top5: 0.3333333333333333\n",
      "    @ 1.50s  Top1: 0.25  Top5: 0.3409090909090909\n",
      "    @ 1.75s  Top1: 0.2391304347826087  Top5: 0.32608695652173914\n",
      "    @ 2.00s  Top1: 0.25  Top5: 0.3333333333333333\n",
      "    overall_top1: 0.22096317280453256, overall_top5: 0.3342776203966006\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19 Train:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19 Val:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 | Time 2.4s\n",
      "  Train Loss: 0.6789 | Val Loss: 7.1082\n",
      "  VERB   Train Top1: 0.9191176470588235, Top5: 1.0; Val Top1: 0.49008498583569404, Top5: 0.8130311614730878\n",
      "  NOUN   Train Top1: 0.9632352941176471, Top5: 0.9981617647058824; Val Top1: 0.3654390934844193, Top5: 0.5410764872521246\n",
      "  ACTION Train Top1: 0.9724264705882353, Top5: 0.9981617647058824; Val Top1: 0.23512747875354106, Top5: 0.3342776203966006\n",
      "  VERB   Val Precision: 0.2610, Recall: 0.2176, F1: 0.2277\n",
      "  NOUN   Val Precision: 0.2978, Recall: 0.2594, F1: 0.2225\n",
      "  ACTION Val Precision: 0.1314, Recall: 0.1068, F1: 0.0989\n",
      "  ---- Mean Top-5 Recall (validation) ----\n",
      "     VERB    Mean Top-5 Recall: 0.8134333832554272\n",
      "     NOUN    Mean Top-5 Recall: 0.5406994840319337\n",
      "     ACTION  Mean Top-5 Recall: 0.3344319386869758\n",
      "  VERB   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.4358974358974359  Top5: 0.8205128205128205\n",
      "    @ 0.50s  Top1: 0.5121951219512195  Top5: 0.8048780487804879\n",
      "    @ 0.75s  Top1: 0.4772727272727273  Top5: 0.7954545454545454\n",
      "    @ 1.00s  Top1: 0.4782608695652174  Top5: 0.782608695652174\n",
      "    @ 1.25s  Top1: 0.5111111111111111  Top5: 0.8444444444444444\n",
      "    @ 1.50s  Top1: 0.5227272727272727  Top5: 0.8409090909090909\n",
      "    @ 1.75s  Top1: 0.5217391304347826  Top5: 0.8478260869565217\n",
      "    @ 2.00s  Top1: 0.4583333333333333  Top5: 0.7708333333333334\n",
      "    overall_top1: 0.49008498583569404, overall_top5: 0.8130311614730878\n",
      "  NOUN   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.38461538461538464  Top5: 0.5384615384615384\n",
      "    @ 0.50s  Top1: 0.3902439024390244  Top5: 0.5365853658536586\n",
      "    @ 0.75s  Top1: 0.38636363636363635  Top5: 0.5454545454545454\n",
      "    @ 1.00s  Top1: 0.3695652173913043  Top5: 0.5217391304347826\n",
      "    @ 1.25s  Top1: 0.35555555555555557  Top5: 0.5555555555555556\n",
      "    @ 1.50s  Top1: 0.3409090909090909  Top5: 0.5227272727272727\n",
      "    @ 1.75s  Top1: 0.34782608695652173  Top5: 0.5217391304347826\n",
      "    @ 2.00s  Top1: 0.3541666666666667  Top5: 0.5833333333333334\n",
      "    overall_top1: 0.3654390934844193, overall_top5: 0.5410764872521246\n",
      "  ACTION per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.20512820512820512  Top5: 0.3333333333333333\n",
      "    @ 0.50s  Top1: 0.24390243902439024  Top5: 0.34146341463414637\n",
      "    @ 0.75s  Top1: 0.22727272727272727  Top5: 0.3409090909090909\n",
      "    @ 1.00s  Top1: 0.21739130434782608  Top5: 0.32608695652173914\n",
      "    @ 1.25s  Top1: 0.24444444444444444  Top5: 0.3333333333333333\n",
      "    @ 1.50s  Top1: 0.25  Top5: 0.3409090909090909\n",
      "    @ 1.75s  Top1: 0.2391304347826087  Top5: 0.32608695652173914\n",
      "    @ 2.00s  Top1: 0.25  Top5: 0.3333333333333333\n",
      "    overall_top1: 0.23512747875354106, overall_top5: 0.3342776203966006\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20 Train:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20 Val:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 | Time 2.4s\n",
      "  Train Loss: 0.6367 | Val Loss: 7.0497\n",
      "  VERB   Train Top1: 0.9319852941176471, Top5: 1.0; Val Top1: 0.49291784702549574, Top5: 0.7932011331444759\n",
      "  NOUN   Train Top1: 0.9577205882352942, Top5: 0.9963235294117647; Val Top1: 0.33994334277620397, Top5: 0.5807365439093485\n",
      "  ACTION Train Top1: 0.9669117647058824, Top5: 1.0; Val Top1: 0.20679886685552407, Top5: 0.3342776203966006\n",
      "  VERB   Val Precision: 0.2618, Recall: 0.2150, F1: 0.2273\n",
      "  NOUN   Val Precision: 0.2684, Recall: 0.2425, F1: 0.2149\n",
      "  ACTION Val Precision: 0.1087, Recall: 0.0964, F1: 0.0854\n",
      "  ---- Mean Top-5 Recall (validation) ----\n",
      "     VERB    Mean Top-5 Recall: 0.7937467067073376\n",
      "     NOUN    Mean Top-5 Recall: 0.5813905790509714\n",
      "     ACTION  Mean Top-5 Recall: 0.3344319386869758\n",
      "  VERB   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.48717948717948717  Top5: 0.8205128205128205\n",
      "    @ 0.50s  Top1: 0.5365853658536586  Top5: 0.8048780487804879\n",
      "    @ 0.75s  Top1: 0.4772727272727273  Top5: 0.7727272727272727\n",
      "    @ 1.00s  Top1: 0.4782608695652174  Top5: 0.7608695652173914\n",
      "    @ 1.25s  Top1: 0.4888888888888889  Top5: 0.7777777777777778\n",
      "    @ 1.50s  Top1: 0.5227272727272727  Top5: 0.7954545454545454\n",
      "    @ 1.75s  Top1: 0.5  Top5: 0.8260869565217391\n",
      "    @ 2.00s  Top1: 0.4583333333333333  Top5: 0.7916666666666666\n",
      "    overall_top1: 0.49291784702549574, overall_top5: 0.7932011331444759\n",
      "  NOUN   per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.358974358974359  Top5: 0.5897435897435898\n",
      "    @ 0.50s  Top1: 0.3170731707317073  Top5: 0.6097560975609756\n",
      "    @ 0.75s  Top1: 0.3409090909090909  Top5: 0.6136363636363636\n",
      "    @ 1.00s  Top1: 0.34782608695652173  Top5: 0.5652173913043478\n",
      "    @ 1.25s  Top1: 0.35555555555555557  Top5: 0.5777777777777777\n",
      "    @ 1.50s  Top1: 0.36363636363636365  Top5: 0.5681818181818182\n",
      "    @ 1.75s  Top1: 0.32608695652173914  Top5: 0.5434782608695652\n",
      "    @ 2.00s  Top1: 0.3125  Top5: 0.5833333333333334\n",
      "    overall_top1: 0.33994334277620397, overall_top5: 0.5807365439093485\n",
      "  ACTION per-horizon (time-based):\n",
      "    @ 0.25s  Top1: 0.1794871794871795  Top5: 0.3333333333333333\n",
      "    @ 0.50s  Top1: 0.1951219512195122  Top5: 0.34146341463414637\n",
      "    @ 0.75s  Top1: 0.18181818181818182  Top5: 0.3409090909090909\n",
      "    @ 1.00s  Top1: 0.1956521739130435  Top5: 0.32608695652173914\n",
      "    @ 1.25s  Top1: 0.2222222222222222  Top5: 0.3333333333333333\n",
      "    @ 1.50s  Top1: 0.22727272727272727  Top5: 0.3409090909090909\n",
      "    @ 1.75s  Top1: 0.21739130434782608  Top5: 0.32608695652173914\n",
      "    @ 2.00s  Top1: 0.22916666666666666  Top5: 0.3333333333333333\n",
      "    overall_top1: 0.20679886685552407, overall_top5: 0.3342776203966006\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "## === EDIT these paths ===\n",
    "FUSED_CSV_PATH = r\"D:\\Datasets\\Datasets\\EPIC\\Features\\FusedFeatures\\P01_02_fused_features.csv\"\n",
    "LABEL_CSV_PATH = r\"D:\\Datasets\\Datasets\\EPIC\\Labels\\P01_02.csv\"\n",
    "BEST_MODEL_PATH = Path(r\"D:\\Datasets\\Datasets\\EPIC\\Model\\P01_02_fused_model.pth\")\n",
    "# ========================\n",
    "\n",
    "# Hyperparams\n",
    "T_OBS = 90\n",
    "FEAT_DIM = 512\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 20\n",
    "LR = 1e-4\n",
    "WD = 1e-4\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "# === Time-based anticipation config ===\n",
    "FPS = 30.0 \n",
    "HORIZONS_S = [0.25, 0.5, 0.75, 1.0, 1.25, 1.50, 1.75, 2.0]   # seconds into the future\n",
    "K_FUT = len(HORIZONS_S)               # model will output one label per horizon\n",
    "# =====================================\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def detect_num_classes_from_labels_df(labels_df):\n",
    "    verbs = set()\n",
    "    nouns = set()\n",
    "    actions = set()\n",
    "    for cand in [\"Verb_class\", \"verb\", \"Verb\", \"verb_class\"]:\n",
    "        if cand in labels_df.columns:\n",
    "            verbs.update(labels_df[cand].dropna().astype(int).tolist())\n",
    "            break\n",
    "    for cand in [\"Noun_class\", \"noun\", \"Noun\", \"noun_class\"]:\n",
    "        if cand in labels_df.columns:\n",
    "            nouns.update(labels_df[cand].dropna().astype(int).tolist())\n",
    "            break\n",
    "    for cand in [\"Action_class\", \"action\", \"Action\", \"ActionLabel\"]:\n",
    "        if cand in labels_df.columns:\n",
    "            actions.update(labels_df[cand].dropna().astype(int).tolist())\n",
    "            break\n",
    "    nv = (max(verbs) + 1) if len(verbs) > 0 else 1\n",
    "    nn_ = (max(nouns) + 1) if len(nouns) > 0 else 1\n",
    "    na = (max(actions) + 1) if len(actions) > 0 else 1\n",
    "    return {\"verb\": int(nv), \"noun\": int(nn_), \"action\": int(na)}\n",
    "\n",
    "\n",
    "def topk_counts(logits, labels, k):\n",
    "    # logits: (B, K_fut, C); labels: (B, K_fut)\n",
    "    with torch.no_grad():\n",
    "        B, K, C = logits.shape\n",
    "        topk_preds = logits.topk(k, dim=-1)[1]  # (B, K, k)\n",
    "        hits = 0\n",
    "        total = 0\n",
    "        for h in range(K):\n",
    "            lab = labels[:, h]  # (B,)\n",
    "            mask = (lab != IGNORE_INDEX)\n",
    "            if int(mask.sum().item()) == 0:\n",
    "                continue\n",
    "            predk = topk_preds[:, h, :]  # (B, k)\n",
    "            lab_exp = lab.unsqueeze(1).expand(-1, k)\n",
    "            masked_pred = predk[mask]   # (M, k)\n",
    "            masked_lab = lab_exp[mask]  # (M, k)\n",
    "            hit_vec = (masked_pred == masked_lab).any(dim=1).float()\n",
    "            hits += int(hit_vec.sum().item())\n",
    "            total += int(mask.sum().item())\n",
    "        return hits, total\n",
    "\n",
    "\n",
    "# Load fused and labels\n",
    "fused_df = pd.read_csv(FUSED_CSV_PATH)\n",
    "labels_df = pd.read_csv(LABEL_CSV_PATH)\n",
    "\n",
    "# Dataset\n",
    "dataset = SingleVideoAnticipationDataset(\n",
    "    fused_df,\n",
    "    labels_df,\n",
    "    t_obs=T_OBS,\n",
    "    k_fut=K_FUT,        # must equal len(HORIZONS_S)\n",
    "    feat_dim=FEAT_DIM,\n",
    "    fps=FPS,\n",
    "    horizons_s=HORIZONS_S\n",
    ")\n",
    "\n",
    "# split indices for train/val (60/40)\n",
    "indices = list(range(len(dataset)))\n",
    "random.seed(42)\n",
    "random.shuffle(indices)\n",
    "split_at = int(0.6 * len(indices))\n",
    "train_idx = indices[:split_at]\n",
    "val_idx = indices[split_at:]\n",
    "\n",
    "train_ds = Subset(dataset, train_idx)\n",
    "val_ds = Subset(dataset, val_idx)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=(DEVICE == \"cuda\")\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=(DEVICE == \"cuda\")\n",
    ")\n",
    "\n",
    "# detect number of classes and instantiate model\n",
    "num_classes = detect_num_classes_from_labels_df(labels_df)\n",
    "print(\"Detected num_classes:\", num_classes)\n",
    "model = AnticipationModel(\n",
    "    feat_dim=FEAT_DIM,\n",
    "    num_classes=num_classes,\n",
    "    k_fut=K_FUT\n",
    ").to(DEVICE)\n",
    "\n",
    "opt = optim.Adam(model.parameters(), lr=LR, weight_decay=WD)\n",
    "sched = optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    t0 = time.time()\n",
    "\n",
    "    # ------------- TRAIN -------------\n",
    "    model.train()\n",
    "    train_loss_sum = 0.0\n",
    "    train_samples = 0\n",
    "    train_counts = {\n",
    "        \"verb_top1\": [0, 0], \"verb_top5\": [0, 0],\n",
    "        \"noun_top1\": [0, 0], \"noun_top5\": [0, 0],\n",
    "        \"action_top1\": [0, 0], \"action_top5\": [0, 0]\n",
    "    }\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch} Train\", leave=False)\n",
    "    for F_batch, y_multi, meta in pbar:\n",
    "        F_batch = F_batch.to(DEVICE)               # (B, T, D)\n",
    "        y_v = y_multi[\"verb\"].to(DEVICE)           # (B, K_fut)\n",
    "        y_n = y_multi[\"noun\"].to(DEVICE)\n",
    "        y_a = y_multi[\"action\"].to(DEVICE)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        logits = model(F_batch)   # dict: \"verb\"/\"noun\"/\"action\" -> (B, K_fut, C)\n",
    "\n",
    "        loss_v = masked_cross_entropy(logits[\"verb\"], y_v)\n",
    "        loss_n = masked_cross_entropy(logits[\"noun\"], y_n)\n",
    "        loss_a = masked_cross_entropy(logits[\"action\"], y_a)\n",
    "        loss = loss_a + 0.5 * loss_v + 0.5 * loss_n\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        b = F_batch.size(0)\n",
    "        train_loss_sum += float(loss.item()) * b\n",
    "        train_samples += b\n",
    "\n",
    "        for (task, lab, lg) in [\n",
    "            (\"verb\", y_v, logits[\"verb\"]),\n",
    "            (\"noun\", y_n, logits[\"noun\"]),\n",
    "            (\"action\", y_a, logits[\"action\"])\n",
    "        ]:\n",
    "            h1, t1 = topk_counts(lg.detach().cpu(), lab.detach().cpu(), k=1)\n",
    "            h5, t5 = topk_counts(lg.detach().cpu(), lab.detach().cpu(), k=5)\n",
    "            train_counts[f\"{task}_top1\"][0] += h1\n",
    "            train_counts[f\"{task}_top1\"][1] += t1\n",
    "            train_counts[f\"{task}_top5\"][0] += h5\n",
    "            train_counts[f\"{task}_top5\"][1] += t5\n",
    "\n",
    "    train_loss = train_loss_sum / max(1, train_samples)\n",
    "    train_metrics = {}\n",
    "    for task in [\"verb\", \"noun\", \"action\"]:\n",
    "        h1, t1 = train_counts[f\"{task}_top1\"]\n",
    "        h5, t5 = train_counts[f\"{task}_top5\"]\n",
    "        train_metrics[f\"{task}_top1\"] = (h1 / t1) if t1 > 0 else None\n",
    "        train_metrics[f\"{task}_top5\"] = (h5 / t5) if t5 > 0 else None\n",
    "\n",
    "    # ------------- VALIDATION -------------\n",
    "    model.eval()\n",
    "    val_loss_sum = 0.0\n",
    "    val_samples = 0\n",
    "    val_counts = {\n",
    "        \"verb_top1\": [0, 0], \"verb_top5\": [0, 0],\n",
    "        \"noun_top1\": [0, 0], \"noun_top5\": [0, 0],\n",
    "        \"action_top1\": [0, 0], \"action_top5\": [0, 0]\n",
    "    }\n",
    "\n",
    "    # store logits/labels for per-horizon + P/R/F1 metrics\n",
    "    val_logits_store = {\"verb\": [], \"noun\": [], \"action\": []}\n",
    "    val_labels_store = {\"verb\": [], \"noun\": [], \"action\": []}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_loader, desc=f\"Epoch {epoch} Val\", leave=False)\n",
    "        for F_batch, y_multi, meta in pbar:\n",
    "            F_batch = F_batch.to(DEVICE)\n",
    "            y_v = y_multi[\"verb\"].to(DEVICE)\n",
    "            y_n = y_multi[\"noun\"].to(DEVICE)\n",
    "            y_a = y_multi[\"action\"].to(DEVICE)\n",
    "\n",
    "            logits = model(F_batch)\n",
    "            loss_v = masked_cross_entropy(logits[\"verb\"], y_v)\n",
    "            loss_n = masked_cross_entropy(logits[\"noun\"], y_n)\n",
    "            loss_a = masked_cross_entropy(logits[\"action\"], y_a)\n",
    "            loss = loss_a + 0.5 * loss_v + 0.5 * loss_n\n",
    "\n",
    "            b = F_batch.size(0)\n",
    "            val_loss_sum += float(loss.item()) * b\n",
    "            val_samples += b\n",
    "\n",
    "            for (task, lab, lg) in [\n",
    "                (\"verb\", y_v, logits[\"verb\"]),\n",
    "                (\"noun\", y_n, logits[\"noun\"]),\n",
    "                (\"action\", y_a, logits[\"action\"])\n",
    "            ]:\n",
    "                h1, t1 = topk_counts(lg.detach().cpu(), lab.detach().cpu(), k=1)\n",
    "                h5, t5 = topk_counts(lg.detach().cpu(), lab.detach().cpu(), k=5)\n",
    "                val_counts[f\"{task}_top1\"][0] += h1\n",
    "                val_counts[f\"{task}_top1\"][1] += t1\n",
    "                val_counts[f\"{task}_top5\"][0] += h5\n",
    "                val_counts[f\"{task}_top5\"][1] += t5\n",
    "\n",
    "            # store for per-horizon + P/R/F1 metrics\n",
    "            val_logits_store[\"verb\"].append(logits[\"verb\"].detach().cpu())\n",
    "            val_logits_store[\"noun\"].append(logits[\"noun\"].detach().cpu())\n",
    "            val_logits_store[\"action\"].append(logits[\"action\"].detach().cpu())\n",
    "            val_labels_store[\"verb\"].append(y_v.detach().cpu())\n",
    "            val_labels_store[\"noun\"].append(y_n.detach().cpu())\n",
    "            val_labels_store[\"action\"].append(y_a.detach().cpu())\n",
    "\n",
    "    val_loss = val_loss_sum / max(1, val_samples)\n",
    "\n",
    "    # overall val metrics (top-1/top-5 over all horizons)\n",
    "    val_metrics = {}\n",
    "    for task in [\"verb\", \"noun\", \"action\"]:\n",
    "        h1, t1 = val_counts[f\"{task}_top1\"]\n",
    "        h5, t5 = val_counts[f\"{task}_top5\"]\n",
    "        val_metrics[f\"{task}_top1\"] = (h1 / t1) if t1 > 0 else None\n",
    "        val_metrics[f\"{task}_top5\"] = (h5 / t5) if t5 > 0 else None\n",
    "\n",
    "    # per-horizon metrics (time-based)\n",
    "    per_horizon_metrics = {\"verb\": {}, \"noun\": {}, \"action\": {}}\n",
    "    for task in [\"verb\", \"noun\", \"action\"]:\n",
    "        if len(val_logits_store[task]) == 0:\n",
    "            continue\n",
    "        logits_all = torch.cat(val_logits_store[task], dim=0)  # (N, K_fut, C)\n",
    "        labels_all = torch.cat(val_labels_store[task], dim=0)  # (N, K_fut)\n",
    "        m = topk_accuracy_per_task(\n",
    "            logits_all,\n",
    "            labels_all,\n",
    "            topk=(1, 5),\n",
    "            ignore_index=IGNORE_INDEX\n",
    "        )\n",
    "        per_horizon_metrics[task] = m\n",
    "\n",
    "    # macro precision / recall / F1 over all horizons (validation)\n",
    "    prf_metrics = {\"verb\": {}, \"noun\": {}, \"action\": {}}\n",
    "    for task in [\"verb\", \"noun\", \"action\"]:\n",
    "        if len(val_logits_store[task]) == 0:\n",
    "            continue\n",
    "\n",
    "        logits_all = torch.cat(val_logits_store[task], dim=0)\n",
    "        labels_all = torch.cat(val_labels_store[task], dim=0)\n",
    "\n",
    "        preds_all = logits_all.argmax(dim=-1)  # (N, K_fut)\n",
    "        mask = (labels_all != IGNORE_INDEX)\n",
    "        if mask.sum().item() == 0:\n",
    "            continue\n",
    "\n",
    "        y_true = labels_all[mask].numpy()\n",
    "        y_pred = preds_all[mask].numpy()\n",
    "\n",
    "        p, r, f1, _ = precision_recall_fscore_support(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            average=\"macro\",\n",
    "            zero_division=0\n",
    "        )\n",
    "        prf_metrics[task][\"precision\"] = p\n",
    "        prf_metrics[task][\"recall\"] = r\n",
    "        prf_metrics[task][\"f1\"] = f1\n",
    "\n",
    "    # mean Top-5 recall across horizons for each task\n",
    "    mean_top5_recall = {}\n",
    "    for task in [\"verb\", \"noun\", \"action\"]:\n",
    "        mh = per_horizon_metrics[task]\n",
    "        if not mh:\n",
    "            mean_top5_recall[task] = None\n",
    "            continue\n",
    "\n",
    "        vals = []\n",
    "        for h_idx in range(K_FUT):\n",
    "            key = f\"per_h{h_idx+1}_top5\"\n",
    "            if key in mh and mh[key] is not None:\n",
    "                vals.append(mh[key])\n",
    "        mean_top5_recall[task] = float(np.mean(vals)) if len(vals) > 0 else None\n",
    "\n",
    "    # scheduler + logging\n",
    "    sched.step(val_loss)\n",
    "    elapsed = time.time() - t0\n",
    "\n",
    "    print(f\"Epoch {epoch}/{NUM_EPOCHS} | Time {elapsed:.1f}s\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    for task in [\"verb\", \"noun\", \"action\"]:\n",
    "        print(\n",
    "            f\"  {task.upper():6s} Train Top1: {train_metrics[f'{task}_top1']}, \"\n",
    "            f\"Top5: {train_metrics[f'{task}_top5']}; \"\n",
    "            f\"Val Top1: {val_metrics[f'{task}_top1']}, \"\n",
    "            f\"Top5: {val_metrics[f'{task}_top5']}\"\n",
    "        )\n",
    "\n",
    "    # print macro precision / recall / F1 (validation)\n",
    "    for task in [\"verb\", \"noun\", \"action\"]:\n",
    "        if prf_metrics[task]:\n",
    "            p = prf_metrics[task][\"precision\"]\n",
    "            r = prf_metrics[task][\"recall\"]\n",
    "            f1 = prf_metrics[task][\"f1\"]\n",
    "            print(\n",
    "                f\"  {task.upper():6s} Val Precision: {p:.4f}, \"\n",
    "                f\"Recall: {r:.4f}, F1: {f1:.4f}\"\n",
    "            )\n",
    "\n",
    "    # print mean Top-5 recall\n",
    "    print(\"  ---- Mean Top-5 Recall (validation) ----\")\n",
    "    for task in [\"verb\", \"noun\", \"action\"]:\n",
    "        print(\n",
    "            f\"     {task.upper():6s}  Mean Top-5 Recall: {mean_top5_recall[task]}\"\n",
    "        )\n",
    "\n",
    "    # print per-horizon by seconds\n",
    "    for task in [\"verb\", \"noun\", \"action\"]:\n",
    "        mh = per_horizon_metrics[task]\n",
    "        if not mh:\n",
    "            continue\n",
    "        print(f\"  {task.upper():6s} per-horizon (time-based):\")\n",
    "        for h_idx, t_sec in enumerate(HORIZONS_S):\n",
    "            key1 = f\"per_h{h_idx+1}_top1\"\n",
    "            key5 = f\"per_h{h_idx+1}_top5\"\n",
    "            v1 = mh.get(key1, None)\n",
    "            v5 = mh.get(key5, None)\n",
    "            print(f\"    @ {t_sec:4.2f}s  Top1: {v1}  Top5: {v5}\")\n",
    "        print(\n",
    "            f\"    overall_top1: {mh.get('overall_top1', None)}, \"\n",
    "            f\"overall_top5: {mh.get('overall_top5', None)}\"\n",
    "        )\n",
    "\n",
    "    # optional: save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(\n",
    "            {\n",
    "                'epoch': epoch,\n",
    "                'model_state': model.state_dict(),\n",
    "                'opt_state': opt.state_dict(),\n",
    "                'val_loss': val_loss\n",
    "            },\n",
    "            BEST_MODEL_PATH\n",
    "        )\n",
    "        print(f\"[SAVED BEST] -> {BEST_MODEL_PATH}\")\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
